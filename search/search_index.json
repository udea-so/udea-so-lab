{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Laboratorio de Sistema Operativos - UdeA","text":"<p>To Do</p>"},{"location":"#to-do","title":"To Do","text":""},{"location":"#general-guidance","title":"General Guidance","text":"<ul> <li>To Do 1</li> <li>To Do 2</li> </ul>"},{"location":"lab1/","title":"Agile Development","text":"<p>In this documentation we refer to the team working on an engagement a \"Crew\". This includes the dev team, dev lead, PM, data scientists, etc.</p>"},{"location":"lab1/#why-agile","title":"Why Agile","text":"<ul> <li>We want to be quick to respond to change</li> <li>We want to get to a state of working software fast, and iterate on it to improve it</li> <li>We want to keep the customer/end users involved all the way through</li> <li>We care about individuals and interactions over documents and processes</li> </ul>"},{"location":"lab1/#the-fundamentals","title":"The Fundamentals","text":"<p>We care about the goal for each activity, but not necessarily about how they are accomplished. The suggestions in parenthesis are common ways to accomplish the goals.</p> <ul> <li>We keep a shared backlog of work, that everyone in the team can always access (ex. Azure DevOps or GitHub)</li> <li>We plan our work in iterations with clear goals (ex. sprints)</li> <li>We have a clear idea of when work items are ready to implement (ex. definition of ready)</li> <li>We have a clear idea of when work items are completed (ex. definition of done)</li> <li>We communicate the progress in one place that everyone can access, and keep the progress up to date (ex. sprint board and daily standups)</li> <li>We reflect on our work regularly to make improvements (ex. retrospectives)</li> <li>The team has a clear idea of the roles and responsibilities in the project (ex. Dev lead, TPM, Process Lead etc.)</li> <li>The team has a joint idea of how we work together (ex. team agreement)</li> <li>We value and respect the opinions and work of all team members.</li> </ul>"},{"location":"lab1/#references","title":"References","text":"<ul> <li>What Is Scrum?</li> <li>Essential Scrum: A Practical Guide to The Most Popular Agile Process</li> </ul>"},{"location":"lab1/backlog-management/","title":"Backlog Management","text":""},{"location":"lab1/backlog-management/#backlog","title":"Backlog","text":"<p>Goals</p> <ul> <li>User stories have a clear acceptance criteria and definition of done.</li> <li>Design activities are planned as part of the backlog (a design for a story that needs it should be done before it is added in a Sprint).</li> </ul> <p>Suggestions</p> <ul> <li>Consider the backlog refinement as an ongoing activity, that expands outside of the typical \"Refinement meeting\".</li> <li>The team should decide on and have a clear understanding of a definition of ready and a definition of done.</li> <li>The team should have a clear understanding of what constitutes good acceptance criteria for a story/task, and decide on how stories/tasks are handled. Eg. in some projects, stories are refined as a crew, but tasks are created by individual developers on an as needed bases.</li> <li>Technical debt is mostly due to shortcuts made in the implementation as well as the future maintenance cost as the natural result of continuous improvement. Shortcuts should generally be avoided. In some rare instances where they happen, prioritizing and planning improvement activities to reduce this debt at a later time is the recommended approach.</li> </ul>"},{"location":"lab1/backlog-management/#resources","title":"Resources","text":"<ul> <li>Product Backlog</li> <li>Sprint Backlog</li> <li>Acceptance Criteria</li> <li>Definition of Done</li> <li>Definition of Ready</li> <li>Estimation Basics in Agile</li> </ul>"},{"location":"lab1/ceremonies/","title":"Agile Ceremonies","text":""},{"location":"lab1/ceremonies/#sprint-planning","title":"Sprint Planning","text":"<p>Goals</p> <ul> <li>The planning supports Diversity and Inclusion principles and provides equal opportunities.</li> <li>The Planning defines how the work is going to be completed in the sprint.</li> <li>Stories fit in a sprint and are designed and ready before the planning.</li> </ul> <p>Note: Self assignment by team members can give a feeling of fairness in how work is split in the team. Sometime, this ends up not being the case as it can give an advantage to the loudest or more experienced voices in the team. Individuals also tend to stay in their comfort zone, which might not be the right approach for their own growth.*</p>"},{"location":"lab1/ceremonies/#sprint-goal","title":"Sprint Goal","text":"<p>Consider defining a sprint goal, or list of goals for each sprint. Effective sprint goals are a concise bullet point list of items. A Sprint goal can be created first and used as an input to choose the Stories for the sprint. A sprint goal could also be created from the list of stories that were picked for the Sprint.</p> <p>The sprint goal can be used:</p> <ul> <li>At the end of each stand up meeting, to remember the north star for the Sprint and help everyone taking a step back</li> <li>During the sprint review (\"was the goal achieved?\", \"If not, why?\")</li> </ul> <p>Note: A simple way to define a sprint goal, is to create a User Story in each sprint backlog and name it \"Sprint XX goal\". You can add the bullet points in the description.*</p>"},{"location":"lab1/ceremonies/#stories","title":"Stories","text":"<p>Example 1: Preparing in advance</p> <ul> <li>The dev lead and product owner plan time to prepare the sprint backlog ahead of sprint planning.</li> <li>The dev lead uses their experience (past and on the current project) and the estimation made for these stories to gauge how many should be in the sprint.</li> <li>The dev lead asks the entire team to look at the tentative sprint backlog in advance of the sprint planning.</li> <li>The dev lead assigns stories to specific developers after confirming with them that it makes sense</li> <li>During the sprint planning meeting, the team reviews the sprint goal and the stories. Everyone confirm they understand the plan and feel it's reasonable.</li> </ul> <p>Example 2: Building during the planning meeting</p> <ul> <li>The product owner ensures that the highest priority items of the product backlog is refined and estimated following the team estimation process.</li> <li>During the Sprint planning meeting, the product owner describe each stories, one by one, starting by highest priority.</li> <li>For each story, the dev lead and the team confirm they understand what needs to be done and add the story to the sprint backlog.</li> <li>The team keeps considering more stories up to a point where they agree the sprint backlog is full. This should be informed by the estimation, past developer experience and past experience in this specific project.</li> <li>Stories are assigned during the planning meeting:<ul> <li>Option 1: The dev lead makes suggestion on who could work on each stories. Each engineer agrees or discuss if required.</li> <li>Option 2: The team review each story and engineer volunteer select the one they want to be assigned to. <p>Note: this option might cause issues with the first core expectations. Who gets to work on what? Ultimately, it is the dev lead responsibility to ensure each engineer gets the opportunity to work on what makes sense for their growth.)</p> </li> </ul> </li> </ul>"},{"location":"lab1/ceremonies/#tasks","title":"Tasks","text":"<p>Examples of approaches for task creation and assignment:</p> <ul> <li>Stories are split into tasks ahead of time by dev lead and assigned before/during sprint planning to engineers.</li> <li>Stories are assigned to more senior engineers who are responsible for splitting into tasks.</li> <li>Stories are split into tasks during the Sprint planning meeting by the entire team.</li> </ul> <p>Note: Depending on the seniority of the team, consider splitting into tasks before sprint planning. This can help getting out of sprint planning with all work assigned. It also increase clarity for junior engineers.</p>"},{"location":"lab1/ceremonies/#sprint-planning-resources","title":"Sprint Planning Resources","text":"<ul> <li>Definition of Ready</li> <li>Sprint Goal Template</li> <li>Planning</li> <li>Refinement</li> <li>User Stories Applied: For Software Development</li> </ul>"},{"location":"lab1/ceremonies/#estimation","title":"Estimation","text":"<p>Goals</p> <ul> <li>Estimation supports the predictability of the team work and delivery.</li> <li>Estimation re-enforces the value of accountability to the team.</li> <li>The estimation process is improved over time and discussed on a regular basis.</li> <li>Estimation is inclusive of the different individuals in the team.</li> </ul> <p>Rough estimation is usually done for a generic SE 2 dev.</p>"},{"location":"lab1/ceremonies/#example-1-t-shirt-sizes","title":"Example 1: T-shirt Sizes","text":"<ul> <li>The team use t-shirt sizes (S, M, L, XL) and agrees in advance which size fits a sprint. In this example: S, M fits a sprint, L, XL too big for a sprint and need to be split / refined</li> <li>The dev lead with support of the team roughly estimates how much S and M stories can be done in the first sprints</li> <li>This rough estimation is refined over time and used to as an input for future sprint planning and to adjust project end date forecasting</li> </ul>"},{"location":"lab1/ceremonies/#example-2-single-indicator","title":"Example 2: Single Indicator","text":"<ul> <li>The team uses a single indicator: \"does this story fits in one sprint?\", if not, the story needs to be split</li> <li>The dev lead with support of the team roughly estimates how many stories can be done in the first sprints</li> <li>How many stories are done in each sprint on average is used as an input for future sprint planning and as an indicator to adjust project end date forecasting</li> </ul>"},{"location":"lab1/ceremonies/#example-3-planning-poker","title":"Example 3: Planning Poker","text":"<ul> <li>The team does planning poker and estimates in story points</li> <li>Story points are roughly used to estimate how much can be done in next sprint</li> <li>The dev lead and the TPM uses the past sprints and observed velocity to adjust project end date forecasting</li> </ul>"},{"location":"lab1/ceremonies/#other-considerations","title":"Other Considerations","text":"<ul> <li>Estimating stories using story points in smaller project does not always provide the value it would in bigger ones.</li> <li>Avoid converting story points or t-shirt sizes to days.</li> </ul>"},{"location":"lab1/ceremonies/#measure-estimation-accuracy","title":"Measure Estimation Accuracy","text":"<ul> <li>Collect data to monitor estimation accuracy and sprint completion over time to drive improvements.</li> <li>Use the sprint goal to understand if the estimation was correct. If the sprint goal is met: does anything else matter?</li> </ul>"},{"location":"lab1/ceremonies/#scrum-practices","title":"Scrum Practices","text":"<p>While Scrum does not prescribe how to size work, Professional Scrum is biased away from absolute estimation (hours, function points, ideal-days, etc.) and towards relative sizing.</p> <p>Planning Poker</p> <p>Planning Poker is a collaborative technique to assign relative size. Developers may choose whatever units they want - story points and t-shirt sizes are examples of units.</p> <p>'Same-Size' Product Backlog Items (PBIs)</p> <p>'Same-Size' PBIs is a relative estimation approach that involves breaking items down small enough that they are roughly the same size. Velocity can be understood as a count of PBIs; this is sometimes used by teams doing continuously delivery.</p> <p>'Right-Size' Product Backlog Items (PBIs)</p> <p>'Right-Size' PBIs is a relative estimation approach that involves breaking things down small enough to deliver value in a certain time period (i.e. get to Done by the end of a Sprint). This is sometimes associated with teams utilizing flow for forecasting. Teams use historical data to determine if they think they can get the PBI done within the confidence level that their historical data says they typically get a PBI done.</p>"},{"location":"lab1/ceremonies/#estimation-resources","title":"Estimation Resources","text":"<ul> <li>The Most Important Thing You Are Missing about Estimation</li> </ul>"},{"location":"lab1/ceremonies/#retrospectives","title":"Retrospectives","text":"<p>Goals</p> <ul> <li>Retrospectives lead to actionable items that help grow the team's engineering practices. These items are in the backlog, assigned, and prioritized to be fixed by a date agreed upon (default being next retrospective).</li> <li>Retrospectives are used to ask the hard questions (\"we usually don't finish what we plan, let's talk about this\") when necessary.</li> </ul> <p>Suggestions</p> <ul> <li>Consider other retro formats available outside of Mad Sad Glad.<ul> <li>Gather Data: Triple Nickels, Timeline, Mad Sad Glad, Team Radar</li> <li>Generate Insights: 5 Whys, Fishbone, Patterns and Shifts</li> </ul> </li> <li>Consider setting a retro focus area.</li> <li>Schedule enough time to ensure that you can have the conversation you need to get the correct plan an action and improve how you work.</li> <li>Bring in a neutral facilitator for project retros or retros that introspect after a difficult period.</li> </ul> <p>Use the following retrospectives techniques to address specific trends that might be emerging on an engagement</p>"},{"location":"lab1/ceremonies/#5-whys","title":"5 Whys","text":"<p>If a team is confronting a problem and is unsure of the exact root cause, the 5 whys exercise taken from the business analysis sector can help get to the bottom of it.\u00a0For example, if a team cannot get to Done each Sprint, that would go at the top of the whiteboard. The team then asks why that problem exists, writing that answer in the box below.\u00a0 Next, the team asks why again, but this time in response to the why they just identified. Continue this process until the team identifies an actual root cause, which usually becomes apparent within five steps.</p>"},{"location":"lab1/ceremonies/#processes-tools-individuals-interactions-and-the-definition-of-done","title":"Processes, Tools, Individuals, Interactions and the Definition of Done","text":"<p>This approach encourages team members to think more broadly.\u00a0 Ask team members to identify what is going well and ideas for improvement within the categories of processes, tools, individuals/interactions, and the Definition of Done.\u00a0 Then, ask team members to vote on which improvement ideas to focus on during the upcoming Sprint.</p>"},{"location":"lab1/ceremonies/#focus","title":"Focus","text":"<p>This retrospective technique incorporates the concept of visioning. Using this technique, you ask team members where they would like to go?\u00a0 Decide what the team should look like in 4 weeks, and then ask what is holding them back from that and how they can resolve the impediment.\u00a0 If you are focusing on specific improvements, you can use this technique for one or two Retrospectives in a row so that the team can see progress over time.</p>"},{"location":"lab1/ceremonies/#retrospective-resources","title":"Retrospective Resources","text":"<ul> <li>Agile Retrospective: Making Good Teams Great</li> <li>Retrospective</li> </ul>"},{"location":"lab1/ceremonies/#sprint-demo","title":"Sprint Demo","text":"<p>Goals</p> <ul> <li>Each sprint ends with demos that illustrate the sprint goal and how it fits in the engagement goal.</li> </ul> <p>Suggestions</p> <ul> <li>Consider not pre-recording sprint demos in advance. You can record the demo meeting and archive them.</li> <li>A demo does not have to be about running code. It can be showing documentation that was written.</li> </ul>"},{"location":"lab1/ceremonies/#sprint-demo-resources","title":"Sprint Demo Resources","text":"<ul> <li>Sprint Review/Demo</li> </ul>"},{"location":"lab1/ceremonies/#stand-up","title":"Stand-Up","text":"<p>Goals</p> <ul> <li>The stand-up is run efficiently.</li> <li>The stand-up helps the team understand what was done, what will be done and what are the blockers.</li> <li>The stand-up helps the team understand if they will meet the sprint goal or not.</li> </ul> <p>Suggestions</p> <ul> <li>Keep stand up short and efficient. Table the longer conversations for a parking lot section, or for a conversation that will be planned later.</li> <li>Run daily stand ups: 15 minutes of stand up and 15 minutes of parking lot.</li> <li>If someone cannot make the stand-up exceptionally: Ask them to do a written stand up in advance.</li> <li>Stand ups should include everyone involved in the project, including the customer.</li> <li>Projects with widely divergent time zones should be avoided if possible, but if you are on one, you should adapt the standups to meet the needs and time constraints of all team members.</li> </ul>"},{"location":"lab1/ceremonies/#stand-up-resources","title":"Stand-Up Resources","text":"<ul> <li>Stand-Up/Daily Scrum</li> </ul>"},{"location":"lab1/roles/","title":"Agile/Scrum Roles","text":"<ul> <li>We prefer using \"process lead\" over \"scrum master\". It describes the same role.</li> </ul> <p>This section has links directing you to definitions for the traditional roles within Agile/Scrum.  After reading through the best practices you should have a basic understanding of the key Agile roles in terms of what they are and the expectations for the role.</p> <ul> <li>Product Owner</li> <li>Scrum Master</li> <li>Development Team</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/external-feedback/","title":"External Feedback","text":"<p>Various stakeholders can provide feedback to the working product during a project, beyond any formal review and feedback sessions required by the organization. The frequency and method of collecting feedback through reviews varies depending on the case, but a couple of good practices are:</p> <ul> <li>Capture each review in the backlog as a separate user story.</li> <li>Standardize the tasks that implement this user story.</li> <li>Plan for a review user story per Epic / Feature in your backlog proactively.</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/","title":"Minimal Slices","text":""},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/#always-deliver-your-work-using-minimal-valuable-slices","title":"Always Deliver Your Work Using Minimal Valuable Slices","text":"<ul> <li>Split your work item into small chunks that are contributed in incremental commits.</li> <li>Contribute your chunks frequently. Follow an iterative approach by regularly providing updates and changes to the team. This allows for instant feedback and early issue discovery and ensures you are developing in the right direction, both technically and functionally.</li> </ul> <ul> <li>Do NOT work independently on your task without providing any updates to your team.</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/#example","title":"Example","text":"<p>Imagine you are working on adding UWP (Universal Windows Platform) application building functionality for existing continuous integration service which already has Android/iOS support.</p>"},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/#bad-approach","title":"Bad Approach","text":"<p>After six weeks of work you created PR with all required functionality, including portal UI (build settings), backend REST API (UWP build functionality), telemetry, unit and integration tests, etc.</p>"},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/#good-approach","title":"Good Approach","text":"<p>You divided your feature into smaller user stories (which in turn were divided into multiple tasks) and started working on them one by one:</p> <ul> <li>As a user I can successfully build UWP apps using current service</li> <li>As a user I can see telemetry when building the apps</li> <li>As a user I have the ability to select build configuration (debug, release)</li> <li>As a user I have the ability to select target platform (arm, x86, x64)</li> <li>...</li> </ul> <p>You also divided your stories into smaller tasks and sent PRs based on those tasks. E.g. you have the following tasks for the first user story above:</p> <ul> <li>Enable UWP platform on backend</li> <li>Add <code>build</code> button to the UI (build first solution file found)</li> <li>Add <code>select solution file</code> dropdown to the UI</li> <li>Implement unit tests</li> <li>Implement integration tests to verify build succeeded</li> <li>Update documentation</li> <li>...</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/minimal-slices/#resources","title":"Resources","text":"<ul> <li>Minimalism Rules</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/risk-management/","title":"Risk Management","text":"<p>Agile methodologies are conceived to be driven by risk management principles, but no methodology can eliminate all risks.</p>"},{"location":"lab1/advanced-topics/backlog-management/risk-management/#goal","title":"Goal","text":"<p>Anticipation is a key aspect of software project management, involving the proactive identification and assessment of potential risks and challenges to enable effective planning and mitigation strategies.</p> <p>The following guidance aims to provide decision-makers with the information needed to make informed choices, understanding trade-offs, costs, and project timelines throughout the project.</p>"},{"location":"lab1/advanced-topics/backlog-management/risk-management/#general-guidance","title":"General Guidance","text":"<ul> <li>Identify risks in every activity such as a planning meetings, design and code reviews, or daily standups. All team members are responsible for identifying relevant risks.</li> <li>Assess risks in terms of their likelihood and potential impact on the project. Use the issues to report and track risks. Issues represent unplanned activities.</li> <li>Prioritize them based on their severity and likelihood, focusing on addressing the most critical ones first.</li> <li>Mitigate or reduce the impact and likelihood of the risks.</li> <li>Monitor continuously to ensure the effectiveness of the mitigation strategies.</li> <li>Prepare contingency plans for high-impact risks that may still materialize.</li> <li>Communicate and report risks to keep all stakeholders informed.</li> </ul>"},{"location":"lab1/advanced-topics/backlog-management/risk-management/#opportunity-management","title":"Opportunity Management","text":"<p>The same process can be applied to opportunities, but while risk management involves applying mitigation actions to decrease the likelihood of a risk, in opportunity management, you enhance actions to increase the likelihood of a positive outcome.</p>"},{"location":"lab1/advanced-topics/collaboration/add-pairing-field-azure-devops-cards/","title":"How to Add a Pairing Custom Field in Azure DevOps User Stories","text":"<p>This document outlines the benefits of adding a custom field of type Identity in Azure DevOps user stories, prerequisites, and a step-by-step guide.</p>"},{"location":"lab1/advanced-topics/collaboration/add-pairing-field-azure-devops-cards/#benefits-of-adding-a-custom-field","title":"Benefits of Adding a Custom Field","text":"<p>Having the names of both individuals pairing on a story visible on the Azure DevOps cards can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. For example, it is easier to keep track of the individuals assigned stories as part of a pair during sprint planning by using the \"pairing names\" field. During stand-up it can also help the Process Lead filter stories assigned to the individual (both as an owner or as a pairing assignee) and show these on the board. Furthermore, the pairing field can provide an additional data point for reports and burndown rates.</p>"},{"location":"lab1/advanced-topics/collaboration/add-pairing-field-azure-devops-cards/#prerequisites","title":"Prerequisites","text":"<p>Prior to customizing Azure DevOps, review Configure and customize Azure Boards.</p> <p>In order to add a custom field to user stories in Azure DevOps changes must be made as an Organization setting. This document therefore assumes use of an existing Organization in Azure DevOps and that the user account used to make these changes is a member of the Project Collection Administrators Group.</p>"},{"location":"lab1/advanced-topics/collaboration/add-pairing-field-azure-devops-cards/#change-the-organization-settings","title":"Change the Organization Settings","text":"<ol> <li> <p>Duplicate the process currently in use.</p> <p>Navigate to the Organization Settings, within the Boards / Process tab.</p> <p></p> </li> <li> <p>Select the Process type, click on the icon with three dots ... and click Create inherited process.</p> <p></p> </li> <li> <p>Click on the newly created inherited process.</p> <p>As you can see in the example below, we called it 'Pairing'.</p> <p></p> </li> <li> <p>Click on the work item type User Story.</p> <p></p> </li> <li> <p>Click New Field.</p> <p></p> </li> <li> <p>Give it a Name and select Identity in Type. Click on Add Field.</p> <p></p> <p>This completes the change in Organization settings. The rest of the instructions must be completed under Project Settings.</p> </li> </ol>"},{"location":"lab1/advanced-topics/collaboration/add-pairing-field-azure-devops-cards/#change-the-project-settings","title":"Change the Project Settings","text":"<ol> <li> <p>Go to the Project that is to be modified, select Project Settings.</p> <p></p> </li> <li> <p>Select Project configuration.</p> <p></p> </li> <li> <p>Click on process customization page.</p> <p></p> </li> <li> <p>Click on Projects then click on Change process.</p> <p></p> </li> <li> <p>Change the target process to Pairing then click Save.</p> <p></p> </li> <li> <p>Go to Boards.</p> <p></p> </li> <li> <p>Click on the Gear icon to open Settings.</p> <p></p> </li> <li> <p>Add field to card.</p> <p>Click on the green + icon to add select the Pairing field. Check the box to display fields, even when they are empty. Save and close.</p> <p></p> </li> <li> <p>View the modified the card.</p> <p>Notice the new Pairing field. The Story can now be assigned an Owner and a Pairing assignee!</p> <p></p> </li> </ol>"},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/","title":"Effortless Pair Programming with GitHub Codespaces and VSCode","text":"<p>Pair programming used to be a software development technique in which two programmers work together on a single computer, sharing one keyboard and mouse, to jointly design, code, test, and debug software. It is one of the patterns explored in the section why collaboration? of this playbook, however with teams that work mostly remotely, sharing a physical computer became a challenge, but opened the door to a more efficient approach of pair programming.</p> <p>Through the effective utilization of a range of tools and techniques, we have successfully implemented both pair and swarm programming methodologies. As such, we are eager to share some of the valuable insights and knowledge gained from this experience.</p>"},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/#how-to-make-pair-programming-a-painless-experience","title":"How to Make Pair Programming a Painless Experience?","text":""},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/#working-sessions","title":"Working Sessions","text":"<p>In order to enhance pair programming capabilities, you can create regular working sessions that are open to all team members. This facilitates smooth and efficient collaboration as everyone can simply join in and work together before branching off into smaller groups. This approach has proven particularly beneficial for new team members who may otherwise feel overwhelmed by a large codebase. It emulates the concept of the \"humble water cooler,\" which fosters a sense of connectedness among team members through their shared work.</p> <p>Additionally, scheduling these working sessions in advance ensures intentional collaboration and provides clarity on user story responsibilities. To this end, assign a single person to each user story to ensure clear ownership and eliminate ambiguity. By doing so, this could eliminate the common problem of engineers being hesitant to modify code outside of their assigned tasks due to the sentiment of lack of ownership. These working sessions are instrumental in promoting a cohesive team dynamic, allowing for effective knowledge sharing and collective problem-solving.</p>"},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/#github-codespaces","title":"GitHub Codespaces","text":"<p>GitHub Codespaces is a vital component in an efficient development environment, particularly in the context of pair programming. Prioritize setting up a Codespace as the initial step of the project, preceding tasks such as local machine project compilation or VSCode plugin installation. To this end, make sure to update the Codespace documentation before incorporating any quick start instructions for local environments. Additionally, consistently demonstrate demos in codespaces environment to ensure its prominent integration into our workflow.</p> <p>With its cloud-based infrastructure, GitHub Codespaces presents a highly efficient and simplified approach to real-time collaborative coding. As a result, new team members can easily access the GitHub project and begin coding within seconds, without requiring installation on their local machines. This seamless, integrated solution for pair programming offers a streamlined workflow, allowing you to direct your attention towards producing exemplary code, free from the distractions of cumbersome setup processes.</p>"},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/#vscode-live-share","title":"VSCode Live Share","text":"<p>VSCode Live Share is specifically designed for pair programming and enables you to work on the same codebase, in real-time, with your team members. The arduous process of configuring complex setups, grappling with confusing configurations, straining one's eyes to work on small screens, or physically switching keyboards is not a problem with LiveShare. This innovative solution enables seamless sharing of your development environment with your team members, facilitating smooth collaborative coding experiences.</p> <p>Fully integrated into Visual Studio Code and Visual Studio, LiveShare offers the added benefit of terminal sharing, debug session collaboration, and host machine control. When paired with GitHub Codespaces, it presents a potent tool set for effective pair programming.</p> <p>Tip: Share VSCode extensions (including Live Share) using a base devcontainer.json. This ensure all team members have available the same set of extensions, and allow them to focus in solving the business needs from day one.</p>"},{"location":"lab1/advanced-topics/collaboration/pair-programming-tools/#resources","title":"Resources","text":"<ul> <li>GitHub Codespaces.</li> <li>VSCode Live Share.</li> <li>Create a Dev Container.</li> <li>How companies have optimized the humble office water cooler.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/social-question/","title":"Social Question of the Day","text":"<p>The social question of the day is an optional short question to follow the three project questions in the daily stand-up. It develops team cohesion and interpersonal trust over the course of an engagement by facilitating the sharing of personal preferences, lifestyle, or other context.</p> <p>The social question should be chosen before the stand-up. The facilitator should select the question either independently or from the team's asynchronous suggestions. This minimizes delays at the start of the stand-up.</p> <p>Tip: having the stand-up facilitator role rotate each sprint lets the facilitator choose the social question independently without burdening any one team member.</p>"},{"location":"lab1/advanced-topics/collaboration/social-question/#properties-of-a-good-question","title":"Properties of a Good Question","text":"<p>A good question has a brief answer with small optional elaboration. A yes or no answer doesn't tell you very much about someone, while knowing that their favorite fruit is a durian is informative.</p> <p>Good questions are low in consequence but allow controversy. Watching someone strongly exclaim that salmon and lox on cinnamon-raisin is the best bagel order is endearing. As a corollary, a good question is one someone is likely to be passionate about. You know a little more about a team member's personality if their eyes light up when describing their favorite karaoke song.</p>"},{"location":"lab1/advanced-topics/collaboration/social-question/#starter-list-of-questions","title":"Starter List of Questions","text":"<p>Potentially good questions include:</p> <ul> <li>What's your Starbucks order?</li> <li>What's your favorite operating system?</li> <li>What's your favorite version of Windows?</li> <li>What's your favorite plant, houseplant or otherwise?</li> <li>What's your favorite fruit?</li> <li>What's your favorite fast food?</li> <li>What's your favorite noodle?</li> <li>What's your favorite text editor?</li> <li>Mountains or beach?</li> <li>DC or Marvel?</li> <li>Coffee with one person from history: who?</li> <li>What's your silliest online purchase?</li> <li>What's your alternate career?</li> <li>What's the best bagel topping?</li> <li>What's your guilty TV pleasure?</li> <li>What's your go-to karaoke song?</li> <li>Would you rather see the past or the future?</li> <li>Would you rather be able to teleport or to fly?</li> <li>Would you rather live underwater or in space for a year?</li> <li>What's your favorite phone app?</li> <li>What's your favorite fish, to eat or otherwise?</li> <li>What was your best costume?</li> <li>Who is someone you admire (from history, from your personal life, etc.)? Give one reason why.</li> <li>What's the best compliment you've ever received?</li> <li>What's your favorite or most used emoji right now?</li> <li>What was your biggest DIY project?</li> <li>What's a spice that you use on everything?</li> <li>What's your top Spotify (or just your favorite) genre/artist for this year?</li> <li>What was your first computer?</li> <li>What's your favorite kind of taco?</li> <li>What's your favorite decade?</li> <li>What's the best way to eat potatoes?</li> <li>What was your best vacation (stay-cations acceptable)?</li> <li>Favorite cartoon?</li> <li>Pick someone in your family and tell us something awesome about them.</li> <li>What was your longest road trip?</li> <li>What thing do you remember learning when you were young that is taught differently now?</li> <li>What was your favorite toy as a child?</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/teaming-up/","title":"Engagement Team Development","text":"<p>In every ISE engagement, dynamics are different so are the team requirements. Based on transfer learning among teams, we aim to build right \"code-with\" environments in every team.</p> <p>This documentation gives a high-level template with some suggestions by aiming to accelerate team swarming phase to achieve a high speed agility however it has no intention to provide a list of \"must-do\" items.</p>"},{"location":"lab1/advanced-topics/collaboration/teaming-up/#identification","title":"Identification","text":"<p>As it's stated in Tuckman's team phases, traditional team development has several stages. However those phases can be extremely fast or sometimes mismatched in teams due to external factors, what applies to ISE engagements.</p> <p>In order to minimize the risk and set the expectations on the right way for all parties, an identification phase is important to understand each other. Some potential steps in this phase may be as following (not limited):</p> <ul> <li>Working agreement</li> </ul> <ul> <li> <p>Identification of styles/preferences in communication, sharing, learning, decision making of each team member</p> <ul> <li>Talking about necessity of pair programming</li> <li>Decisions on backlog management &amp; refinement meetings, weekly design sessions, social time sessions...etc.</li> <li>Sync/Async communication methods, work hours/flexible times</li> </ul> </li> </ul> <ul> <li>Decisions and identifications of charts that will be helpful to provide transparent and true information to everyone</li> </ul> <ul> <li> <p>Identification of \"Software Craftspersonship\" areas which means the tools and methods will be widely used during the engagement and taking the required actions on team upskilling side if necessary.</p> <ul> <li>GitHub, VSCode LiveShare, AzDevOps, necessary development tools &amp; libraries ... more.</li> <li>If upskilling on certain topic(s) is needed, identifying the areas and arranging code spikes for increasing the team knowledge on the regarding topic(s).</li> </ul> </li> </ul> <ul> <li>Identification of communication channels, feedback loops and recurrent team call slots out of regular sprint meetings</li> </ul> <ul> <li>Introduction to Technical Agility Team Manifesto and planning the technical delivery by aiming to keep technical debt risk minimum.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/teaming-up/#following-the-plan-and-agile-debugging","title":"Following the Plan and Agile Debugging","text":"<p>Identification phase accelerates the process of building a safe environment for every individual in the team, later on team has the required assets to follow the plan. And it is team's itself responsibility (engineers,PO,Process Lead) to debug their Agility level.</p> <p>In every team stabilization takes time and pro-active agile debugging is the best accelerator to decrease the distraction away from sprint/engagement goal. Team is also responsible to keep the plan up-to-date based on team changes/needs and debugging results.</p> <p>Just as an example, agility debugging activities may include:</p> <ul> <li>Dashboards related with \"Goal\" such as burndown/burnout, Item/PR Aging, Mood Chart ..etc. are accessible to the team and team is always up-to-date</li> </ul> <ul> <li>Backlog Refinement meetings<ul> <li>Size of stories (Too big? Too small?)</li> <li>Are \"User Stories\" and \"Tasks\" clear ?</li> <li>Are Acceptance Criteria enough and right?</li> <li>Is everyone ready-to-go after taking the User Story/Task?</li> </ul> </li> </ul> <ul> <li>Running efficient retrospectives</li> </ul> <ul> <li>Is the Sprint Goal clear in every iteration ?</li> </ul> <ul> <li>Is the estimation process in the team improving over time or does it meet the delivery/workload prediction?</li> </ul> <p>Kindly check Scrum Values to have a better understanding to improve team commitment.</p> <p>Following that, above suggestions aim to remove agile/team disfunctionalities and provide a broader team understanding, potential time savings and full transparency.</p>"},{"location":"lab1/advanced-topics/collaboration/teaming-up/#resources","title":"Resources","text":"<ul> <li>Tuckman's Stages of Group Development</li> <li>Scrum Values</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/","title":"Virtual Collaboration and Pair Programming","text":"<p>Pair programming is the de facto work method that most large engineering organizations use for \u201chands on keyboard\u201d coding. Two developers, working synchronously, looking at the same screen and attempting to code and design together, which often results in better and clearer code than either could produce individually.</p> <p>Pair programming works well under the correct circumstances, but it loses some of its charm when executed in a completely virtual setting. The virtual setup still involves two developers looking at the same screen and talking out their designs, but there are often logistical issues to deal with, including lag, microphone set up issues, workspace and personal considerations, and many other small, individually trivial problems that worsen the experience.</p> <p>Virtual work patterns are different from the in-person patterns we are accustomed to. Pair programming at its core is based on the following principles:</p> <ol> <li>Generating clarity through communication</li> <li>Producing higher quality through collaboration</li> <li>Creating ownership through equal contribution</li> </ol> <p>Pair programming is one way to achieve these results. Red Team Testing (RTT) is an alternate programming method that uses the same principles but with some of the advantages that virtual work methods provide.</p>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/#red-team-testing-rtt","title":"Red Team Testing (RTT)","text":"<p>Red Team Testing borrows its name from the \u201cRed Team\u201d and \u201cBlue Team\u201d paradigm of penetration testing, and is a collaborative, parallel way of working virtually. In Red Team Testing, two developers jointly decide on the interface, architecture, and design of the program, and then separate for the implementation phase. One developer writes tests using the public interface, attempting to perform edge case testing, input validation, and otherwise stress testing the interface. The second developer is simultaneously writing the implementation which will eventually be tested.</p> <p>Red Team Testing has the same philosophy as any other Test-Driven Development lifecycle: All implementation is separated from the interface, and the interface can be tested with no knowledge of the implementation.</p> <p></p>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/#steps","title":"Steps","text":"<ol> <li>Design Phase: Both developers design the interface together. This includes:     - Method signatures and names     - Writing documentation or docstrings for what the methods are intended to do.     - Architecture decisions that would influence testing (Factory patterns, etc.)</li> <li>Implementation Phase: The developers separate and parallelize work, while continuing to communicate.     - Developer A will design the implementation of the methods, adhering to the previously decided design.     - Developer B will concurrently write tests for the same method signatures, without knowing details of the implementation.</li> <li>Integration &amp; Testing Phase: Both developers commit their code and run the tests.     - Utopian Scenario: All tests run and pass correctly.     - Realistic Scenario: The tests have either broken or failed due to flaws in testing. This leads to further clarification of the design and a discussion of why the tests failed.</li> <li>The developers will repeat the three phases until the code is functional and tested.</li> </ol>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/#when-to-follow-the-rtt-strategy","title":"When to Follow the RTT Strategy","text":"<p>RTT works well under specific circumstances. If collaboration needs to happen virtually, and all communication is virtual, RTT reduces the need for constant communication while maintaining the benefits of a joint design session. This considers the human element: Virtual communication is more exhausting than in person communication.</p> <p>RTT also works well when there is complete consensus, or no consensus at all, on what purpose the code serves. Since creating the design jointly and agreeing to implement and test against it are part of the RTT method, RTT forcibly creates clarity through iteration and communication.</p>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/#benefits","title":"Benefits","text":"<p>RTT has many of the same benefits as Pair Programming and Test-Driven development but tries to update them for a virtual setting.</p> <ul> <li>Code implementation and testing can be done in parallel, over long distances or across time zones, which reduces the overall time taken to finish writing the code.</li> <li>RTT maintains the pair programming paradigm, while reducing the need for video communication or constant communication between developers.</li> <li>RTT allows detailed focus on design and engineering alignment before implementing any code, leading to cleaner and simpler interfaces.</li> <li>RTT encourages testing to be prioritized alongside implementation, instead of having testing follow or be influenced by the implementation of the code.</li> <li>Documentation is inherently a part of RTT, since both the implementer and the tester need correct, up to date documentation, in the implementation phase.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/virtual-collaboration/#what-you-need-for-rtt-to-work-well","title":"What You Need for RTT to Work Well","text":"<ul> <li>Demand for constant communication and good teamwork may pose a challenge; daily updates amongst team members are essential to maintain alignment on varying code requirements.</li> <li>Clarity of the code design and testing strategy must be established beforehand and documented as reference. Lack of an established design will cause misalignment between the two major pieces of work and a need for time-consuming refactoring.</li> <li>RTT does not work well if only one developer has knowledge of the overall design. Team communication is critical to ensuring that every developer involved in RTT is on the same page.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/","title":"Why Collaboration","text":""},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#why-is-collaboration-important","title":"Why is Collaboration Important","text":"<p>In engagements, we aim to be highly collaborative because when we code together, we perform better, have a higher sprint velocity, and have a greater degree of knowledge sharing across the team.</p> <p>There are two common patterns we use for collaboration: Pairing and swarming.</p> <p>Pair programming (\u201cpairing\u201d) - two software engineers assigned to, and working on, one shared story at a time during the sprint. The Dev Lead assigns a user story to two engineers -- one primary engineer (story owner) and one secondary engineer (pairing assignee).</p> <p>Swarm programming (\u201cswarming\u201d) - three or more software engineers collaborating on a high-priority item to bring it to completion.</p>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#how-to-pair-program","title":"How to Pair Program","text":"<p>As mentioned, every story is intentionally assigned to a pair. The pairing assignee may be in the process of upskilling, nevertheless, they are equal partners in the development effort. Below are some general guidelines for pairing:</p> <ul> <li>Upon assignment of the story/product backlog item (PBI), the pair needs to be deliberate about defining how to work together and have a firm definition of the work to be completed. This information should be expressed clearly in the story\u2019s description and acceptance criteria. The expectations about this need to be communicated and agreed upon by both engineers and should be done prior to any actual working sessions.</li> <li>The story owner and pairing assignee do not merely split the work up and sync regularly \u2013 they actively work together on the same tasks, and might share their screens via a Teams online session. Collaborative tools like VS Live Share can be preferable to sharing screens. Not all collaboration needs to be screen-share based.</li> <li>During the collaborative sessions, one engineer provides the development environment while the other actively views and comments verbally.</li> <li>Engineers trade places often from one session to the next so that everyone has time in control of the keyboard.</li> <li>Engineers leverage feature branches for the collaboration during the development of each story to have small Pull Requests (PRs) (as opposed to a single giant PR) at the end of the sprint.</li> <li>Code is committed to the repository by both members of the assigned pair where and when it makes sense as tasks were completed.</li> <li>The pairing assignee is the voice representing the pair during the daily standup while being supported by the story owner.</li> <li>Having the names of both individuals (owner and pair assignee) visible on the PBI can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. An example of this using Azure DevOps cards can be found here.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#why-pair-programming-helps-collaboration","title":"Why Pair Programming Helps Collaboration","text":"<p>Pair programming helps collaboration because both engineers share equal responsibility for bringing the story to completion. This is a mutually beneficial exercise because, while the story owner often has more experience to lean on, the pairing assignee brings a fresh view that is unclouded by repetition.</p> <p>Some other benefits include:</p> <ul> <li>Fewer defects and increased accountability. Having two sets of eyes allows the engineers more opportunity to catch errors and to remember often-overlooked tasks such as writing unit and integration tests.</li> <li>Pairing allows engineers with different experience and expertise to learn from one another by collaborating and receiving feedback in real-time. Instead of having an engineer work alone on a task for long hours and hit an isolation breaking point, pairing allows the pair to check in with one another.</li> <li>Even something as simple as describing the problem out loud can help uncover issues or bugs in the code.</li> <li>Pairing can help brainstorming as well as validating details such as making the variable names consistent.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#when-to-swarm-program","title":"When to Swarm Program","text":"<p>It is important to know that not every PBI needs to use swarming. Some sprints may not even warrant swarming at all. Swarm when:</p> <ul> <li>The work is complex enough to have collective minds collaborating (not because the quantity of work is more than what would be completed in one sprint).</li> <li>The task that the swarm works on has become (or is in imminent danger of becoming) a blocker to other stories.</li> <li>An unknown is discovered that needs a collaborative effort to form a decision on how to move forward. The collective knowledge and expertise help move the story forward more quickly and ultimately produced better quality code.</li> <li>A conflict or unresolved difference of opinion arises during a pairing session. Promote the work to become a swarming session to help resolve the conflict.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#how-to-swarm-program","title":"How to Swarm Program","text":"<p>As soon the pair finds out that the PBI will warrant swarming, the pair brings it up to the rest of the team (via parking lot during stand-up or asynchronously). Members of the team agree or volunteer to assist.</p> <ul> <li>The story owner (or pairing assignee) sends Teams call invite to the interested parties. This allows the swarm to have dedicated focus time by blocking time in calendars.</li> <li>During a swarming session, an engineer can branch out if there is something that needs to be handled while the swarm tackles the main problem at hand, then reconnects and reports back. This allows the swarm to focus on a core aspect and to be all on the same page.</li> <li>The Teams call is repeated until resolution is found or alternative path forward is formulated.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#why-swarm-programming-helps-collaboration","title":"Why Swarm Programming Helps Collaboration","text":"<ul> <li>Swarming allows the collective knowledge and expertise of the team to come together in a focused and unified way.</li> <li>Not only does swarming help close out the item faster, but it also helps the team understand each other\u2019s strengths and weaknesses.</li> <li>Allows the team to build a higher level of trust and work as a cohesive unit.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#when-to-decide-to-swarm-pair-andor-split","title":"When to Decide to Swarm, Pair, and/or Split","text":"<ul> <li>While a lot of time can be spent on pair programming, it does make sense to split the work when folks understand how the work will be carried out, and the work to be done is largely prescriptive.</li> <li>Once the story has been jointly tasked out by both engineers, the engineers may choose to tackle some tasks separately and then combine the work together at the end.</li> <li>Pair programming is more helpful when the engineers do not have perfect clarity about what is needed to be done or how it can be done.</li> <li>Swarming is done when the two engineers assigned to the story need an additional sounding board or need expertise that other team members could provide.</li> </ul>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#benefits-of-increased-collaboration","title":"Benefits of Increased Collaboration","text":"<p>Knowledge sharing and bringing ISE and customer engineers together in a \u2018code-with\u2019 manner is an important aspect of ISE engagements. This grows both our customers\u2019 and our ISE team\u2019s capability to build on Azure. We are responsible for demonstrating engineering fundamentals and leaving the customer in a better place after we disengage. This can only happen if we collaborate and engage together as a team. In addition to improved software quality, this also adds a beneficial social aspect to the engagements.</p>"},{"location":"lab1/advanced-topics/collaboration/why-collaboration/#resources","title":"Resources","text":"<ul> <li>How to add a pairing custom field in Azure DevOps User Stories - adding a custom field of type Identity in Azure DevOps for pairing</li> <li>On Pair Programming - Martin Fowler</li> <li>Pair Programming hands-on lessons - these can be used (and adapted) to support bringing pair programming into your team (MS internal or including customers)</li> <li>Effortless Pair Programming with GitHub Codespaces and VSCode</li> </ul>"},{"location":"lab1/advanced-topics/effective-organization/delivery-plan/","title":"Delivery Plan","text":""},{"location":"lab1/advanced-topics/effective-organization/delivery-plan/#goals","title":"Goals","text":"<p>While Scrum does not require and discourages planning more than one sprint at a time. Most of us work in enterprises where we are dependent outside teams (for example: marketing, sales, support).</p> <p>A rough assessment of the planned project scope is achievable within a reasonable time frame and resources. The goal is to have a rough plan and estimate as a starting point, not to implement \"Agilefall.\"</p> <p>Note that this is just a starting point to enable planning discussions. We expect the actual schedule to evolve and shift over time and that you will update the scope and timeline as you progress.</p> <p>Delivery Plans ensure your teams are aligning with your organizational goals.</p>"},{"location":"lab1/advanced-topics/effective-organization/delivery-plan/#benefits","title":"Benefits","text":"<ul> <li>As you complete the assessment, you can push back on the scope, time frame or ask for more resources.</li> <li>As you progress in your project/product delivery, you can highlight risks to the scope, time frame, and resources.</li> </ul>"},{"location":"lab1/advanced-topics/effective-organization/delivery-plan/#approach","title":"Approach","text":"<p>One approach you can take to accomplish is with stickies and a spreadsheet.</p> <ol> <li> <p>Stack rank the features for everything in your backlog</p> <p>- Functional Features   - Non-functional Features   - User Research and Design   - Testing   - Documentation   - Knowledge Transfer/Support Processes</p> </li> <li> <p>T-Shirt Features in terms of working weeks per person. In some scenarios, you have no idea how complex the work. In this situation, you can ask for time to conduct a spike (timebox the effort so you can get back on time).</p> </li> <li> <p>Calculate the capacity for the team based on the number of weeks person with his/her start and end date and minus holidays, vacation, conferences, training, and onboarding days. Also, minus time if the person is also working on defects and support.</p> </li> </ol> <p>Based on your capacity, you know have the options</p> <ul> <li>Ask for more resources. Caution: onboarding new resources take time.</li> <li>Reduce the scope to the most MVP.  Caution: as you trim more of the scope, it might not be valuable anymore to the customer. Consider a cupcake which is everything you need. You don't want to skim off the frosting.</li> <li>Ask for more time. Usually, this is the most flexible, but if there is a marketing date that you need to hit, this might be as flexible.</li> </ul>"},{"location":"lab1/advanced-topics/effective-organization/delivery-plan/#tools","title":"Tools","text":"<p>You can also leverage one of these tools by creating your epics and features and add the weeks estimates.</p> <p>The Plans (Preview) feature on Azure DevOps will help you make a plan. Delivery Plans provide a schedule of stories or features your team plan to deliver. Delivery Plans show the scheduled work items by a sprint (iteration path) of selected teams against a calendar view.</p> <p>Confluence JIRA, Trello, Rally, Asana, Basecamp, and GitHub Issues are other similar tools in the market (some are free, others you pay a monthly fee, or you can install on-prem) that you can leverage.</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/","title":"Scrum of Scrums","text":"<p>Scrum of scrums is a technique used to scale Scrum to a larger group working towards the same project goal. In Scrum, we consider a team being too big when going over 10-12 individuals. This should be decided on a case by case basis. If the project is set up in multiple work streams that contain a fixed group of people and a common stand-up meeting is slowing down productivity: scrum of scrums should be considered. The team would identify the different subgroups that would act as a separate scrum teams with their own backlog, board and stand-up.</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/#goals","title":"Goals","text":"<p>The goal of the scrum of scrums ceremony is to give sub-teams the agility they need while not loosing visibility and coordination. It also helps to ensure that the sub-teams are achieving their sprint goals, and they are going in the right direction to achieve the overall project goal.</p> <p>The scrum of scrums ceremony happens every day and can be seen as a regular stand-up:</p> <ul> <li>What was done the day before by the sub-team.</li> <li>What will be done today by the sub-team.</li> <li>What are blockers or other issues for the sub-team.</li> <li>What are the blockers or issues that may impact other sub-teams.</li> </ul> <p>The outcome of the meeting will result in a list of impediments related to coordination of the whole project. Solutions could be: agreeing on interfaces between teams, discussing architecture changes, evolving responsibility boundaries, etc.</p> <p>This list of impediments is usually managed in a separate backlog but does not have to.</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/#participation","title":"Participation","text":"<p>The common guideline is to have on average one person per sub-team to participate in the scrum of scrums. Ideally, the Process Lead of each sub-team would represent them in this ceremony. In some instances, the representative for the day is selected at the end of each sub-team daily stand-up and could change every day. In practice, having a fixed representative tends to be more efficient in the long term.</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/#impact","title":"Impact","text":"<p>This practice is helpful in cases of longer projects and with a larger scope, requiring more people. When having more people, it is usually easier to divide the project in sub-teams. Having a daily scrum of scrums improves communication, lowers the risk of integration issues and increases the project chances of success.</p> <p>When choosing to implement Scrum of Scrums, you need to keep in mind that some team members will have additional meetings to coordinate and participate in. Also: all team members for each sub-team need to be updated on the decisions at a later point to ensure a good flow of information.</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/#measures","title":"Measures","text":"<p>The easiest way to measure the impact is by tracking the time to resolve issues in the scrum of scrums backlog. You can also track issues reported during the retrospective related to global coordination (is it well done? can it be improved?).</p>"},{"location":"lab1/advanced-topics/effective-organization/scrum-of-scrums/#facilitation-guidance","title":"Facilitation Guidance","text":"<p>This should be facilitated like a regular stand-up.</p>"},{"location":"lab1/team-agreements/definition-of-done/","title":"Definition of Done","text":"<p>To close a user story, a sprint, or a milestone it is important to verify that the tasks are complete.</p> <p>The development team should decide together what their Definition of Done is and document this in the project. Below are some examples of checks to verify that the user story, sprint, task is completed.</p>"},{"location":"lab1/team-agreements/definition-of-done/#featureuser-story","title":"Feature/User Story","text":"<ul> <li> Acceptance criteria are met</li> <li> Refactoring is complete</li> <li> Code builds with no error</li> <li> Unit tests are written and pass</li> <li> Existing Unit Tests pass</li> <li> Sufficient diagnostics/telemetry are logged</li> <li> Code review is complete</li> <li> UX review is complete (if applicable)</li> <li> Documentation is updated</li> <li> The feature is merged into the develop branch</li> <li> The feature is signed off by the product owner</li> </ul>"},{"location":"lab1/team-agreements/definition-of-done/#sprint-goal","title":"Sprint Goal","text":"<ul> <li> Definition of Done for all user stories included in the sprint are met</li> <li> Product backlog is updated</li> <li> Functional and Integration tests pass</li> <li> Performance tests pass</li> <li> End 2 End tests pass</li> <li> All bugs are fixed</li> <li> The sprint is signed off from developers, software architects, project manager, product owner etc.</li> </ul>"},{"location":"lab1/team-agreements/definition-of-done/#releasemilestone","title":"Release/Milestone","text":"<ul> <li> Code Complete (goals of sprints are met)</li> <li> Release is marked as ready for production deployment by product owner</li> </ul>"},{"location":"lab1/team-agreements/definition-of-ready/","title":"Definition of Ready","text":"<p>When the development team picks a user story from the top of the backlog, the user story needs to have enough detail to estimate the work needed to complete the story within the sprint. If it has enough detail to estimate, it is Ready to be developed.</p> <p>If a user story is not Ready in the beginning of the Sprint it increases the chance that the story will not be done at the end of this sprint.</p>"},{"location":"lab1/team-agreements/definition-of-ready/#what-it-is","title":"What it is","text":"<p>Definition of Ready is the agreement made by the scrum team around how complete a user story should be in order to be selected as candidate for estimation in the sprint planning. These can be codified as a checklist in user stories using GitHub Issue Templates or Azure DevOps Work Item Templates.</p> <p>It can be understood as a checklist that helps the Product Owner to ensure that the user story they wrote contains all the necessary details for the scrum team to understand the work to be done.</p>"},{"location":"lab1/team-agreements/definition-of-ready/#examples-of-ready-checklist-items","title":"Examples of Ready Checklist Items","text":"<ul> <li> Does the description have the details including any input values required to implement the user story?</li> <li> Does the user story have clear and complete acceptance criteria?</li> <li> Does the user story address the business need?</li> <li> Can we measure the acceptance criteria?</li> <li> Is the user story small enough to be implemented in a short amount of time, but large enough to provide value to the customer?</li> <li> Is the user story blocked? For example, does it depend on any of the following:<ul> <li>The completion of unfinished work</li> <li>A deliverable provided by another team (code artifact, data, etc...)</li> </ul> </li> </ul>"},{"location":"lab1/team-agreements/definition-of-ready/#who-writes-it","title":"Who Writes it","text":"<p>The ready checklist can be written by a Product Owner in agreement with the development team and the Process Lead.</p>"},{"location":"lab1/team-agreements/definition-of-ready/#when-should-a-definition-of-ready-be-updated","title":"When Should a Definition of Ready be Updated","text":"<p>Update or change the definition of ready anytime the scrum team observes that there are missing information in the user stories that recurrently impacts the planning.</p>"},{"location":"lab1/team-agreements/definition-of-ready/#what-should-be-avoided","title":"What Should be Avoided","text":"<p>The ready checklist should contain items that apply broadly. Don't include items or details that only apply to one or two user stories. This may become an overhead when writing the user stories.</p>"},{"location":"lab1/team-agreements/definition-of-ready/#how-to-get-stories-ready","title":"How to get Stories Ready","text":"<p>In the case that the highest priority work is not yet ready, it still may be possible to make forward progress. Here are some strategies that may help:</p> <ul> <li>Backlog Refinement sessions are a good time to validate that high priority user stories are verified to have a clear description, acceptance criteria and demonstrable business value. It is also a good time to breakdown large stories will likely not be completable in a single sprint.</li> <li>Prioritization sessions are a good time to prioritize user stories that unblock other blocked high priority work.</li> <li>Blocked user stories can often be broken down in a way that unblocks a portion of the original stories scope. This is a good way to make forward progress even when some work is blocked.</li> </ul>"},{"location":"lab1/team-agreements/team-manifesto/","title":"Team Manifesto","text":""},{"location":"lab1/team-agreements/team-manifesto/#introduction","title":"Introduction","text":"<p>ISE teams work with a new development team in each customer engagement which requires a phase of introduction &amp; knowledge transfer before starting an engagement.</p> <p>Completion of this phase of ice-breakers and discussions about the standards takes time, but is required to start increasing the learning curve of the new team.</p> <p>A team manifesto is a light-weight one page agile document among team members which summarizes the basic principles and values of the team and aiming to provide a consensus about technical expectations from each team member in order to deliver high quality output at the end of each engagement.</p> <p>It aims to reduce the time on setting the right expectations without arranging longer \"team document reading\" meetings and provide a consensus among team members to answer the question - \"How does the new team develop the software?\" - by covering all engineering fundamentals and excellence topics such as release process, clean coding, testing.</p> <p>Another main goal of writing the manifesto is to start a conversation during the \"manifesto building session\" to detect any differences of opinion around how the team should work.</p> <p>It also serves in the same way when a new team member joins to the team. New joiners can quickly get up to speed on the agreed standards.</p>"},{"location":"lab1/team-agreements/team-manifesto/#how-to-build-a-team-manifesto","title":"How to Build a Team Manifesto","text":"<p>It can be said that the best time to start building it is at the very early phase of the engagement when teams meet with each other for swarming or during the preparation phase.</p> <p>It is recommended to keep team manifesto as simple as possible, so preferably, one-page simple document which doesn't include any references or links is a nice format for it. If there is a need for providing knowledge on certain topics, the way to do is delivering brown-bag sessions, technical katas, team practices, documentations and others later on.</p> <p>A few important points about the team manifesto</p> <ul> <li>The team manifesto is built by the development team itself</li> <li>It should cover all required technical engineering points for the excellence as well as behavioral agility mindset items that the team finds relevant</li> <li>It aims to give a common understanding about the desired expertise, practices and/or mindset within the team</li> <li>Based on the needs of the team and retrospective results, it can be modified during the engagement.</li> </ul> <p>In ISE, we aim for quality over quantity, and well-crafted software as well as to a comfortable/transparent environment where each team member can reach their highest potential.</p> <p>The difference between the team manifesto and other team documents is that it is used to give a short summary of expectations around the technical way of working and supported mindset in the team, before code-with sprints starts.</p> <p>Below, you can find some including, but not limited, topics many teams touch during engagements,</p> Topic What is it about ? Collective Ownership Does team own the code rather than individuals? What is the expectation? Respect Any preferred statement about it's a \"must-have\" team value Collaboration Any preferred statement about how does team want to collaborate ? Transparency A simple statement about it's a \"must-have\" team value and if preferred, how does this being provided by the team ? meetings, retrospective, feedback mechanisms etc. Craftspersonship Which tools such as Git, VS Code LiveShare, etc. are being used ? What is the definition of expected best usage of them? PR sizing What does team prefer in PRs ? Branching Team's branching strategy and standards Commit standards Preferred format in commit messages, rules and more Clean Code Does team follow clean code principles ? Pair/Mob Programming Will team apply pair/mob programming ? If yes, what programming styles are suitable for the team ? Release Process Principles around release process such as quality gates, reviewing process ...etc. Code Review Any rule for code reviewing such as min number of reviewers, team rules ...etc. Action Readiness How the backlog will be refined? How do we ensure clear Definition of Done and Acceptance Criteria ? TDD Will the team follow TDD ? Test Coverage Is there any expected number, percentage or measurement ? Dimensions in Testing Required tests for high quality software, eg : unit, integration, functional, performance, regression, acceptance Build process build for all? or not; The clear statement of where code and under what conditions code should work ? eg : OS, DevOps, tool dependency Bug fix The rules of bug fixing in the team ? eg: contact people, attaching PR to the issue etc. Technical debt How does team manage/follow it? Refactoring How does team manage/follow it? Agile Documentation Does team want to use diagrams and tables more rather than detailed KB articles ? Efficient Documentation When is it necessary ? Is it a prerequisite to complete tasks/PRs etc.? Definition of Fun How will we have fun for relaxing/enjoying the team spirit during the engagement?"},{"location":"lab1/team-agreements/team-manifesto/#tools","title":"Tools","text":"<p>Generally team sessions are enough for building a manifesto and having a consensus around it, and if there is a need for improving it in a structured way, there are many blogs and tools online, any retrospective tool can be used.</p>"},{"location":"lab1/team-agreements/team-manifesto/#resources","title":"Resources","text":"<p>Technical Agility*</p>"},{"location":"lab1/team-agreements/working-agreement/","title":"Sections of a Working Agreement","text":"<p>A working agreement is a document, or a set of documents that describe how we work together as a team and what our expectations and principles are.</p> <p>The working agreement created by the team at the beginning of the project, and is stored in the repository so that it is readily available for everyone working on the project.</p> <p>The following are examples of sections and points that can be part of a working agreement but each team should compose their own, and adjust times, communication channels, branch naming policies etc. to fit their team needs.</p>"},{"location":"lab1/team-agreements/working-agreement/#general","title":"General","text":"<ul> <li>We work as one team towards a common goal and clear scope</li> <li>We make sure everyone's voice is heard, listened to</li> <li>We show all team members equal respect</li> <li>We work as a team to have common expectations for technical delivery that are documented in a Team Manifesto.</li> <li>We make sure to spread our expertise and skills in the team, so no single person is relied on for one skill</li> <li>All times below are listed in CET</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#communication","title":"Communication","text":"<ul> <li>We communicate all information relevant to the team through the Project Teams channel</li> <li>We add all technical spikes, trade studies, and other technical documentation to the project repository through async design reviews in PRs</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#work-life-balance","title":"Work-life Balance","text":"<ul> <li>Our office hours, when we can expect to collaborate via Microsoft Teams, phone or face-to-face are Monday to Friday 10AM - 5PM</li> <li>We are not expected to answer emails past 6PM, on weekends or when we are on holidays or vacation.</li> <li>We work in different time zones and respect this, especially when setting up recurring meetings.</li> <li>We record meetings when possible, so that team members who could not attend live can listen later.</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#quality-and-not-quantity","title":"Quality and not Quantity","text":"<ul> <li>We agree on a Definition of Done for our user story's and sprints and live by it.</li> <li>We follow engineering best practices like the Engineering Fundamentals Engineering Playbook</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#scrum-rhythm","title":"Scrum Rhythm","text":"Activity When Duration Who Accountable Goal Project Standup Tue-Fri 9AM 15 min Everyone Process Lead What has been accomplished, next steps, blockers Sprint Demo Monday 9AM 1 hour Everyone Dev Lead Present work done and sign off on user story completion Sprint Retro Monday 10AM 1 hour Everyone Process Lead Dev Teams shares learnings and what can be improved Sprint Planning Monday 11AM 1 hour Everyone PO Size and plan user stories for the sprint Task Creation After Sprint Planning - Dev Team Dev Lead Create tasks to clarify and determine velocity Backlog refinement Wednesday 2PM 1 hour Dev Lead, PO PO Prepare for next sprint and ensure that stories are ready for next sprint."},{"location":"lab1/team-agreements/working-agreement/#process-lead","title":"Process Lead","text":"<p>The Process Lead is responsible for leading any scrum or agile practices to enable the project to move forward.</p> <ul> <li>Facilitate standup meetings and hold team accountable for attendance and participation.</li> <li>Keep the meeting moving as described in the Project Standup page.</li> <li>Make sure all action items are documented and ensure each has an owner and a due date and tracks the open issues.</li> <li>Notes as needed after planning / stand-ups.</li> <li>Make sure that items are moved to the parking lot and ensure follow-up afterwards.</li> <li>Maintain a location showing team\u2019s work and status and removing impediments that are blocking the team.</li> <li>Hold the team accountable for results in a supportive fashion.</li> <li>Make sure that project and program documentation are up-to-date.</li> <li>Guarantee the tracking/following up on action items from retrospectives (iteration and release planning) and from daily standup meetings.</li> <li>Facilitate the sprint retrospective.</li> <li>Coach Product Owner and the team in the process, as needed.</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#backlog-management","title":"Backlog Management","text":"<ul> <li>We work together on a Definition of Ready and all user stories assigned to a sprint need to follow this</li> <li>We communicate what we are working on through the board</li> <li>We assign ourselves a task when we are ready to work on it (not before) and move it to active</li> <li>We capture any work we do related to the project in a user story/task</li> <li>We close our tasks/user stories only when they are done (as described in the Definition of Done)</li> <li>We work with the PM if we want to add a new user story to the sprint</li> <li>If we add new tasks to the board, we make sure it matches the acceptance criteria of the user story (to avoid scope creep).   If it doesn't match the acceptance criteria we should discuss with the PM to see if we need a new user story for the task or if we should adjust the acceptance criteria.</li> </ul>"},{"location":"lab1/team-agreements/working-agreement/#code-management","title":"Code Management","text":"<ul> <li>We follow the git flow branch naming convention for branches and identify the task number e.g. <code>feature/123-add-working-agreement</code></li> <li>We merge all code into main branches through PRs</li> <li>All PRs are reviewed by one person from  and one from Microsoft (for knowledge transfer and to ensure code and security standards are met) <li>We always review existing PRs before starting work on a new task</li> <li>We look through open PRs at the end of stand-up to make sure all PRs have reviewers.</li> <li>We treat documentation as code and apply the same standards to Markdown as code</li>"},{"location":"lab2/","title":"Testing","text":""},{"location":"lab2/#why-testing","title":"Why Testing","text":"<ul> <li>Tests allow us to find flaws in our software</li> <li>Good tests document the code by describing the intent</li> <li>Automated tests saves time, compared to manual tests</li> <li>Automated tests allow us to safely change and refactor our code without introducing regressions</li> </ul>"},{"location":"lab2/#the-fundamentals","title":"The Fundamentals","text":"<ul> <li>We consider code to be incomplete if it is not accompanied by tests</li> <li>We write unit tests (tests without external dependencies) that can run before every PR merge to validate that we don\u2019t have regressions</li> <li>We write Integration tests/E2E tests that test the whole system end to end, and run them regularly</li> <li>We write our tests early and block any further code merging if tests fail.</li> <li>We run load tests/performance tests where appropriate to validate that the system performs under stress</li> </ul>"},{"location":"lab2/#build-for-testing","title":"Build for Testing","text":"<p>Testing is a critical part of the development process.  It is important to build your application with testing in mind.  Here are some tips to help you build for testing:</p> <ul> <li>Parameterize everything. Rather than hard-code any variables, consider making everything a configurable parameter with a reasonable default. This will allow you to easily change the behavior of your application during testing. Particularly during performance testing, it is common to test different values to see what impact that has on performance. If a range of defaults need to change together, consider one or more parameters which set \"modes\", changing the defaults of a group of parameters together.</li> </ul> <ul> <li>Document at startup. When your application starts up, it should log all parameters. This ensures the person reviewing the logs and application behavior know exactly how the application is configured.</li> </ul> <ul> <li>Log to console. Logging to external systems like Azure Monitor is desirable for traceability across services. This requires logs to be dispatched from the local system to the external system and that is a dependency that can fail. It is important that someone be able to console logs directly on the local system.</li> </ul> <ul> <li>Log to external system. In addition to console logs, logging to an external system like Azure Monitor is desirable for traceability across services and durability of logs.</li> </ul> <ul> <li>Log all activity. If the system is performing some activity (reading data from a database, calling an external service, etc.), it should log that activity. Ideally, there should be a log message saying the activity is starting and another log message saying the activity is complete. This allows someone reviewing the logs to understand what the application is doing and how long it is taking. Depending on how noisy this is, different messages can be associated with different log levels, but it is important to have the information available when it comes to debugging a deployed system.</li> </ul> <ul> <li>Correlate distributed activities. If the system is performing some activity that is distributed across multiple systems, it is important to correlate the activity across those systems. This can be done using a Correlation ID that is passed from system to system. This allows someone reviewing the logs to understand the entire flow of activity. For more information, please see Observability in Microservices.</li> </ul> <ul> <li>Log metadata. When logging, it is important to include metadata that is relevant to the activity. For example, a Tenant ID, Customer ID, or Order ID. This allows someone reviewing the logs to understand the context of the activity and filter to a manageable set of logs.</li> </ul> <ul> <li>Log performance metrics. Even if you are using App Insights to capture how long dependency calls are taking, it is often useful to know long certain functions of your application took. It then becomes possible to evaluate the performance characteristics of your application as it is deployed on different compute platforms with different limitations on CPU, memory, and network bandwidth. For more information, please see Metrics.</li> </ul>"},{"location":"lab2/#map-of-outcomes-to-testing-techniques","title":"Map of Outcomes to Testing Techniques","text":"<p>The table below maps outcomes (the results that you may want to achieve in your validation efforts) to one or more techniques that can be used to accomplish that outcome.</p> When I am working on... I want to get this outcome... ...so I should consider Development Prove backward compatibility with existing callers and clients Shadow testing Development Ensure telemetry is sufficiently detailed and complete to trace and diagnose malfunction in End-to-End testing flows Distributed Debug challenges; Orphaned call chain analysis Development Ensure program logic is correct for a variety of expected, mainline, edge and unexpected inputs Unit testing; Functional tests; Consumer-driven Contract Testing; Integration testing Development Prevent regressions in logical correctness; earlier is better Unit testing; Functional tests; Consumer-driven Contract Testing; Integration testing; Rings (each of these are expanding scopes of coverage) Development Quickly validate mainline correctness of a point of functionality (e.g. single API), manually Manual smoke testing Tools: postman, powershell, curl Development Validate interactions between components in isolation, ensuring that consumer and provider components are compatible and conform to a shared understanding documented in a contract Consumer-driven Contract Testing Development Validate that multiple components function together across multiple interfaces in a call chain, incl network hops Integration testing; End-to-end (End-to-End testing) tests; Segmented end-to-end (End-to-End testing) Development Prove disaster recoverability \u2013 recover from corruption of data DR drills Development Find vulnerabilities in service Authentication or Authorization Scenario (security) Development Prove correct RBAC and claims interpretation of Authorization code Scenario (security) Development Document and/or enforce valid API usage Unit testing; Functional tests; Consumer-driven Contract Testing Development Prove implementation correctness in advance of a dependency or absent a dependency Unit testing (with mocks); Unit testing (with emulators); Consumer-driven Contract Testing Development Ensure that the user interface is accessible Accessibility Development Ensure that users can operate the interface UI testing (automated) (human usability observation) Development Prevent regression in user experience UI automation; End-to-End testing Development Detect and prevent 'noisy neighbor' phenomena Load testing Development Detect availability drops Synthetic Transaction testing; Outside-in probes Development Prevent regression in 'composite' scenario use cases / workflows (e.g. an e-commerce system might have many APIs that used together in a sequence perform a \"shop-and-buy\" scenario) End-to-End testing; Scenario Development; Operations Prevent regressions in runtime performance metrics e.g. latency / cost / resource consumption; earlier is better Rings; Synthetic Transaction testing / Transaction; Rollback Watchdogs Development; Optimization Compare any given metric between 2 candidate implementations or variations in functionality Flighting; A/B testing Development; Staging Prove production system of provisioned capacity meets goals for reliability, availability, resource consumption, performance Load testing (stress); Spike; Soak; Performance testing Development; Staging Understand key user experience performance characteristics \u2013 latency, chattiness, resiliency to network errors Load; Performance testing; Scenario (network partitioning) Development; Staging; Operation Discover melt points (the loads at which failure or maximum tolerable resource consumption occurs) for each individual component in the stack Squeeze; Load testing (stress) Development; Staging; Operation Discover overall system melt point (the loads at which the end-to-end system fails) and which component is the weakest link in the whole stack Squeeze; Load testing (stress) Development; Staging; Operation Measure capacity limits for given provisioning to predict or satisfy future provisioning needs Squeeze; Load testing (stress) Development; Staging; Operation Create / exercise failover runbook Failover drills Development; Staging; Operation Prove disaster recoverability \u2013 loss of data center (the meteor scenario); measure MTTR DR drills Development; Staging; Operation Understand whether observability dashboards are correct, and telemetry is complete; flowing Trace Validation; Load testing (stress); Scenario; End-to-End testing Development; Staging; Operation Measure impact of seasonality of traffic Load testing Development; Staging; Operation Prove Transaction and alerts correctly notify / take action Synthetic Transaction testing (negative cases); Load testing Development; Staging; Operation; Optimizing Understand scalability curve, i.e. how the system consumes resources with load Load testing (stress); Performance testing Operation; Optimizing Discover system behavior over long-haul time Soak Optimizing Find cost savings opportunities Squeeze Staging; Operation Measure impact of failover / scale-out (repartitioning, increasing provisioning) / scale-down Failover drills; Scale drills Staging; Operation Create/Exercise runbook for increasing/reducing provisioning Scale drills Staging; Operation Measure behavior under rapid changes in traffic Spike Staging; Optimizing Discover cost metrics per unit load volume (what factors influence cost at what load points, e.g. cost per million concurrent users) Load (stress) Development; Operation Discover points where a system is not resilient to unpredictable yet inevitable failures (network outage, hardware failure, VM host servicing, rack/switch failures, random acts of the Malevolent Divine, solar flares, sharks that eat undersea cable relays, cosmic radiation, power outages, renegade backhoe operators, wolves chewing on junction boxes, \u2026) Chaos Development Perform unit testing on Power platform custom connectors Custom Connector Testing"},{"location":"lab2/#technology-specific-testing","title":"Technology Specific Testing","text":"<ul> <li>Using DevTest Pattern for building containers with AzDO</li> <li>Using Azurite to run blob storage tests in pipeline</li> </ul>"},{"location":"lab2/cdc-testing/","title":"Consumer-Driven Contract Testing (CDC)","text":"<p>Consumer-driven Contract Testing (or CDC for short) is a software testing methodology used to test components of a system in isolation while ensuring that provider components are compatible with the expectations that consumer components have of them.</p>"},{"location":"lab2/cdc-testing/#why-consumer-driven-contract-testing","title":"Why Consumer-Driven Contract Testing","text":"<p>CDC tries to overcome the several painful drawbacks of automated E2E tests with components interacting together:</p> <ul> <li>E2E tests are slow</li> <li>E2E tests break easily</li> <li>E2E tests are expensive and hard to maintain</li> <li>E2E tests of larger systems may be hard or impossible to run outside a dedicated testing environment</li> </ul> <p>Although testing best practices suggest to write just a few E2E tests compared to the cheaper, faster and more stable integration and unit tests as pictured in the testing pyramid below, experience shows many teams end up writing too many E2E tests. A reason for this is that E2E tests give developers the highest confidence to release as they are testing the \"real\" system.</p> <p></p> <p>CDC addresses these issues by testing interactions between components in isolation using mocks that conform to a shared understanding documented in a \"contract\". Contracts are agreed between consumer and provider, and are regularly verified against a real instance of the provider component. This effectively partitions a larger system into smaller pieces that can be tested individually in isolation of each other, leading to simpler, fast and stable tests that also give confidence to release.</p> <p>Some E2E tests are still required to verify the system as a whole when deployed in the real environment, but most functional interactions between components can be covered with CDC tests.</p> <p>CDC testing was initially developed for testing RESTful API's, but the pattern scales to all consumer-provider systems and tooling for other messaging protocols besides HTTP does exist.</p>"},{"location":"lab2/cdc-testing/#consumer-driven-contract-testing-design-blocks","title":"Consumer-Driven Contract Testing Design Blocks","text":"<p>In a consumer-driven approach the consumer drives changes to contracts between a consumer (the client) and a provider (the server). This may sound counterintuitive, but it helps providers create APIs that fit the real requirements of the consumers rather than trying to guess these in advance. Next we describe the CDC building blocks ordered by their occurrence in the development cycle.</p> <p></p>"},{"location":"lab2/cdc-testing/#consumer-tests-with-provider-mock","title":"Consumer Tests with Provider Mock","text":"<p>The consumers start by creating integration tests against a provider mock and running them as part of their CI pipeline. Expected responses are defined in the provider mock for requests fired from the tests. Through this, the consumer essentially defines the contract they expect the provider to fulfill.</p>"},{"location":"lab2/cdc-testing/#contract","title":"Contract","text":"<p>Contracts are generated from the expectations defined in the provider mock as a result of a successful test run. CDC frameworks like Pact provide a specification for contracts in json format consisting of the list of request/responses generated from the consumer tests plus some additional metadata.</p> <p>Contracts are not a replacement for a discussion between the consumer and provider team. This is the moment where this discussion should take place (if not already done before). The consumer tests and generated contract are refined with the feedback and cooperation of the provider team. Lastly the finalized contract is versioned and stored in a central place accessible by both consumer and provider.</p> <p>Contracts are complementary to API specification documents like OpenAPI. API specifications describe the structure and the format of the API. A contract instead specifies that for a given request, a given response is expected. An API specifications document is helpful in writing an API contract and can be used to validate that the contract conforms to the API specification.</p>"},{"location":"lab2/cdc-testing/#provider-contract-verification","title":"Provider Contract Verification","text":"<p>On the provider side tests are also executed as part of a separate pipeline which verifies contracts against real responses of the provider. Contract verification fails if real responses differ from the expected responses as specified in the contract. The cause of this can be:</p> <ol> <li>Invalid expectations on the consumer side leading to incompatibility with the current provider implementation</li> <li>Broken provider implementation due to some missing functionality or a regression</li> </ol> <p>Either way, thanks to CDC it is easy to pinpoint integration issues down to the consumer/provider of the affected interaction. This is a big advantage compared to the debugging pain this could have been with an E2E test approach.</p>"},{"location":"lab2/cdc-testing/#cdc-testing-frameworks-and-tools","title":"CDC Testing Frameworks and Tools","text":"<p>Pact is an implementation of CDC testing that allows mocking of responses in the consumer codebase, and verification of the interactions in the provider codebase, while defining a specification for contracts. It was originally written in Ruby but has available wrappers for multiple languages. Pact is the de-facto standard to use when working with CDC.</p> <p>Spring Cloud Contract is an implementation of CDC testing from Spring, and offers easy integration in the Spring ecosystem. Support for non-Spring and non-JVM providers and consumers also exists.</p>"},{"location":"lab2/cdc-testing/#conclusion","title":"Conclusion","text":"<p>CDC has several benefits that make it an approach worth considering when dealing with systems composed of multiple components interacting together.</p> <p>Maintenance efforts can be reduced by testing consumer-provider interactions in isolation without the need of a complex integrated environment, specially as the interactions between components grow in number and become more complex.</p> <p></p> <p>Additionally, a close collaboration between consumer and provider teams is strongly encouraged through the CDC development process, which can bring many other benefits. Contracts offer a formal way to document the shared understanding how components interact with each other, and serve as a base for the communication between teams. In a way, the contract repository serves as a live documentation of all consumer-provider interactions of a system.</p> <p>CDC has some drawbacks as well. An extra layer of testing is added requiring a proper investment in education for team members to understand and use CDC correctly.</p> <p>Additionally, the CDC test scope should be considered carefully to prevent blurring CDC with other higher level functional testing layers. Contract tests are not the place to verify internal business logic and correctness of the consumer.</p>"},{"location":"lab2/cdc-testing/#resources","title":"Resources","text":"<ul> <li>Testing pyramid from Kent C. Dodd's blog</li> <li>Pact, a code-first consumer-driven contract testing tool with support for several different programming languages</li> <li>Consumer-driven contracts from Ian Robinson</li> <li>Contract test from Martin Fowler</li> <li>A simple example of using Pact consumer-driven contract testing in a Java client-server application</li> <li>Pact dotnet workshop</li> </ul>"},{"location":"lab2/e2e-testing/","title":"E2E Testing","text":"<p>End-to-end (E2E) testing is a Software testing methodology to test a functional and data application flow consisting of several sub-systems working together from  start to end.</p> <p>At times, these systems are developed in different technologies by different teams or organizations. Finally, they come together to form a functional business application.  Hence, testing a single system would not suffice. Therefore, end-to-end testing verifies the application from start to end putting all its components together.</p> <p></p>"},{"location":"lab2/e2e-testing/#why-e2e-testing","title":"Why E2E Testing","text":"<p>In many commercial software application scenarios, a modern software system consists of its interconnection with multiple sub-systems. These sub-systems can be within the same organization or can be components of different organizations. Also, these sub-systems can have somewhat similar or different lifetime release cycle from the current system. As a result, if there is any failure or fault in any sub-system, it can adversely affect the whole software system leading to its collapse.</p> <p></p> <p>The above illustration is a testing pyramid from Kent C. Dodd's blog which is a combination of the pyramids from Martin Fowler\u2019s blog and the Google Testing Blog.</p> <p>The majority of your tests are at the bottom of the pyramid. As you move up the pyramid, the number of tests gets smaller. Also, going up the pyramid, tests get slower and more expensive to write, run, and maintain. Each type of testing vary for its purpose, application and the areas it's supposed to cover. For more information on comparison analysis of different testing types, please see this ## Unit vs Integration vs System vs E2E Testing document.</p>"},{"location":"lab2/e2e-testing/#e2e-testing-design-blocks","title":"E2E Testing Design Blocks","text":"<p>We will look into all the 3 categories one by one:</p>"},{"location":"lab2/e2e-testing/#user-functions","title":"User Functions","text":"<p>Following actions should be performed as a part of building user functions:</p> <ul> <li>List user initiated functions of the software systems, and their interconnected sub-systems.</li> <li>For any function, keep track of the actions performed as well as Input and Output data.</li> <li>Find the relations, if any between different Users functions.</li> <li>Find out the nature of different user functions i.e. if they are independent or are reusable.</li> </ul>"},{"location":"lab2/e2e-testing/#conditions","title":"Conditions","text":"<p>Following activities should be performed as a part of building conditions based on user functions:</p> <ul> <li>For each and every user functions, a set of conditions should be prepared.</li> <li>Timing, data conditions and other factors that affect user functions can be considered as parameters.</li> </ul>"},{"location":"lab2/e2e-testing/#test-cases","title":"Test Cases","text":"<p>Following factors should be considered for building test cases:</p> <ul> <li>For every scenario, one or more test cases should be created to test each and every functionality of the user functions. If possible, these test cases should be automated through the standard CI/CD build pipeline processes with the track of each successful and failed build in AzDO.</li> <li>Every single condition should be enlisted as a separate test case.</li> </ul>"},{"location":"lab2/e2e-testing/#applying-the-e2e-testing","title":"Applying the E2E Testing","text":"<p>Like any other testing, E2E testing also goes through formal planning, test execution, and closure phases.</p> <p>E2E testing is done with the following steps:</p>"},{"location":"lab2/e2e-testing/#planning","title":"Planning","text":"<ul> <li>Business and Functional Requirement analysis</li> <li>Test plan development</li> <li>Test case development</li> <li>Production like Environment setup for the testing</li> <li>Test data setup</li> <li>Decide exit criteria</li> <li>Choose the testing methods that most applicable to your system. For the definition of the various testing methods, please see Testing Methods document.</li> </ul>"},{"location":"lab2/e2e-testing/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>System Testing should be complete for all the participating systems.</li> <li>All subsystems should be combined to work as a complete application.</li> <li>Production like test environment should be ready.</li> </ul>"},{"location":"lab2/e2e-testing/#test-execution","title":"Test Execution","text":"<ul> <li>Execute the test cases</li> <li>Register the test results and decide on pass and failure</li> <li>Report the Bugs in the bug reporting tool</li> <li>Re-verify the bug fixes</li> </ul>"},{"location":"lab2/e2e-testing/#test-closure","title":"Test Closure","text":"<ul> <li>Test report preparation</li> <li>Evaluation of exit criteria</li> <li>Test phase closure</li> </ul>"},{"location":"lab2/e2e-testing/#test-metrics","title":"Test Metrics","text":"<p>The tracing the quality metrics gives insight about the current status of testing. Some common metrics of E2E testing are:</p> <ul> <li>Test case preparation status: Number of test cases ready versus the total number of test cases.</li> <li>Frequent Test progress: Number of test cases executed in the consistent frequent manner, e.g. weekly, versus a target number of the test cases in the same time period.</li> <li>Defects Status: This metric represents the status of the defects found during testing. Defects should be logged into defect tracking tool (e.g. AzDO backlog) and resolved as per their severity and priority. Therefore, the percentage of open and closed defects as per their severity and priority should be calculated to track this metric. The AzDO Dashboard Query can be used to track this metric.</li> <li>Test environment availability: This metric tracks the duration of the test environment used for end-to-end testing versus its scheduled allocation duration.</li> </ul>"},{"location":"lab2/e2e-testing/#e2e-testing-frameworks-and-tools","title":"E2E Testing Frameworks and Tools","text":""},{"location":"lab2/e2e-testing/#1-gauge-framework","title":"1. Gauge Framework","text":"<p>Gauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:</p> <ul> <li>Simple, flexible and rich syntax based on Markdown.</li> <li>Consistent cross-platform/language support for writing test code.</li> <li>A modular architecture with plugins support.</li> <li>Supports data driven execution and external data sources.</li> <li>Helps you create maintainable test suites.</li> <li>Supports Visual Studio Code, Intellij IDEA, IDE Support.</li> <li>Supports html, json and XML reporting.</li> </ul> <p>Gauge Framework Website</p>"},{"location":"lab2/e2e-testing/#2-robot-framework","title":"2. Robot Framework","text":"<p>Robot Framework is a generic open source automation framework. The framework has easy syntax, utilizing human-readable keywords. Its capabilities can be extended by libraries implemented with Python or Java.</p> <p>Robot shares a lot of the same \"pros\" as Gauge, except the developer tooling and the syntax. In our usage, we found the VS Code Intellisense offered with Gauge to be much more stable than the offerings for Robot. We also found the syntax to be less readable than what Gauge offered. While both frameworks allow for markup based test case definitions, the Gauge syntax reads much more like an English sentence than Robot. Finally, Intellisense is baked into the markup files for Gauge test cases, which will create a function stub for the actual test definition if the developer allows it. The same cannot be said of the Robot Framework.</p> <p>Robot Framework Website</p>"},{"location":"lab2/e2e-testing/#3-testcraft","title":"3. TestCraft","text":"<p>TestCraft is a codeless Selenium test automation platform. Its revolutionary AI technology and unique visual modeling allow for faster test creation and execution while eliminating test maintenance overhead.</p> <p>The testers create fully automated test scenarios without coding. Customers find bugs faster, release more frequently, integrate with the CI/CD approach and improve the overall quality of their digital products. This all creates a complete end-to-end testing experience.</p> <p>Perfecto (TestCraft) Website or get it  from the Visual Studio Marketplace</p>"},{"location":"lab2/e2e-testing/#4-ranorex-studio","title":"4. Ranorex Studio","text":"<p>Ranorex Studio is a complete end-to-end test automation tool for desktop, web, and mobile applications. Create reliable tests fast without any coding at all, or using the full IDE. Use external CSV or Excel files, or a SQL database as inputs to your tests.</p> <p>Run tests in parallel or on a Selenium Grid with built-in Selenium WebDriver. Ranorex Studio integrates with your CI/CD process to shorten your release cycles without sacrificing quality.</p> <p>Ranorex Studio tests also integrate with Azure DevOps (AzDO), which can be run as part of a build pipeline in AzDO.</p> <p>Ranorex Studio Website</p>"},{"location":"lab2/e2e-testing/#5-katalon-studio","title":"5. Katalon Studio","text":"<p>Katalon Studio is an excellent end-to-end automation solution for web, API, mobile, and desktop testing with DevOps support.</p> <p>With Katalon Studio, automated testing can be easily integrated into any CI/CD pipeline to release products faster while guaranteeing high quality. Katalon Studio customizes for users from beginners to experts. Robust functions such as Spying, Recording, Dual-editor interface and Custom Keywords make setting up, creating and maintaining tests possible for users.</p> <p>Built on top of Selenium and Appium, Katalon Studio helps standardize your end-to-end tests standardized. It also complies with the most popular frameworks to work seamlessly with other tools in the automated testing ecosystem.</p> <p>Katalon is endorsed by Gartner, IT professionals, and a large testing community.</p> <p>Note: At the time of this writing, Katalon Studio extension for AzDO was NOT available for Linux.</p> <p>Katalon Studio Website or read about its integration with AzDO</p>"},{"location":"lab2/e2e-testing/#6-bugbugio","title":"6. BugBug.io","text":"<p>BugBug is an easy way to automate tests for web applications. The tool focuses on simplicity, yet allows you to cover all essential test cases without coding. It's an all-in-one solution - you can easily create tests and use the built-in cloud to run them on schedule or from your CI/CD, without changes to your own infrastructure.</p> <p>BugBug is an interesting alternative to Selenium because it's actually a completely different technology. It is based on a Chrome extension that allows BugBug to record and run tests faster than old-school frameworks.</p> <p>The biggest advantage of BugBug is its user-friendliness. Most tests created with BugBug simply work out of the box. This makes it easier for non-technical people to maintain tests - with BugBug you can save money on hiring a QA engineer.</p> <p>BugBug Website</p>"},{"location":"lab2/e2e-testing/#conclusion","title":"Conclusion","text":"<p>Hope you learned various aspects of E2E testing like its processes, metrics, the difference between Unit, Integration  and E2E testing, and the various recommended E2E test frameworks and tools.</p> <p>For any commercial release of the software, E2E test verification plays an important role as it tests the entire application in an environment that exactly imitates real-world users like network communication, middleware and backend services interaction, etc.</p> <p>Finally, the E2E test is often performed manually as the cost of automating such test cases is too high to be afforded by any organization. Having said that, the ultimate goal of each organization is to make the e2e testing as streamlined as possible adding full and semi-automation testing components into the process. Hence, the various E2E testing frameworks and tools listed in this article come to the rescue.</p>"},{"location":"lab2/e2e-testing/#resources","title":"Resources","text":"<ul> <li>Wikipedia: Software testing</li> <li>Wikipedia: Unit testing</li> <li>Wikipedia: Integration testing</li> <li>Wikipedia: System testing</li> </ul>"},{"location":"lab2/e2e-testing/testing-comparison/","title":"Unit vs Integration vs System vs E2E Testing","text":"<p>The table below illustrates the most critical characteristics and differences among Unit, Integration, System, and End-to-End Testing, and when to apply each methodology in a project.</p> Unit Test Integration Test System Testing E2E Test Scope Modules, APIs Modules, interfaces Application, system All sub-systems, network dependencies, services and databases Size Tiny Small to medium Large X-Large Environment Development Integration test QA test Production like Data Mock data Test data Test data Copy of real production data System Under Test Isolated unit test Interfaces and flow data between the modules Particular system as a whole Application flow from start to end Scenarios Developer perspectives Developers and IT Pro tester perspectives Developer and QA tester perspectives End-user perspectives When After each build After Unit testing Before E2E testing and after Unit and Integration testing After System testing Automated or Manual Automated Manual or automated Manual or automated Manual"},{"location":"lab2/e2e-testing/testing-methods/","title":"E2E Testing Methods","text":""},{"location":"lab2/e2e-testing/testing-methods/#horizontal-test","title":"Horizontal Test","text":"<p>This method is used very commonly. It occurs horizontally across the context of multiple applications. Take an example of a data ingest management system.</p> <p></p> <p>The inbound data may be  injected from various sources, but it then \"flatten\" into a horizontal processing pipeline that may include various components, such as a gateway API, data transformation, data validation, storage, etc... Throughout the entire Extract-Transform-Load (ETL) processing, the data flow can be tracked and monitored under the horizontal spectrum with little sprinkles of optional, and thus not important for the overall E2E test case, services, like logging, auditing, authentication.</p>"},{"location":"lab2/e2e-testing/testing-methods/#vertical-test","title":"Vertical Test","text":"<p>In this method, all most critical transactions of any application are verified and evaluated right from the start to finish. Each individual layer of the application is tested starting from top to bottom. Take an example of a web-based application that uses middleware services for reaching back-end resources.</p> <p></p> <p>In such case, each layer (tier) is required to be fully tested in conjunction with the \"connected\" layers above and beneath, in which services \"talk\" to each other during the end to end data flow. All these complex testing scenarios will require proper validation and dedicated automated testing. Thus, this method is much more difficult.</p>"},{"location":"lab2/e2e-testing/testing-methods/#e2e-test-cases-design-guidelines","title":"E2E Test Cases Design Guidelines","text":"<p>Below enlisted are few guidelines that should be kept in mind while designing the test cases for performing E2E testing:</p> <ul> <li>Test cases should be designed from the end user\u2019s perspective.</li> <li>Should focus on testing some existing features of the system.</li> <li>Multiple scenarios should be considered for creating multiple test cases.</li> <li>Different sets of test cases should be created to focus on multiple scenarios of the system.</li> </ul>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/","title":"Gauge Framework","text":"<p>Gauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:</p> <ul> <li>Simple, flexible and rich syntax based on Markdown.</li> <li>Consistent cross-platform/language support for writing test code.</li> <li>A modular architecture with plugins support</li> <li>Extensible through plugins and hackable.</li> <li>Supports data driven execution and external data sources</li> <li>Helps you create maintainable test suites</li> <li>Supports Visual Studio Code, Intellij IDEA, IDE Support</li> </ul>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#what-is-a-specification","title":"What is a Specification","text":"<p>Gauge specifications are written using a Markdown syntax. For example</p> <pre><code># Search for the data blob\n\n## Look for file\n* Goto Azure blob\n</code></pre> <p>In this specification Search for the data blob is the specification heading, Look for file is a scenario with a step Goto Azure blob</p>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#what-is-an-implementation","title":"What is an Implementation","text":"<p>You can implement the steps in a specification using a programming language, for example:</p> <pre><code>from getgauge.python import step\nimport os\nfrom step_impl.utils.driver import Driver\n@step(\"Goto Azure blob\")\ndef gotoAzureStorage():\n  URL = os.getenv('STORAGE_ENDPOINT')\n  Driver.driver.get(URL)\n</code></pre> <p>The Gauge runner reads and runs steps and its implementation for every scenario in the specification and generates a report of passing or failing scenarios.</p> <pre><code># Search for the data blob\n\n## Look for file  \u2714\n\nSuccessfully generated html-report to =&gt; reports/html-report/index.html\nSpecifications:       1 executed      1 passed        0 failed        0 skipped\nScenarios:    1 executed      1 passed        0 failed        0 skipped\n</code></pre>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#re-using-steps","title":"Re-using Steps","text":"<p>Gauge helps you focus on testing the flow of an application. Gauge does this by making steps as re-usable as possible. With Gauge, you don\u2019t need to build custom frameworks using a programming language.</p> <p>For example, Gauge steps can pass parameters to an implementation by using a text with quotes.</p> <pre><code># Search for the data blob\n\n## Look for file\n* Goto Azure blob\n* Search for \"store_data.csv\"\n</code></pre> <p>The implementation can now use \u201cstore_data.csv\u201d as follows</p> <pre><code>from getgauge.python import step\nimport os\n@step(\"Search for &lt;query&gt;\")\ndef searchForQuery(query):\n  write(query)\n  press(\"Enter\")\n\nstep(\"Search for &lt;query&gt;\", (query) =&gt; {\n  write(query);\n  press(\"Enter\");\n</code></pre> <p>You can then re-use this step within or across scenarios with different parameters:</p> <pre><code># Search for the data blob\n\n## Look for Store data #1\n* Goto Azure blob\n* Search for \"store_1.csv\"\n\n## Look for Store data #2\n* Goto Azure blob\n* Search for \"store_2.csv\"\n</code></pre> <p>Or combine more than one step into concepts</p> <pre><code># Search Azure Storage for &lt;query&gt;\n* Goto Azure blob\n* Search for \"store_1.csv\"\n</code></pre> <p>The concept, Search Azure Storage for <code>&lt;query&gt;</code> can be used like a step in a specification</p> <pre><code># Search for the data blob\n\n## Look for Store data #1\n* Search Azure Storage for \"store_1.csv\"\n\n## Look for Store data #2\n* Search Azure Storage for \"store_2.csv\"\n</code></pre>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#data-driven-testing","title":"Data-Driven Testing","text":"<p>Gauge also supports data driven testing using Markdown tables as well as external csv files for example</p> <pre><code># Search for the data blob\n\n| query   |\n|---------|\n| store_1 |\n| store_2 |\n| store_3 |\n\n## Look for stores data\n* Search Azure Storage for &lt;query&gt;\n</code></pre> <p>This will execute the scenario for all rows in the table.</p> <p>In the examples above, we refactored a specification to be concise and flexible without changing the implementation.</p>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#other-features","title":"Other Features","text":"<p>This is brief introduction to a few Gauge features. Please refer to the Gauge documentation for additional features such as:</p> <ul> <li>Reports</li> <li>Tags</li> <li>Parallel execution</li> <li>Environments</li> <li>Screenshots</li> <li>Plugins</li> <li>And much more</li> </ul>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#installing-gauge","title":"Installing Gauge","text":"<p>This getting started guide takes you through the core features of Gauge. By the end of this guide, you\u2019ll be able to install Gauge and learn how to create your first Gauge test automation project.</p>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#installation-instructions-for-windows-os","title":"Installation Instructions for Windows OS","text":""},{"location":"lab2/e2e-testing/recipes/gauge-framework/#step-1-installing-gauge-on-windows","title":"Step 1: Installing Gauge on Windows","text":"<p>This section gives specific instructions on setting up Gauge in a Microsoft Windows environment. Download the following installation bundle to get the latest stable release of Gauge.</p>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#step-2-installing-gauge-extension-for-visual-studio-code","title":"Step 2: Installing Gauge Extension for Visual Studio Code","text":"<p>Follow the steps to add the Gauge Visual Studio Code plugin from the IDE</p> <ol> <li>Install the following Gauge extension for Visual Studio Code.</li> </ol>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#troubleshooting-installation","title":"Troubleshooting Installation","text":"<p>If, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:</p> <pre><code>python.exe -m pip install getgauge==0.3.7 --user\n</code></pre>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#installation-instructions-for-macos","title":"Installation Instructions for macOS","text":""},{"location":"lab2/e2e-testing/recipes/gauge-framework/#step-1-installing-gauge-on-macos","title":"Step 1: Installing Gauge on macOS","text":"<p>This section gives specific instructions on setting up Gauge in a macOS environment.</p> <ol> <li>Install brew if you haven\u2019t already: Go to the brew website, and follow the directions there.</li> <li> <p>Run the brew command to install Gauge</p> <pre><code>&gt; brew install gauge\n</code></pre> <p>if HomeBrew is working properly, you should see something similar to the following:</p> </li> </ol> <pre><code>==&gt; Fetching gauge\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gauge/manifests/1.4.3\n######################################################################## 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gauge/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893\n==&gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893?se=2022-12-13T12%3A35%3A00Z&amp;sig=I78SuuwNgSMFoBTT\n######################################################################## 100.0%\n==&gt; Pouring gauge--1.4.3.ventura.bottle.tar.gz\n    /usr/local/Cellar/gauge/1.4.3: 6 files, 18.9MB\n</code></pre>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#step-2-installing-gauge-extension-for-visual-studio-code_1","title":"Step 2 : Installing Gauge Extension for Visual Studio Code","text":"<p>Follow the steps to add the Gauge Visual Studio Code plugin from the IDE</p> <ol> <li>Install the following Gauge extension for Visual Studio Code.</li> </ol>"},{"location":"lab2/e2e-testing/recipes/gauge-framework/#post-installation-troubleshooting","title":"Post-Installation Troubleshooting","text":"<p>If, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:</p> <pre><code>python.exe -m pip install getgauge==0.3.7 --user\n</code></pre>"},{"location":"lab2/e2e-testing/recipes/postman-testing/","title":"Postman Testing","text":"<p>This purpose of this document is to provide guidance on how to use Newman in your CI/CD pipeline to run End-to-end (E2E) tests defined in Postman Collections while following security best practices.</p> <p>First, we'll introduce Postman and Newman and then outline several Postman testing use cases that answer why you may want to go beyond local testing with Postman Collections.</p> <p>In the final use case, we are looking to use a shell script that references the Postman Collection file path and Environment file path as inputs to Newman. Below is a flow diagram representing the outcome of the final use case:</p> <p></p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#postman-and-newman","title":"Postman and Newman","text":"<p>Postman is a free API platform for testing APIs. Key features highlighted in this guidance include:</p> <ul> <li>Postman Collections</li> <li>Postman Environment Files</li> <li>Postman Scripts</li> </ul> <p>Newman is a command-line Collection Runner for Postman. It enables you to run and test a Postman Collection directly from the command line. Key features highlighted in this guidance include:</p> <ul> <li>Newman Run Command</li> </ul>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#what-is-a-collection","title":"What is a Collection","text":"<p>A Postman Collection is a group of executable saved requests. A collection can be exported as a json file.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#what-is-an-environment-file","title":"What is an Environment File","text":"<p>A Postman Environment file holds environment variables that can be referenced by a valid Postman Collection.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#what-is-a-postman-script","title":"What is a Postman Script","text":"<p>A Postman Script is Javascript hosted within a Postman Collection that can be written to execute against your Postman Collection and Environment File.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#what-is-the-newman-run-command","title":"What is the Newman Run Command","text":"<p>A Newman CLI command that allows you to specify a Postman Collection to be run.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#installing-postman-and-newman","title":"Installing Postman and Newman","text":"<p>For specific instruction on installing Postman, visit the Downloads Postman page.</p> <p>For specific instruction on installing Newman, visit the NPMJS Newman package page.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#implementing-automated-end-to-end-e2e-tests-with-postman-collections","title":"Implementing Automated End-to-end (E2E) Tests With Postman Collections","text":"<p>In order to provide guidance on implementing automated E2E tests with Postman, the section below begins with a use case that explains the trade-offs a dev or QA analyst might face when intending to use Postman for early testing. Each use case represents scenarios that facilitate the end goal of automated E2E tests.</p>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#use-case-hands-on-functional-testing-of-endpoints","title":"Use Case - Hands-on Functional Testing Of Endpoints","text":"<p>A developer or QA analyst would like to locally test input data against API services all sharing a common oauth2 token. As a result, they use Postman to craft an API test suite of Postman Collections that can be locally executed against individual endpoints across environments. After validating that their Postman Collection works, they share it with their team.</p> <p>Steps may look like the following:</p> <ol> <li> <p>For each of your existing API services, use the Postman IDE's import feature to import its OpenAPI Spec (Swagger) as a Postman Collection.</p> <p>If a service is not already using Swagger, look for language specific guidance on how to use Swagger to generate an OpenAPI Spec for your service. Finally, if your service only has a few endpoints, read Postman docs for guidance on how to manually build a Postman Collection.</p> </li> <li> <p>Provide extra clarity about a request in a Postman Collection by using Postman's Example feature to save its responses as examples. You can also simply add an example manually. Please read Postman docs for guidance on how to specify examples.</p> </li> <li>Combine each Postman Collection into a centralized Postman Collection.</li> <li>Build Postman Environment files (local, Dev and/or QA) and parameterize all saved requests of the Postman Collection in a way that references the Postman Environment files.</li> <li> <p>Use the Postman Script feature to create a shared prefetch script that automatically refreshes expired auth tokens per saved request. This would require referencing secrets from a Postman Environment file.</p> <pre><code>// Please treat this as pseudocode, and adjust as necessary.\n\n/* The request to an oauth2 authorization endpoint that will issue a token \nbased on provided credentials.*/\nconst oauth2Request = POST {...};\nvar getToken = true;\nif (pm.environment.get('ACCESS_TOKEN_EXPIRY') &lt;= (new Date()).getTime()) {\n    console.log('Token is expired')\n} else {\n    getToken = false;\n    console.log('Token and expiry date are all good');\n}\nif (getToken === true) {\n    pm.sendRequest(oauth2Request, function (_, res) {\n            console.log('Save the token')\n            var responseJson = res.json();\n            pm.environment.set('token', responseJson.access_token)\n            console.log('Save the expiry date')\n            var expiryDate = new Date();\n            expiryDate.setSeconds(expiryDate.getSeconds() + responseJson.expires_in);\n            pm.environment.set('ACCESS_TOKEN_EXPIRY', expiryDate.getTime());\n    });\n}\n</code></pre> </li> <li> <p>Use Postman IDE to exercise endpoints.</p> </li> <li>Export collection and environment files then remove any secrets before committing to your repo.</li> </ol> <p>Starting with this approach has the following upsides:</p> <ul> <li>You've set yourself up for the beginning stages of an E2E postman collection by aggregating the collections into a single file and using environment files to make it easier to switch environments.</li> <li>Token is refreshed automatically on every call in the collection. This saves you time normally lost from manually having to request a token that expired.</li> <li>Grants QA/Dev granular control of submitting combinations of input data per endpoint.</li> <li>Grants developers a common experience via Postman IDE features.</li> </ul> <p>Ending with this approach has the following downsides:</p> <ul> <li>Promotes unsafe sharing of secrets. Credentials needed to request JWT token in the prefetch script are being manually shared.</li> <li>Secrets may happen to get exposed in the git commit history for various reasons (ex. Sharing the exported Postman Environment files).</li> <li>Collections can only be used locally to hit APIs (local or deployed). Not CI based.</li> <li>Each developer has to keep both their Postman Collection and Postman environment file(s) updated in order to keep up with latest changes to deployed services.</li> </ul>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#use-case-hands-on-functional-testing-of-endpoints-with-azure-key-vault-and-azure-app-config","title":"Use Case - Hands-on Functional Testing Of Endpoints with Azure Key Vault and Azure App Config","text":"<p>A developer or QA analyst may have an existing API test suite of Postman Collections, however, they now want to discourage unsafe sharing of secrets. As a result, they build a script that connects to both Key Vault and Azure App Config in order to automatically generate Postman Environment files instead of checking them into a shared repository.</p> <p>Steps may look like the following:</p> <ol> <li>Create an Azure Key Vault and store authentication secrets per environment:     - <code>\"Key:value\"</code> (ex. <code>\"dev-auth-password:12345\"</code>)     - <code>\"Key:value\"</code> (ex. <code>\"qa-auth-password:12345\"</code>)</li> <li>Create a shared Azure App Configuration instance and save all your Postman environment variables. This instance will be dedicated to holding all your Postman environment variables:     &gt; NOTE: Use the Label feature to delineate between environments.     - <code>\"Key:value\" -&gt; \"apiRoute:url\"</code> (ex. <code>\"servicename:https://servicename.net\" &amp; Label = \"QA\"</code>)     - <code>\"Key:value\" -&gt; \"Header:value\"</code>(ex. <code>\"token: \" &amp; Label = \"QA\"</code>)     - <code>\"Key:value\" -&gt; \"KeyVaultKey:KeyVaultSecret\"</code> (ex. <code>\"authpassword:qa-auth-password\" &amp; Label = \"QA\"</code>)</li> <li>Install Powershell or Bash. Powershell works for both Azure Powershell and Azure CLI.</li> <li> <p>Download Azure CLI, login to the appropriate subscription and ensure you have access to the appropriate resources. Some helpful commands are below:</p> <pre><code># login to the appropriate subscription\naz login\n# validate login\naz account show\n# validate access to Key Vault\naz keyvault secret list --vault-name \"$KeyvaultName\"\n# validate access to App Configuration\naz appconfig kv list --name \"$AppConfigName\"\n</code></pre> </li> <li> <p>Build a script that automatically generates your environment files.     &gt; Note: App Configuration references Key Vault, however, your script is responsible for authenticating properly to both App Configuration and Key Vault. The two services don't communicate directly.</p> <p>```powershell (CreatePostmanEnvironmentFiles.ps1)   # Please treat this as pseudocode, and adjust as necessary.   ############################################################</p> <p>env = $arg1   # 1. list app config vars for an environment   envVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json   # 2. step through envVars array to get Key Vault uris   keyvaultURI = \"\"   $envVars | % {if($.key -eq 'password'){keyvaultURI = $.value}}    # 3. parse uris for Key Vault name and secret names   # 4. get secret from Key Vault   kvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"   # 5. set password value to returned Key Vault secret   $envVars | % {if($.key -eq 'password'){$.value=$kvsecret}}   # 6. create environment file   envFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }   foreach($var in $envVars){           $envFile.values += @{ key = $var.key; value = $var.value; }   }   $envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII -FilePath .\\$env.postman_environment.json   ```</p> </li> <li> <p>Use Postman IDE to import the Postman Environment files to be referenced by your collection.</p> </li> </ol> <p>This approach has the following upsides:</p> <ul> <li>Inherits all the upsides of the previous case.</li> <li>Discourages unsafe sharing of secrets. Secrets are now pulled from Key Vault via Azure CLI. Key Vault Uri also no longer needs to be shared for access to auth tokens.</li> <li>Single source of truth for Postman Environment files. There's no longer a need to share them via repo.</li> <li>Developer only has to manage a single Postman Collection.</li> </ul> <p>Ending with this approach has the following downsides:</p> <ul> <li>Secrets may happen to get exposed in the git commit history if .gitIgnore is not updated to ignore Postman Environment files.</li> <li>Collections can only be used locally to hit APIs (local or deployed). Not CI based.</li> </ul>"},{"location":"lab2/e2e-testing/recipes/postman-testing/#use-case-e2e-testing-with-continuous-integration-and-newman","title":"Use Case - E2E Testing with Continuous Integration and Newman","text":"<p>A developer or QA analyst may have an existing API test suite of local Postman Collections that follow security best practices for development, however, they now want E2E tests to run as part of automated CI pipeline. With the advent of Newman, you can now more readily use Postman to craft an API test suite executable in your CI.</p> <p>Steps may look like the following:</p> <ol> <li>Update your Postman Collection to use the Postman Test feature in order to craft test assertions that will cover all saved requests E2E. Read Postman docs for guidance on how to use the Postman Test feature.</li> <li> <p>Locally use Newman to validate tests are working as intended</p> <pre><code>newman run tests\\e2e_Postman_collection.json -e qa.postman_environment.json\n</code></pre> </li> <li> <p>Build a script that automatically executes Postman Test assertions via Newman and Azure CLI.     &gt; NOTE: An Azure Service Principal must be setup to continue using azure cli in this CI pipeline example.</p> <p>```powershell (RunPostmanE2eTests.ps1)   # Please treat this as pseudocode, and adjust as necessary.   ############################################################</p> <p># 1. login to Azure using a Service Principal   az login --service-principal -u $APP_ID -p $AZURE_SECRET --tenant $AZURE_TENANT   # 2. list app config vars for an environment   envVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json   # 3. step through envVars array to get Key Vault uris   keyvaultURI = \"\"   @envVars | % {if($.key -eq 'password'){keyvaultURI = $.value}}   # 4. parse uris for Key Vault name and secret names   # 5. get secret from Key Vault   kvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"   # 6. set password value to returned Key Vault secret   $envVars | % {if($.key -eq 'password'){$.value=$kvsecret}}   # 7. create environment file   envFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }   foreach($var in $envVars){           $envFile.values += @{ key = $var.key; value = $var.value; }   }   $envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII $env.postman_environment.json   # 8. install Newman   npm install --save-dev newman   # 9. run automated E2E tests via Newman   node_modules.bin\\newman run tests\\e2e_Postman_collection.json -e $env.postman_environment.json   ```</p> </li> <li> <p>Create a yaml file and define a step that will run your test script. (ex. A yaml file targeting Azure Devops that runs a Powershell script.)</p> <pre><code># Please treat this as pseudocode, and adjust as necessary.\n############################################################\ndisplayName: 'Run Postman E2E tests'\ninputs:\n    targetType: 'filePath'\n    filePath: RunPostmanE2eTests.ps1\nenv:\n    APP_ID: $(environment.appId) # credentials for az cli\n    AZURE_SECRET: $(environment.secret)\n    AZURE_TENANT: $(environment.tenant)\n</code></pre> </li> </ol> <p>This approach has the following upside:</p> <ul> <li>E2E tests can now be run automatically as part of a CI pipeline.</li> </ul> <p>Ending with this approach has the following downside:</p> <ul> <li>Postman Environment files are no longer being output to a local environment for hands-on manual testing. However, this can be solved by managing 2 scripts.</li> </ul>"},{"location":"lab2/fault-injection-testing/","title":"Fault Injection Testing","text":"<p>Fault injection testing is the deliberate introduction of errors and faults to a system to validate and harden its stability and reliability. The goal is to improve the system's design for resiliency and performance under intermittent failure conditions over time.</p>"},{"location":"lab2/fault-injection-testing/#when-to-use","title":"When To Use","text":""},{"location":"lab2/fault-injection-testing/#problem-addressed","title":"Problem Addressed","text":"<p>Systems need to be resilient to the conditions that caused inevitable production disruptions. Modern applications are built with an increasing number of dependencies; on infrastructure, platform, network, 3rd party software or APIs, etc. Such systems increase the risk of impact from dependency disruptions. Each dependent component may fail. Furthermore, its interactions with other components may propagate the failure.</p> <p>Fault injection methods are a way to increase coverage and validate software robustness and error handling, either at build-time or at run-time, with the intention of \"embracing failure\" as part of the development lifecycle. These methods assist engineering teams in designing and continuously validating for failure, accounting for known and unknown failure conditions, architect for redundancy, employ retry and back-off mechanisms, etc.</p>"},{"location":"lab2/fault-injection-testing/#applicable-to","title":"Applicable to","text":"<ul> <li>Software - Error handling code paths, in-process memory management.<ul> <li>Example tests: Edge-case unit/integration tests and/or load tests (i.e. stress and soak).</li> </ul> </li> <li>Protocol - Vulnerabilities in communication interfaces such as command line parameters or APIs.<ul> <li>Example tests: Fuzzing provides invalid, unexpected, or random data as input we can assess the level of protocol stability of a component.</li> </ul> </li> <li>Infrastructure - Outages, networking issues, hardware failures.<ul> <li>Example tests: Using different methods to cause fault in the underlying infrastructure such as Shut down virtual machine (VM) instances, crash processes, expire certificates, introduce network latency, etc. This level of testing relies on statistical metrics observations over time and measuring the deviations of its observed behavior during fault, or its recovery time.</li> </ul> </li> </ul>"},{"location":"lab2/fault-injection-testing/#how-to-use","title":"How to Use","text":""},{"location":"lab2/fault-injection-testing/#architecture","title":"Architecture","text":""},{"location":"lab2/fault-injection-testing/#terminology","title":"Terminology","text":"<ul> <li>Fault - The adjudged or hypothesized cause of an error.</li> <li>Error - That part of the system state that may cause a subsequent failure.</li> <li>Failure - An event that occurs when the delivered service deviates from correct state.</li> <li>Fault-Error-Failure cycle - A key mechanism in dependability: A fault may cause an error. An error may cause further errors within the system boundary; therefore each new error acts as a fault. When error states are observed at the system boundary, they are termed failures.</li> </ul>"},{"location":"lab2/fault-injection-testing/#fault-injection-testing-basics","title":"Fault Injection Testing Basics","text":"<p>Fault injection is an advanced form of testing where the system is subjected to different failure modes, and where the testing engineer may know in advance what is the expected outcome, as in the case of release validation tests, or in an exploration to find potential issues in the product, which should be mitigated.</p>"},{"location":"lab2/fault-injection-testing/#fault-injection-and-chaos-engineering","title":"Fault Injection and Chaos Engineering","text":"<p>Fault injection testing is a specific approach to testing one condition. It introduces a failure into a system to validate its robustness. Chaos engineering, coined by Netflix, is a practice for generating new information. There is an overlap in concerns and often in tooling between the terms, and many times chaos engineering uses fault injection to introduce the required effects to the system.</p>"},{"location":"lab2/fault-injection-testing/#high-level-step-by-step","title":"High-level Step-by-Step","text":""},{"location":"lab2/fault-injection-testing/#fault-injection-testing-in-the-development-cycle","title":"Fault injection testing in the development cycle","text":"<p>Fault injection is an effective way to find security bugs in software, so much so that the Microsoft Security Development Lifecycle requires fuzzing at every untrusted interface of every product and penetration testing which includes introducing faults to the system, to uncover potential vulnerabilities resulting from coding errors, system configuration faults, or other operational deployment weaknesses.</p> <p>Automated fault injection coverage in a CI pipeline promotes a Shift-Left approach of testing earlier in the lifecycle for potential issues. Examples of performing fault injection during the development lifecycle:</p> <ul> <li>Using fuzzing tools in CI.</li> <li>Execute existing end-to-end scenario tests (such as integration or stress tests), which are augmented with fault injection.</li> <li>Write regression and acceptance tests based on issues that were found and fixed or based on resolved service incidents.</li> <li>Ad-hoc (manual) validations of fault in the dev environment for new features.</li> </ul>"},{"location":"lab2/fault-injection-testing/#fault-injection-testing-in-the-release-cycle","title":"Fault Injection Testing in the Release Cycle","text":"<p>Much like Synthetic Monitoring Tests, fault injection testing in the release cycle is a part of Shift-Right testing approach, which uses safe methods to perform tests in a production or pre-production environment. Given the nature of distributed, cloud-based applications, it is very difficult to simulate the real behavior of services outside their production environment. Testers are encouraged to run tests where it really matters, on a live system with customer traffic.</p> <p>Fault injection tests rely on metrics observability and are usually statistical; The following high-level steps provide a sample of practicing fault injection and chaos engineering:</p> <ul> <li>Measure and define a steady (healthy) state for the system's interoperability.</li> <li>Create hypotheses based on predicted behavior when a fault is introduced.</li> <li>Introduce real-world fault-events to the system.</li> <li>Measure the state and compare it to the baseline state.</li> <li>Document the process and the observations.</li> <li>Identify and act on the result.</li> </ul>"},{"location":"lab2/fault-injection-testing/#fault-injection-testing-in-kubernetes","title":"Fault Injection Testing in Kubernetes","text":"<p>With the advancement of kubernetes (k8s) as the infrastructure platform, fault injection testing in kubernetes has become inevitable to ensure that system behaves in a reliable manner in the event of a fault or failure. There could be different type of workloads running within a k8s cluster which are written in different languages. For eg. within a K8s cluster, you can run a micro service, a web app and/or a scheduled job. Hence you need to have mechanism to inject fault into any kind of workloads running within the cluster. In addition, kubernetes clusters are managed differently from traditional infrastructure. The tools used for fault injection testing within kubernetes should have compatibility with k8s infrastructure. These are the main characteristics which are required:</p> <ul> <li>Ease of injecting fault into kubernetes pods.</li> <li>Support for faster tool installation within the cluster.</li> <li>Support for YAML based configurations which works well with kubernetes.</li> <li>Ease of customization to add custom resources.</li> <li>Support for workflows to deploy various workloads and faults.</li> <li>Ease of maintainability of the tool</li> <li>Ease of integration with telemetry</li> </ul>"},{"location":"lab2/fault-injection-testing/#best-practices-and-advice","title":"Best Practices and Advice","text":"<p>Experimenting in production has the benefit of running tests against a live system with real user traffic, ensuring its health, or building confidence in its ability to handle errors gracefully. However, it has the potential to cause unnecessary customer pain. A test can either succeed or fail. In the event of failure, there will likely be some impact on the production environment. Thinking about the Blast Radius of the effect, should the test fail, is a crucial step to conduct beforehand. The following practices may help minimize such risk:</p> <ul> <li>Run tests in a non-production environment first. Understand how the system behaves in a safe environment, using synthetic workload, before introducing potential risk to customer traffic.</li> <li>Use fault injection as gates in different stages through the CD pipeline.</li> <li>Deploy and test on Blue/Green and Canary deployments. Use methods such as traffic shadowing (a.k.a. Dark Traffic) to get customer traffic to the staging slot.</li> <li>Strive to achieve a balance between collecting actual result data while affecting as few production users as possible.</li> <li>Use defensive design principles such as circuit breaking and the bulkhead patterns.</li> <li>Agreed on a budget (in terms of Service Level Objective (SLO)) as an investment in chaos and fault injection.</li> <li>Grow the risk incrementally - Start with hardening the core and expand out in layers. At each point, progress should be locked in with automated regression tests.</li> </ul>"},{"location":"lab2/fault-injection-testing/#fault-injection-testing-frameworks-and-tools","title":"Fault Injection Testing Frameworks and Tools","text":""},{"location":"lab2/fault-injection-testing/#fuzzing","title":"Fuzzing","text":"<ul> <li>OneFuzz - is a Microsoft open-source self-hosted fuzzing-as-a-service platform which is easy to integrate into CI pipelines.</li> <li>AFL and WinAFL - Popular fuzz tools by Google's project zero team which is used locally to target binaries on Linux or Windows.</li> <li>WebScarab - A web-focused fuzzer owned by OWASP which can be found in Kali linux distributions.</li> </ul>"},{"location":"lab2/fault-injection-testing/#chaos","title":"Chaos","text":"<ul> <li>Azure Chaos Studio - An in-preview tool for orchestrating controlled fault injection experiments on Azure resources.</li> <li>Chaos toolkit - A declarative, modular chaos platform with many extensions, including the Azure actions and probes kit.</li> <li>Kraken - An Openshift-specific chaos tool, maintained by Redhat.</li> <li>Chaos Monkey - The Netflix platform which popularized chaos engineering (doesn't support Azure OOTB).</li> <li>Simmy - A .NET library for chaos testing and fault injection integrated with the Polly library for resilience engineering.</li> <li>Litmus - A CNCF open source tool for chaos testing and fault injection for kubernetes cluster.</li> <li>This ISE dev blog post provides code snippets as an example of how to use Polly and Simmy to implement a hypothesis-driven approach to resilience and chaos testing.</li> </ul>"},{"location":"lab2/fault-injection-testing/#conclusion","title":"Conclusion","text":"<p>From the principals of chaos: \"The harder it is to disrupt the steady-state, the more confidence we have in the behavior of the system. If a weakness is uncovered, we now have a target for improvement before that behavior manifests in the system at large\".</p> <p>Fault injection techniques increase resilience and confidence in the products we ship. They are used across the industry to validate applications and platforms before and while they are delivered to customers. Fault injection is a powerful tool and should be used with caution. Cases such as the Cloudflare 30 minute global outage, which was caused due to a deployment of code that was meant to be \u201cdark launched\u201d, entail the importance of curtailing the blast radius in the system during experiments.</p>"},{"location":"lab2/fault-injection-testing/#resources","title":"Resources","text":"<ul> <li>Mark Russinovich's fault injection and chaos engineering blog post</li> <li>Cindy Sridharan's Testing in production blog post</li> <li>Cindy Sridharan's Testing in production blog post cont.</li> <li>Fault injection in Azure Search</li> <li>Azure Architecture Framework - Chaos engineering</li> <li>Azure Architecture Framework - Testing resilience</li> <li>Landscape of Software Failure Cause Models</li> </ul>"},{"location":"lab2/integration-testing/","title":"Integration Testing","text":"<p>Integration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behavior between two or more units of code.</p>"},{"location":"lab2/integration-testing/#why-integration-testing","title":"Why Integration Testing","text":"<p>Because one component of a system may be developed independently or in isolation of another it is important to verify the interaction of some or all components. A complex system may be composed of databases, APIs, interfaces, and more, that all interact with each other or additional external systems. Integration tests expose system-level issues such as broken database schemas or faulty third-party API integration. It ensures higher test coverage and serves as an important feedback loop throughout development.</p>"},{"location":"lab2/integration-testing/#integration-testing-design-blocks","title":"Integration Testing Design Blocks","text":"<p>Consider a banking application with three modules: login, transfers, and current balance, all developed independently. An integration test may verify when a user logs in they are re-directed to their current balance with the correct amount for the specific mock user. Another integration test may perform a transfer of a specified amount of money. The test may confirm there are sufficient funds in the account to perform the transfer, and after the transfer the current balance is updated appropriately for the mock user. The login page may be mocked with a test user and mock credentials if this module is not completed when testing the transfers module.</p> <p>Integration testing is done by the developer or QA tester. In the past, integration testing always happened after unit and before system and E2E testing. Compared to unit-tests, integration tests are fewer in quantity, usually run slower, and are more expensive to set up and develop. Now, if a team is following agile principles, integration tests can be performed before or after unit tests, early and often, as there is no need to wait for sequential processes. Additionally, integration tests can utilize mock data in order to simulate a complete system. There is an abundance of language-specific testing frameworks that can be used throughout the entire development lifecycle.</p> <p>It is important to note the difference between integration and acceptance testing. Integration testing confirms a group of components work together as intended from a technical perspective, while acceptance testing confirms a group of components work together as intended from a business scenario.</p>"},{"location":"lab2/integration-testing/#applying-integration-testing","title":"Applying Integration Testing","text":"<p>Prior to writing integration tests, the engineers must identify the different components of the system, and their intended behaviors and inputs and outputs. The architecture of the project must be fully documented or specified somewhere that can be readily referenced (e.g., the architecture diagram).</p> <p>There are two main techniques for integration testing.</p>"},{"location":"lab2/integration-testing/#big-bang","title":"Big Bang","text":"<p>Big Bang integration testing is when all components are tested as a single unit. This is best for small system as a system too large may be difficult to localize for potential errors from failed tests. This approach also requires all components in the system under test to be completed which may delay when testing begins.</p> <p></p>"},{"location":"lab2/integration-testing/#incremental-testing","title":"Incremental Testing","text":"<p>Incremental testing is when two or more components that are logically related are tested as a unit. After testing the unit, additional components are combined and tested all together. This process repeats until all necessary components are tested.</p>"},{"location":"lab2/integration-testing/#top-down","title":"Top Down","text":"<p>Top down testing is when higher level components are tested following the control flow of a software system. In the scenario, what is commonly referred to as stubs are used to emulate the behavior of lower level modules not yet complete or merged in the integration test.</p> <p></p>"},{"location":"lab2/integration-testing/#bottom-up","title":"Bottom Up","text":"<p>Bottom up testing is when lower level modules are tested together. In the scenario, what is commonly referred to as drivers are used to emulate the behavior of higher level modules not yet complete or included in the integration test.</p> <p></p> <p>A third approach known as the sandwich or hybrid model combines the bottom up and town down approaches to test lower and higher level components at the same time.</p>"},{"location":"lab2/integration-testing/#things-to-avoid","title":"Things to Avoid","text":"<p>There is a tradeoff a developer must make between integration test code coverage and engineering cycles. With mock dependencies, test data, and multiple environments at test, too many integration tests are infeasible to maintain and become increasingly less meaningful. Too much mocking will slow down the test suite, make scaling difficult, and may be a sign the developer should consider other tests for the scenario such as acceptance or E2E.</p> <p>Integration tests of complex systems require high maintenance. Avoid testing business logic in integration tests by keeping test suites separate. Do not test beyond the acceptance criteria of the task and be sure to clean up any resources created for a given test. Additionally, avoid writing tests in a production environment. Instead, write them in a scaled-down copy environment.</p>"},{"location":"lab2/integration-testing/#integration-testing-frameworks-and-tools","title":"Integration Testing Frameworks and Tools","text":"<p>Many tools and frameworks can be used to write both unit and integration tests. The following tools are for automating integration tests.</p> <ul> <li>JUnit</li> <li>Robot Framework</li> <li>moq</li> <li>Cucumber</li> <li>Selenium</li> <li>Behave (Python)</li> </ul>"},{"location":"lab2/integration-testing/#conclusion","title":"Conclusion","text":"<p>Integration testing demonstrates how one module of a system, or external system, interfaces with another. This can be a test of two components, a sub-system, a whole system, or a collection of systems. Tests should be written frequently and throughout the entire development lifecycle using an appropriate amount of mocked dependencies and test data. Because integration tests prove that independently developed modules interface as technically designed, it increases confidence in the development cycle providing a path for a system that deploys and scales.</p>"},{"location":"lab2/integration-testing/#resources","title":"Resources","text":"<ul> <li>Integration testing approaches</li> <li>Integration testing pros and cons</li> <li>Integration tests mocks and stubs</li> <li>Software Testing: Principles and Practices</li> <li>Integration testing Behave test quick start</li> </ul>"},{"location":"lab2/performance-testing/","title":"Performance Testing","text":"<p>Performance Testing is an overloaded term that is used to refer to several subcategories of performance related testing, each of which has different purpose.</p> <p>A good description of overall performance testing is as follows:</p> <p>Performance testing is a type of testing intended to determine the responsiveness, throughput, reliability, and/or scalability of a system under a given workload. Performance Testing Guidance for Web Applications.</p> <p>Before getting into the different subcategories of performance tests let us understand why performance testing is typically done.</p>"},{"location":"lab2/performance-testing/#why-performance-testing","title":"Why Performance Testing","text":"<p>Performance testing is commonly conducted to accomplish one or more the following:</p> <ul> <li> <p>Tune the system's performance</p> <ul> <li>Identifying bottlenecks and issues with the system at different load   levels.</li> </ul> <ul> <li>Comparing performance characteristics of the system for different system   configurations.</li> </ul> <ul> <li>Come up with a scaling strategy for the system.</li> </ul> </li> </ul> <ul> <li> <p>Assist in capacity planning</p> <ul> <li>Capacity planning is the process of determining what type of hardware and   software resources are required to run an application to support pre-defined performance goals.</li> </ul> <ul> <li>Capacity planning involves identifying business   expectations, the periodic fluctuations of application usage, considering   the cost of running the hardware and software infrastructure.</li> </ul> </li> </ul> <ul> <li> <p>Assess the system's readiness for release:</p> <ul> <li>Evaluating the system's performance characteristics (response time, throughput) in a production-like environment. The goal is to ensure that performance goals can be achieved upon release.</li> </ul> </li> </ul> <ul> <li> <p>Evaluate the performance impact of application changes</p> <ul> <li>Comparing the performance characteristics of an application after a change   to the values of performance characteristics during previous runs (or   baseline values), can provide an indication of performance issues (performance regression) or   enhancements introduced due to a change</li> </ul> </li> </ul>"},{"location":"lab2/performance-testing/#key-performance-testing-categories","title":"Key Performance Testing Categories","text":"<p>Performance testing is a broad topic. There are many areas where you can perform tests. In broad strokes you can perform tests on the backend and on the front end. You can test the performance of individual components as well as testing the end-to-end functionality.</p> <p>There are several categories of tests as well:</p>"},{"location":"lab2/performance-testing/#load-testing","title":"Load Testing","text":"<p>This is the subcategory of performance testing that focuses on validating the performance characteristics of a system, when the system faces the load volumes which are expected during production operation. An Endurance Test or a Soak Test is a load test carried over a long duration ranging from several hours to days.</p>"},{"location":"lab2/performance-testing/#stress-testing","title":"Stress Testing","text":"<p>This is the subcategory of performance testing that focuses on validating the performance characteristics of a system when the system faces extreme load. The goal is to evaluate how does the system handles being pressured to its limits, does it recover (i.e., scale-out) or does it just break and fail?</p>"},{"location":"lab2/performance-testing/#endurance-testing","title":"Endurance Testing","text":"<p>The goal of endurance testing is to make sure that the system can maintain good performance under extended periods of load.</p>"},{"location":"lab2/performance-testing/#spike-testing","title":"Spike Testing","text":"<p>The goal of Spike testing is to validate that a software system can respond well to large and sudden spikes.</p>"},{"location":"lab2/performance-testing/#chaos-testing","title":"Chaos Testing","text":"<p>Chaos testing or Chaos engineering is the practice of experimenting on a system to build confidence that the system can withstand turbulent conditions in production. Its goal is to identify weaknesses before they manifest system wide. Developers often implement fallback procedures for service failure. Chaos testing arbitrarily shuts down different parts of the system to validate that fallback procedures function correctly.</p>"},{"location":"lab2/performance-testing/#best-practices","title":"Best Practices","text":"<p>Consider the following best practices for performance testing:</p> <ul> <li>Make one change at a time. Don't make multiple changes to the system   between tests. If you do, you won't know which change caused the performance   to improve or degrade.</li> </ul> <ul> <li>Automate testing. Strive to automate the setup and teardown of resources   for a performance run as much as possible. Manual execution can lead to   misconfigurations.</li> </ul> <ul> <li>Use different IP addresses. Some systems will throttle requests from a   single IP address. If you are testing a system that has this type of   restriction, you can use different IP addresses to simulate multiple users.</li> </ul>"},{"location":"lab2/performance-testing/#performance-monitor-metrics","title":"Performance Monitor Metrics","text":"<p>When executing the various types of testing approaches, whether it is stress, endurance, spike, or chaos testing, it is important to capture various metrics to see how the system performs.</p> <p>At the basic hardware level, there are four areas to consider.</p> <ul> <li>Physical disk</li> <li>Memory</li> <li>Processor</li> <li>Network</li> </ul> <p>These four areas are inextricably linked, meaning that poor performance in one area will lead to poor performance in another area. Engineers concerned with understanding application performance, should focus on these four core areas.</p> <p>The classic example of how performance in one area can affect performance in another area is memory pressure.</p> <p>If an application's available memory is running low, the operating system will try to compensate for shortages in memory by transferring pages of data from memory to disk, thus freeing up memory. But this work requires help from the CPU and the physical disk.</p> <p>This means that when you look at performance when there are low amounts of memory, you will also notice spikes in disk activity as well as CPU.</p>"},{"location":"lab2/performance-testing/#physical-disk","title":"Physical Disk","text":"<p>Almost all software systems are dependent on the performance of the physical disk. This is especially true for the performance of databases. More modern approaches to using SSDs for physical disk storage can dramatically improve the performance of applications. Here are some of the metrics that you can capture and analyze:</p> Counter Description Avg. Disk Queue Length This value is derived using the (Disk Transfers/sec)*(Disk sec/Transfer) counters. This metric describes the disk queue over time, smoothing out any quick spikes. Having any physical disk with an average queue length over 2 for prolonged periods of time can be an indication that your disk is a bottleneck. % Idle Time This is a measure of the percentage of time that the disk was idle. ie. there are no pending disk requests from the operating system waiting to be completed. A low number here is a positive sign that disk has excess capacity to service or write requests from the operating system. Avg. Disk sec/Read and Avg. Disk sec/Write These both measure the latency of your disks. Latency is defined as the average time it takes for a disk transfer to complete. You obviously want is low numbers as possible but need to be careful to account for inherent speed differences between SSD and traditional spinning disks. For this counter is important to define a baseline after the hardware is installed. Then use this value going forward to determine if you are experiencing any latency issues related to the hardware. Disk Reads/sec and Disk Writes/sec These counters each measure the total number of IO requests completed per second. Similar to the latency counters, good and bad values for these counters depend on your disk hardware but values higher than your initial baseline don't normally point to a hardware issue in this case. This counter can be useful to identify spikes in disk I/O."},{"location":"lab2/performance-testing/#processor","title":"Processor","text":"<p>It is important to understand the amount of time spent in kernel or privileged mode. In general, if code is spending too much time executing operating system calls, that could be an area of concern because it will not allow you to run your user mode applications, such as your databases, Web servers/services, etc.</p> <p>The guideline is that the CPU should only spend about 20% of the total processor time running in kernel mode.</p> Counter Description % Processor time This is the percentage of total elapsed time that the processor was busy executing. This counter can either be too high or too low. If your processor time is consistently below 40%, then there is a question as to whether you have over provisioned your CPU. 70% is generally considered a good target number and if you start going higher than 70%, you may want to explore why there is high CPU pressure. % Privileged (Kernel Mode) time This measures the percentage of elapsed time the processor spent executing in kernel mode. Since this counter takes into account only kernel operations a high percentage of privileged time (greater than 25%) may indicate driver or hardware issue that should be investigated. % User time The percentage of elapsed time the processor spent executing in user mode (your application code). A good guideline is to be consistently below 65% as you want to have some buffer for both the kernel operations mentioned above as well as any other bursts of CPU required by other applications. Queue Length This is the number of threads that are ready to execute but waiting for a core to become available. On single core machines a sustained value greater than 2-3 can mean that you have some CPU pressure. Similarly, for a multicore machine divide the queue length by the number of cores and if that is continuously greater than 2-3 there might be CPU pressure."},{"location":"lab2/performance-testing/#network-adapter","title":"Network Adapter","text":"<p>Network speed is often a hidden culprit of poor performance. Finding the root cause to poor network performance is often difficult. The source of issues can originate from bandwidth hogs such as videoconferencing, transaction data, network backups, recreational videos.</p> <p>In fact, the three most common reasons for a network slow down are:</p> <ul> <li>Congestion</li> <li>Data corruption</li> <li>Collisions</li> </ul> <p>Some of the tools that can help include:</p> <ul> <li>ifconfig</li> <li>netstat</li> <li>iperf</li> <li>tcpretrans</li> <li>tcpdump</li> <li>WireShark</li> </ul> <p>Troubleshooting network performance usually begins with checking the hardware. Typical things to explore is whether there are any loose wires or checking that all routers are powered up. It is not always possible to do so, but sometimes a simple case of power recycling of the modem or router can solve many problems.</p> <p>Network specialists often perform the following sequence of troubleshooting steps:</p> <ul> <li>Check the hardware</li> <li>Use IP config</li> <li>Use ping and tracert</li> <li>Perform DNS Check</li> </ul> <p>More advanced approaches often involve looking at some of the networking performance counters, as explained below.</p>"},{"location":"lab2/performance-testing/#network-counters","title":"Network Counters","text":"<p>The table above gives you some reference points to better understand what you can expect out of your network. Here are some counters that can help you understand where the bottlenecks might exist:</p> Counter Description Bytes Received/sec The rate at which bytes are received over each network adapter. Bytes Sent/sec The rate at which bytes are sent over each network adapter. Bytes Total/sec The number of bytes sent and received over the network. Segments Received/sec The rate at which segments are received for the protocol Segments Sent/sec The rate at which segments are sent. % Interrupt Time The percentage of time the processor spends receiving and servicing hardware interrupts. This value is an indirect indicator of the activity of devices that generate interrupts, such as network adapters. <p>There is an important distinction between latency and throughput. Latency measures the time it takes for a packet to be transferred across the network, either in terms of a one-way transmission or a round-trip transmission. Throughput is different and attempts to measure the quantity of data being sent and received within a unit of time.</p>"},{"location":"lab2/performance-testing/#memory","title":"Memory","text":"Counter Description Available MBs This counter represents the amount of memory that is available to applications that are executing. Low memory can trigger Page Faults, whereby additional pressure is put on the CPU to swap memory to and from the disk. if the amount of available memory dips below 10%, more memory should be obtained. Pages/sec This is actually the sum of \"Pages Input/sec\" and \"Pages Output/sec\" counters which is the rate at which pages are being read and written as a result of pages faults. Small spikes with this value do not mean there is an issue but sustained values of greater than 50 can mean that system memory is a bottleneck. Paging File(_Total)\\% Usage The percentage of the system page file that is currently in use. This is not directly related to performance, but you can run into serious application issues if the page file does become completely full and additional memory is still being requested by applications."},{"location":"lab2/performance-testing/#key-performance-testing-activities","title":"Key Performance Testing Activities","text":"<p>Performance testing activities vary depending on the subcategory of performance testing and the system's requirements and constraints. For specific guidance you can follow the link to the subcategory of performance tests listed above. The following activities might be included depending on the performance test subcategory:</p>"},{"location":"lab2/performance-testing/#identify-the-acceptance-criteria-for-the-tests","title":"Identify the Acceptance Criteria for the Tests","text":"<p>This will generally include identifying the goals and constraints for the performance characteristics of the system</p>"},{"location":"lab2/performance-testing/#plan-and-design-the-tests","title":"Plan and Design the Tests","text":"<p>In general we need to consider the following points:</p> <ul> <li>Defining the load the application should be tested with</li> </ul> <ul> <li>Establishing the metrics to be collected</li> </ul> <ul> <li>Establish what tools will be used for the tests</li> </ul> <ul> <li>Establish the performance test frequency: whether the performance tests be   done as a part of the feature development sprints, or only prior to release to   a major environment?</li> </ul>"},{"location":"lab2/performance-testing/#implementation","title":"Implementation","text":"<ul> <li>Implement the performance tests according to the designed approach.</li> </ul> <ul> <li>Instrument the system and ensure that is emitting the needed performance metrics.</li> </ul>"},{"location":"lab2/performance-testing/#test-execution","title":"Test Execution","text":"<ul> <li>Execute the tests and collect performance metrics.</li> </ul>"},{"location":"lab2/performance-testing/#result-analysis-and-re-testing","title":"Result Analysis and Re-testing","text":"<ul> <li>Analyze the results/performance metrics from the tests.</li> </ul> <ul> <li>Identify needed changes to tweak the system (i.e., code, infrastructure) to better accommodate the test objectives.</li> </ul> <ul> <li>Then test again. This cycle continues until the test objective is achieved.</li> </ul> <p>The Iterative Performance Test Template can be used to capture details about the test result for every iterations.</p>"},{"location":"lab2/performance-testing/#resources","title":"Resources","text":"<ul> <li>Patters and Practices: Performance Testing Guidance for Web Applications</li> </ul>"},{"location":"lab2/performance-testing/iterative-perf-test-template/","title":"Performance Test Iteration Template","text":"<p>This document provides template for capturing results of performance tests. Performance tests are done in iterations and each iteration should have a clear goal. The results of any iteration is immutable regardless whether the goal was achieved or not. If the iteration failed or the goal is not achieved then a new iteration of testing is carried out with appropriate fixes. It is recommended to keep track of the recorded iterations to maintain a timeline of how system evolved and which changes affected the performance in what way. Feel free to modify this template as needed.</p>"},{"location":"lab2/performance-testing/iterative-perf-test-template/#iteration-template","title":"Iteration Template","text":""},{"location":"lab2/performance-testing/iterative-perf-test-template/#goal","title":"Goal","text":"<p>Mention in bullet points the goal for this iteration of test. The goal should be small and measurable within this iteration.</p>"},{"location":"lab2/performance-testing/iterative-perf-test-template/#test-details","title":"Test Details","text":"<ul> <li>Date: Date and time when this iteration started and ended</li> <li>Duration: Time it took to complete this iteration.</li> <li>Application Code: Commit id and link to the commit for the code(s) which are being tested in this iteration</li> <li>Benchmarking Configuration:<ul> <li>Application Configuration: In bullet points mention the configuration for application that should be recorded</li> <li>System Configuration: In bullet points mention the configuration of the infrastructure</li> </ul> </li> </ul> <p>Record different types of configurations. Usually application specific configuration changes between iterations whereas system or infrastructure configurations rarely change</p>"},{"location":"lab2/performance-testing/iterative-perf-test-template/#work-items","title":"Work Items","text":"<p>List of links to relevant work items (task, story, bug) being tested in this iteration.</p>"},{"location":"lab2/performance-testing/iterative-perf-test-template/#results","title":"Results","text":"<pre><code>In bullet points document the results from the test.\n- Attach any documents supporting the test results.\n- Add links to the dashboard for metrics and logs such as Application Insights.\n- Capture screenshots for metrics and include it in the results. Good candidate for this is CPU/Memory/Disk usage.\n</code></pre>"},{"location":"lab2/performance-testing/iterative-perf-test-template/#observations","title":"Observations","text":"<p>Observations are insights derived from test results. Keep the observations brief and as bullet points. Mention outcomes supporting the goal of the iteration. If any of the observation results in a work item (task, story, bug) then add the link to the work item together with the observation.</p>"},{"location":"lab2/performance-testing/load-testing/","title":"Load Testing","text":"<p>\"Load testing is performed to determine a system's behavior under both normal and anticipated peak load conditions.\" - Load testing - Wikipedia</p> <p>A load test is designed to determine how a system behaves under expected normal and peak workloads. Specifically its main purpose is to confirm if a system can handle the expected load level. Depending on the target system this could be concurrent users, requests per second or data size.</p>"},{"location":"lab2/performance-testing/load-testing/#why-load-testing","title":"Why Load Testing","text":"<p>The main objective is to prove the system can behave normally under the expected normal load before releasing it to production. The criteria that define \"behave normally\" will depend on your target, this may be as simple as \"the system remains available\", but it could also include meeting a response time SLA or error rate.</p> <p>Additionally, the results of a load test can also be used as data to help with capacity planning and calculating scalability.</p>"},{"location":"lab2/performance-testing/load-testing/#load-testing-design-blocks","title":"Load Testing Design Blocks","text":"<p>There are a number of basic components that are required to carry out a load test.</p> <ol> <li> <p>In order to have meaningful results the system needs to be tested in a production-like environment with a network and hardware which closely resembles the expected deployment environment.</p> </li> <li> <p>The load test will consist of a module which simulates user activity. Of course the composition of this \"user activity\" will vary based on the type of application being tested. For example, an e-commerce website might simulate user browsing and purchasing items, but an IoT data ingestion pipeline would simulate a stream of device readings. Please ensure the simulation is as close to real activity as possible, and consider not just volume but also patterns and variability. For example, if the simulator data is too uniform or predictable, then cache/hit ratios may impact your results.</p> </li> <li> <p>The load test will be initiated from a component external to the target system which can control the amount of load applied. This can be a single agent, but may need to scaled to multiple agents in order to achieve higher levels of activity.</p> </li> <li> <p>Although not required to run a load test, it is advisable to have monitoring and/or logging in place to be able to measure the impact of the test and discover potential bottlenecks.</p> </li> </ol>"},{"location":"lab2/performance-testing/load-testing/#applying-the-load-testing","title":"Applying the Load Testing","text":""},{"location":"lab2/performance-testing/load-testing/#planning","title":"Planning","text":"<ol> <li>Identify key scenarios to measure - Gather these scenarios from Product Owner, they should provide a representative sample of real world traffic. The key activity of this phase is to agree on and define the load test cases.</li> <li>Determine expected normal and peak load for the scenarios - Determine a load level such as concurrent users or requests per second to find the size of the load test you will run.</li> <li>Identify success criteria metrics - These may be on testing side such as response time and error rate, or they may be on the system side such as CPU and memory usage.</li> <li>Agree on test matrix - Which load test cases should be run for which combinations of input parameters.</li> <li>Select the right tool - Many frameworks exist for load testing so consider if features and limitations are suitable for your needs (Some popular tools are listed below). This may also include development of a custom load test client, see Preparation phase below.</li> <li>Observability - Determine which metrics need to gathered to gain insight into throughput, latency, resource utilization, etc.</li> <li>Scalability - Determine the amount of scale needed by load generator, workload application, CPU, Memory, and network components needed to achieve testing goals. The use of kubernetes on the cloud can be used to make testing infinitely scalable.</li> </ol>"},{"location":"lab2/performance-testing/load-testing/#preparation","title":"Preparation","text":"<p>The key activity is to replace the end user client with a test bench that simulates one or more instances of the original client. For standard 3rd party tools it may suffice to configure the existing test UI before initiating the load tests.  </p> <p>If a custom client is used, code development will be required:</p> <ol> <li>Custom development - Design for minimal impact/overhead. Be sure to capture only those features of the production client that are relevant from a load perspective. Does it matter if the same test is duplicated, or must the workload be unique for each test? Can all tests be run under the same user context?</li> <li>Test environment - Create test environment that resembles production environment. This includes the platform as well as external systems, e.g., data sources.</li> <li>Security contexts - Be sure to have all requisite security contexts for the test environment. Automation like pipelines may require special setup, e.g., OAuth2 client credential flow instead of auth code flow, because interactive login is replaced by non-interactive. Allow planning leeway in case admin approval is required for new security contexts.</li> <li>Test data strategy - Make sure that output data format (ascii/binary/...) is compatible with whatever analysis tool is used in the analysis phase. This also includes storage areas (local/cloud/...), which may trigger new security contexts. Bear in mind that it may be necessary to collect data from sources external to the application to correlate potential performance issues with the application behavior. This includes platform and network metrics. Make sure to collect data that covers analysis needs (statistical measures, distributions, graphs, etc.).</li> <li>Automation - Repeatability is critical. It must be possible to re-run a given test multiple times to verify consistency and resilience of the application itself and the underlying platform.  Pipelines are recommended whenever possible. Evaluate whether load tests should be run as part of the PR strategy.</li> <li>Test client debugging - All test modules should be carefully debugged to ensure that the execution phase progresses smoothly.</li> <li>Test client validation - All test modules should be validated for extreme values of the input parameters. This reduces the risk of running into unexpected difficulties when stepping through the full test matrix during the execution phase.</li> </ol>"},{"location":"lab2/performance-testing/load-testing/#execution","title":"Execution","text":"<p>It is recommended to use an existing testing framework (see below). These tools will provide a method of both specifying the user activity scenarios and how to execute those at load. Depending on the situation, it may be advisable to coordinate testing activities with the platform operations team.</p> <p>It is common to slowly ramp up to your desired load to better replicate real world behavior. Once you have reached your defined workload, maintain this level long enough to see if your system stabilizes. To finish up the test you should also ramp to see record how the system slows down as well.</p> <p>You should also consider the origin of your load test traffic. Depending on the scope of the target system you may want to initiate from a different location to better replicate real world traffic such as from a different region.</p> <p>Note: Before starting please be aware of any restrictions on your network such as DDOS protection where you may need to notify a network administrator or apply for an exemption.</p> <p>Note: In general, the preferred approach to load testing would be the usage of a standard test framework such as the ones discussed below.  There are cases, however, where a custom test client may be advantageous. Examples include batch oriented workloads that can be run under a single security context and the same test data can be re-used for multiple load tests.  In such a scenario it may be beneficial to develop a custom script that can be used interactively as well as non-interactively.</p>"},{"location":"lab2/performance-testing/load-testing/#analysis","title":"Analysis","text":"<p>The analysis phase represents the work that brings all previous activities together:</p> <ul> <li>Set aside time to allow for collection of new test data based on the analysis of the load tests.</li> <li>Correlate application metrics and platform metrics to identify potential pitfalls and bottlenecks.</li> <li>Include business stakeholders early in the analysis phase to validate application findings. Include platform operations to validate platform findings.</li> </ul>"},{"location":"lab2/performance-testing/load-testing/#report-writing","title":"Report Writing","text":"<p>Summarize your findings from the analysis phase. Be sure to include application and platform enhancement suggestions, if any.</p>"},{"location":"lab2/performance-testing/load-testing/#further-testing","title":"Further Testing","text":"<p>After completing your load test you should be set up to continue on to additional related testing such as;</p> <ul> <li>Soak Testing - Also known as Endurance Testing. Performing a load test over an extended period of time to ensure long term stability.</li> <li>Stress Testing - Gradually increasing the load to find the limits of the system and identify the maximum capacity.</li> <li>Spike Testing - Introduce a sharp short-term increase into the load scenarios.</li> <li>Scalability Testing - Re-testing of a system as your expand horizontally or vertically to measure how it scales.</li> <li>Distributed Testing - Distributed testing allows you to leverage the power of multiple machines to perform larger or more in-depth tests faster. Is necessary when a fully optimized node cannot produce the load required by your extremely large test.</li> </ul>"},{"location":"lab2/performance-testing/load-testing/#load-generation-testing-frameworks-and-tools","title":"Load Generation Testing Frameworks and Tools","text":"<p>Here are a few popular load testing frameworks you may consider, and the languages used to define your scenarios.</p> <ul> <li>Azure Load Testing (https://learn.microsoft.com/en-us/azure/load-testing/) - Managed platform for running load tests on Azure. It allows to run and monitor tests automatically, source secrets from the KeyVault, generate traffic at scale, and load test Azure private endpoints. In the simple case, it executes load tests with HTTP GET traffic to a given endpoint. For the more complex cases, you can upload your own JMeter scenarios.</li> <li>JMeter (https://github.com/apache/jmeter) - Has built in patterns to test without coding, but can be extended with Java.</li> <li>Artillery (https://artillery.io/) - Write your scenarios in Javascript, executes a node application.</li> <li>Gatling (https://gatling.io/) -  Write your scenarios in Scala with their DSL.</li> <li>Locust (https://locust.io/) - Write your scenarios in Python using the concept of concurrent user activity.</li> <li>K6 (https://k6.io/) - Write your test scenarios in Javascript, available as open source kubernetes operator, open source Docker image, or as SaaS. Particularly useful for distributed load testing. Integrates easily with prometheus.</li> <li>NBomber (https://nbomber.com/) - Write your test scenarios in C# or F#, available integration with test runners (NUnit/xUnit).</li> <li>WebValidate (https://github.com/microsoft/webvalidate) - Web request validation tool used to run end-to-end tests and long-running performance and availability tests.</li> </ul>"},{"location":"lab2/performance-testing/load-testing/#sample-workload-applications","title":"Sample Workload Applications","text":"<p>In the case where a specific workload application is not being provided and the focus is instead on the system, here are a few popular sample workload applications you may consider.</p> <ul> <li>HttpBin (Python, GoLang) - Supports variety of endpoint types and language implementations. Can echo data used in request.</li> </ul> <ul> <li>NGSA (Java, C#) - Intended for Kubernetes Platform and Monitoring Testing. Built on top of IMDB data store with many CRUD endpoints available. Does not need to have a live database connection.</li> <li>MockBin (https://github.com/Kong/mockbin) - Allows you to generate custom endpoints to test, mock, and track HTTP requests &amp; responses between libraries, sockets and APIs.</li> </ul>"},{"location":"lab2/performance-testing/load-testing/#conclusion","title":"Conclusion","text":"<p>A load test is critical step to understand if a target system will be reliable under the expected real world traffic.</p> <p>Of course, it's only as good as your ability to predict the expected load, so it's important to follow up with other further testing to truly understand how your system behaves in different situations.</p>"},{"location":"lab2/performance-testing/load-testing/#resources","title":"Resources","text":"<p>List additional readings about this test type for those that would like to dive deeper.</p> <ul> <li>Microsoft Azure Well-Architected Framework &gt; Load Testing</li> </ul>"},{"location":"lab2/shadow-testing/","title":"Shadow Testing","text":"<p>Shadow testing is one approach to reduce risks before going to production. Shadow testing is also known as \"Shadow Deployment\" or \"Shadowing Traffic\" and similarities with \"Dark launching\".</p>"},{"location":"lab2/shadow-testing/#when-to-use","title":"When to Use","text":"<p>Shadow Testing reduces risks when you consider replacing the current environment (V-Current) with candidate environment with new feature (V-Next). This approach is monitoring and capturing differences between two environments then compare and reduces all risks before you introduce a new feature/release.</p> <p>In our test cases, code coverage is very important however sometimes providing code coverage can be tricky to replicate real-life combinations and possibilities. In this approach, to test V-Next environment we have side by side deployment, we're replicating the same traffic with V-Current environment and directing same traffic to V-Next environment, the only difference is we don't return any response from V-Next environment to users, but we collect those responses to compare with V-Current responses.</p> <p></p> <p>Referencing back to one of the Principles of Chaos Engineering, mentions importance of sampling real traffic like below:</p> <p>Systems behave differently depending on environment and traffic patterns. Since the behavior of utilization can change at any time, sampling real traffic is the only way to reliably capture the request path. To guarantee both authenticity of the way in which the system is exercised and relevance to the current deployed system, Chaos strongly prefers to experiment directly on production traffic.</p> <p>With this Shadow Testing approach we're leveraging real customer behavior in V-Next environment with sampling real traffic and mitigating the risks which users may face on production. At the same time we're testing V-Next environment infrastructure for scaling with real sampled traffic. V-Next should scale with the same way V-Current does. We're testing actual behavior of the product and this cause zero impact to production to test new features since traffic is replicated to V-next environment.</p> <p>There are some similarities with Dark Launching, Dark Launching proposes to integrate new feature into production code, but users can't use the feature. On the backend you can test your feature and improve the performance until it's acceptable. It is also similar to Feature Toggles which provides you with an ability to enable/disable your new feature in production on a UI level. With this approach your new feature will be visible to users, and you can collect feedback. Using Dark Launching with Feature Toggles can be very useful for introducing a new feature.</p>"},{"location":"lab2/shadow-testing/#applicable-to","title":"Applicable to","text":"<ul> <li>Production deployments: V-Next in Shadow testing always working separately and not effecting production. Users are not effected with this test.</li> <li>Infrastructure: Shadow testing replicating the same traffic, in test environment you can have the same traffic on the production. It helps to produce real life test scenarios</li> <li>Handling Scale: All traffic is replicated, and you have a chance to see how your system scaling.</li> </ul>"},{"location":"lab2/shadow-testing/#shadow-testing-frameworks-and-tools","title":"Shadow Testing Frameworks and Tools","text":"<p>There are some tools to implement shadow testing. The main purpose of these tools is to compare responses of V-Current and V-Next then find the differences.</p> <ul> <li>Diffy</li> <li>Envoy</li> <li>McRouter</li> <li>Scientist</li> <li>Keploy</li> </ul> <p>One of the most popular tools is Diffy. It was created and used at Twitter. Now the original author and a former Twitter employee maintains their own version of this project, called Opendiffy. Twitter announced this tool on their engineering blog as \"Testing services without writing tests\".</p> <p>As of today Diffy is used in production by Twitter, Airbnb, Baidu and Bytedance companies. Diffy explains the shadow testing feature like this:</p> <p>Diffy finds potential bugs in your service using running instances of your new code, and your old code side by side. Diffy behaves as a proxy and multicasts whatever requests it receives to each of the running instances. It then compares the responses, and reports any regressions that may surface from those comparisons. The premise for Diffy is that if two implementations of the service return \u201csimilar\u201d responses for a sufficiently large and diverse set of requests, then the two implementations can be treated as equivalent, and the newer implementation is regression-free.</p> <p></p> <p>Diffy architecture</p>"},{"location":"lab2/shadow-testing/#conclusion","title":"Conclusion","text":"<p>Shadow Testing is a useful approach to reduce risks when you consider replacing the current environment with candidate environment using new feature(s). Shadow testing replicates traffic of the production to candidate environment for testing, so you get same production use case scenarios in the test environment. You can compare differences on both environments and validate your candidate environment to be ready for releasing.</p> <p>Some advantages of shadow testing are:</p> <ul> <li>Zero impact to production environment</li> <li>No need to generate test scenarios and test data</li> <li>We can test real-life scenarios with real-life data.</li> <li>We can simulate scale with replicated production traffic.</li> </ul>"},{"location":"lab2/shadow-testing/#resources","title":"Resources","text":"<ul> <li>Martin Fowler - Dark Launching</li> <li>Martin Fowler - Feature Toggle</li> <li>Traffic Shadowing/Mirroring</li> </ul>"},{"location":"lab2/smoke-testing/","title":"Smoke Testing","text":"<p>Smoke tests, sometimes named Sanity, Acceptance, or Build/Release Verification tests, are a sub-type of system/functional tests that are usually used as gates that verify the application's readiness as a preliminary step. If an application passes the smoke tests, it is acceptable, or in a stable-enough state, for the next stages of testing or deployment.</p>"},{"location":"lab2/smoke-testing/#when-to-use","title":"When To Use","text":""},{"location":"lab2/smoke-testing/#problem-addressed","title":"Problem Addressed","text":"<p>Smoke tests are meant to find, as early as possible, if an application is working or not. The goal of smoke tests is to save time; if the current version of the application does not pass smoke tests, then the rest of the integration or deployment chain for it can be abandoned. Smoke tests do not aim to provide full functionality coverage but instead focus on a few quick acceptance invocations for which the application should, at all times, respond correctly to.</p>"},{"location":"lab2/smoke-testing/#roi-tipping-point","title":"ROI Tipping Point","text":"<p>Smoke tests cover only the most critical application path, and should not be used to actually test the application's behavior, keeping execution time and complexity to minimum. The tests can be formed of a subset of the application's integration or e2e tests, and they cover as much of the functionality with as little depth as required.</p> <p>The golden rule of a good smoke test is that it saves time on validating that the application is acceptable to a stage where better, more thorough testing will begin.</p>"},{"location":"lab2/smoke-testing/#applicable-to","title":"Applicable to","text":"<ul> <li> Local dev desktop - Example: Applying manual smoke testing to verify that the application is OK.</li> <li> Build pipelines - Example: Running a small set of the integration test suite before running the full coverage of tests, which may take a long time.</li> <li> Non-production and Production deployments - Example: Running a curl command to the product's API and asserting the response is 200 before running load test which consume resources.</li> <li> PR Validation - Example: - Deploying the application chart to a test namespace and validating the release is successful and no immediate regressions are merged.</li> </ul>"},{"location":"lab2/smoke-testing/#conclusion","title":"Conclusion","text":"<p>Smoke testing is a low-effort, high-impact step to ship more reliable software. It should be considered amongst the first stages to implement when planning continuously integrated and delivered systems.</p>"},{"location":"lab2/smoke-testing/#resources","title":"Resources","text":"<ul> <li>Wikipedia - Smoke Testing</li> <li>Google SRE Book - System Tests</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/","title":"Synthetic Monitoring Tests","text":"<p>Synthetic Monitoring Tests are a set of functional tests that target a live system in production. The focus of these tests, which are sometimes named \"watchdog\", \"active monitoring\" or \"synthetic transactions\", is to verify the product's health and resilience continuously.</p>"},{"location":"lab2/synthetic-monitoring-tests/#why-synthetic-monitoring-tests","title":"Why Synthetic Monitoring Tests","text":"<p>Traditionally, software providers rely on testing through CI/CD stages in the well known testing pyramid (unit, integration, e2e) to validate that the product is healthy and without regressions. Such tests will run on the build agent or in the test/stage environment before being deployed to production and released to live user traffic. During the services' lifetime in the production environment, they are safeguarded by monitoring and alerting tools that rely on Real User Metrics/Monitoring (RUM).</p> <p>However, as more organizations today provide highly-available (99.9+ SLA) products, they find that the nature of long-lived distributed applications, which typically rely on several hardware and software components, is to fail. Frequent releases (sometimes multiple times per day) of various components of the system can create further instability. This rapid rate of change to the production environment tends to make testing during CI/CD stages not hermetic and actually not representative of the end user experience and how the production system actually behaves.</p> <p>For such systems, the ambition of service engineering teams is to reduce to a minimum the time it takes to fix errors, or the MTTR - Mean Time To Repair. It is a continuous effort, performed on the live/production system. Synthetic Monitors can be used to detect the following issues:</p> <ul> <li>Availability - Is the system or specific region available.</li> <li>Transactions and customer journeys - Known good requests should work, while known bad requests should error.</li> <li>Performance - How fast are actions and is that performance maintained through high loads and through version releases.</li> <li>3rd Party components - Cloud or software components used by the system may fail.</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/#shift-right-testing","title":"Shift-Right Testing","text":"<p>Synthetic Monitoring tests are a subset of tests that run in production, sometimes named Test-in-Production or Shift-Right tests. With Shift-Left paradigms that are so popular, the approach is to perform testing as early as possible in the application development lifecycle (i.e., moved left on the project timeline). Shift right compliments and adds on top of Shift-Left. It refers to running tests late in the cycle, during deployment, release, and post-release when the product is serving production traffic. They provide modern engineering teams a broader set of tools to assure high SLAs over time.</p>"},{"location":"lab2/synthetic-monitoring-tests/#synthetic-monitoring-tests-design-blocks","title":"Synthetic Monitoring Tests Design Blocks","text":"<p>A synthetic monitoring test is a test that uses synthetic data and real testing accounts to inject user behaviors to the system and validates their effect, usually by passively relying on existing monitoring and alerting capabilities. Components of synthetic monitoring tests include Probes, test code/ accounts which generates data, and Monitoring tools placed to validate both the system's behavior under test and the health of the probes themselves.</p> <p></p>"},{"location":"lab2/synthetic-monitoring-tests/#probes","title":"Probes","text":"<p>Probes are the source of synthetic user actions that drive testing. They target the product's front-end or publicly-facing APIs and are running on their own production environment. A Synthetic Monitoring test is, in fact, very related to black-box tests and would usually focus on end-to-end scenarios from a user's perspective. It is not uncommon for the same code for e2e or integration tests to be used to implement the probe.</p>"},{"location":"lab2/synthetic-monitoring-tests/#monitoring","title":"Monitoring","text":"<p>Given that Synthetic Monitoring tests are continuously running, at intervals, in a production environment, the assertion of system behavior through analysis relies on existing monitoring pillars used in live system (Logging, Metrics, Distributed Tracing). There would usually be a finite set of tests, and key metrics that are used to build monitors and alerts to assert against the known SLO, and verify that the OKR for that system are maintained. The monitoring tools are effectively capturing both RUMs and synthetic data generated by the probes.</p>"},{"location":"lab2/synthetic-monitoring-tests/#applying-synthetic-monitoring-tests","title":"Applying Synthetic Monitoring Tests","text":""},{"location":"lab2/synthetic-monitoring-tests/#asserting-the-system-under-test","title":"Asserting the System under Test","text":"<p>Synthetic monitoring tests are usually statistical. Test metrics are compared against some historical or running average with a time dimension (Example: Over the last 30 days, for this time of day, the mean average response time is 250ms for AddToCart operation with a standard deviation from the mean of +/- 32ms). So if an observed measurement is within a deviation of the norm at any time, the services are probably healthy.</p>"},{"location":"lab2/synthetic-monitoring-tests/#building-a-synthetic-monitoring-solution","title":"Building a Synthetic Monitoring Solution","text":"<p>At a high level, building synthetic monitors usually consists of the following steps:</p> <ul> <li>Determine the metric to be validated (functional result, latency, etc.)</li> <li>Build a piece of automation that measures that metric against the system, and gathers telemetry into the system's existing monitoring infrastructure.</li> <li>Set up monitoring alarms/actions/responses that detect the failure of the system to meet the desired goal of the metric.</li> <li>Run the test case automation continuously at an appropriate interval.</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/#monitoring-the-health-of-tests","title":"Monitoring the Health of Tests","text":"<p>Probes runtime is a production environment on its own, and the health of tests is critical. Many providers offer cloud-based systems that host such runtimes, while some organizations use existing production environments to run these tests on. In either way, a monitor-the-monitor strategy should be a first-class citizen of the production environment's alerting systems.</p>"},{"location":"lab2/synthetic-monitoring-tests/#synthetic-monitoring-and-real-user-monitoring","title":"Synthetic Monitoring and Real User Monitoring","text":"<p>Synthetic monitoring does not replace the need for RUM. Probes are predictable code that verifies specific scenarios, and they do not 100% completely and truly represent how a user session is handled. On the other hand, prefer not to use RUMs to test for site reliability because:</p> <ul> <li>As the name implies, RUM requires user traffic. The site may be down, but since no user visited the monitored path, no alerts were triggered yet.</li> <li>Inconsistent Traffic and usage patterns make it hard to gauge for benchmarks.</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/#risks","title":"Risks","text":"<p>Testing in production, in general, has a risk factor attached to it, which does not exist tests executed during CI/CD stages. Specifically, in synthetic monitoring tests, the following may affect the production environment:</p> <ul> <li>Corrupted or invalid data - Tests inject test data which may be in some ways corrupt. Consider using a testing schema.</li> <li>Protected data leakage - Tests run in a production environment and emit logs or trace that may contain protected data.</li> <li>Overloaded systems - Synthetic tests may cause errors or overload the system.</li> <li>Unintended side effects or impacts on other production systems.</li> <li>Skewed analytics (traffic funnels, A/B test results, etc.)</li> <li>Auth/AuthZ - Tests are required to run in production where access to tokens and secrets may be restricted or more challenging to retrieve.</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/#synthetic-monitoring-tests-frameworks-and-tools","title":"Synthetic Monitoring Tests Frameworks and Tools","text":"<p>Most key monitoring/APM players have an enterprise product that supports synthetic monitoring built into their systems (see list below). Such offerings make some of the risks raised above irrelevant as the integration and runtime aspects of the solution are OOTB. However, such solutions are typically pricey.</p> <p>Some organizations prefer running probes on existing infrastructure using known tools such as Postman, Wrk, JMeter, Selenium or even custom code to generate the synthetic data. Such solutions must account for isolating and decoupling the probe's production environment from the core product's as well as provide monitoring, geo-distribution, and maintaining test health.</p> <ul> <li>Application Insights availability - Simple availability tests that allow some customization using Multi-step web test</li> <li>DataDog Synthetics</li> <li>Dynatrace Synthetic Monitoring</li> <li>New Relic Synthetics</li> <li>Checkly</li> </ul>"},{"location":"lab2/synthetic-monitoring-tests/#conclusion","title":"Conclusion","text":"<p>The value of production tests, in general, and specifically Synthetic monitoring, is only there for particular engagement types, and there is associated risk and cost to them. However, when applicable, they provide continuous assurance that there are no system failures from a user's perspective. When developing a PaaS/SaaS solution, Synthetic monitoring is key to the success of service reliability teams, and they are becoming an integral part of the quality assurance stack of highly available products.</p>"},{"location":"lab2/synthetic-monitoring-tests/#resources","title":"Resources","text":"<ul> <li>Google SRE book - Testing Reliability</li> <li>Microsoft DevOps Architectures - Shift Right to Test in Production</li> <li>Martin Fowler - Synthetic Monitoring</li> </ul>"},{"location":"lab2/tech-specific-samples/building-containers-with-azure-devops/","title":"Building Containers with Azure DevOps Using the DevTest Pattern","text":"<p>In this documents, we highlight learnings from applying the DevTest pattern to container development in Azure DevOps through pipelines.</p> <p>The pattern enabled as to build container for development, testing and releasing the container for further reuse (production ready).</p> <p>We will dive into tools needed to build, test and push a container, our environment and go through each step separately.</p> <p>Follow this link to dive deeper or revisit the DevTest pattern.</p>"},{"location":"lab2/tech-specific-samples/building-containers-with-azure-devops/#build-the-container","title":"Build the Container","text":"<p>The first step in container development, after creating the necessary Dockerfiles and source code, is building the container. Even the Dockerfile itself can include some basic testing. Code tests are performed when pushing the code to the repository origin, where it is then used to build the container.</p> <p>The first step in our pipeline is to run the <code>docker build</code> command with a temporary tag and the required build arguments:</p> <pre><code>- task: Bash@3\n  name: BuildImage\n  displayName: 'Build the image via docker'\n  inputs:\n    workingDirectory: \"$(System.DefaultWorkingDirectory)${{ parameters.buildDirectory }}\"\n    targetType: 'inline'\n    script: |\n      docker build -t ${{ parameters.imageName }} --build-arg YOUR_BUILD_ARG -f ${{ parameters.dockerfileName }} .\n  env:\n    PredefinedPassword: $(Password)\n    NewVariable: \"newVariableValue\"\n</code></pre> <p>This task includes the parameters <code>buildDirectory</code>, <code>imageName</code> and <code>dockerfileName</code>, which have to be set beforehand. This task can for example be used in a template for multiple containers to improve code reuse.</p> <p>It is also possible to pass environment variables directly to the Dockerfile through the <code>env</code> section of the task.</p> <p>If this task succeeds, the Dockerfile was build without errors and we can continue to testing the container itself.</p>"},{"location":"lab2/tech-specific-samples/building-containers-with-azure-devops/#test-the-container","title":"Test the Container","text":"<p>To test the container, we are using the tox environment. For more details on tox please visit the tox section of this repository or visit the official tox documentation page.</p> <p>Before we test the container, we are checking for exposed credentials in the docker image history. If known passwords, used to access our internal resources, are exposed here, the build step will fail:</p> <pre><code>- task: Bash@3\n  name: CheckIfPasswordInDockerHistory\n  displayName: 'Check for password in docker history'\n  inputs:\n    workingDirectory: \"$(System.DefaultWorkingDirectory)\"\n    targetType: 'inline'\n    failOnStdErr: true\n    script: |\n      if docker image history --no-trunc ${{ parameters.imageName }} | grep -qF $PredefinedPassword; then\n        exit 1;\n      fi\n      exit 0;\n  env:\n    PredefinedPassword: $(Password)\n</code></pre> <p>After the credential test, the container is tested through the pytest extension testinfra. Testinfra is a Python-based tool which can be used to start a container, gather prerequisites, test the container and shut it down again, without any effort besides writing the tests. These tests can for example include:</p> <ul> <li>if files exist</li> <li>if environment variables are set correctly</li> <li>if certain processes are running</li> <li>if the correct host environment is used</li> </ul> <p>For a complete collection of capabilities and requirements, please visit the testinfra project on GitHub.</p> <p>A few methods of a Linux-based container test can look like this:</p> <pre><code>def test_dependencies(host):\n    '''\n    Check all files needed to run the container properly.\n    '''\n    env_file = \"/app/environment.sh.env\"\n    assert host.file(env_file).exists\n\n    activate_sh_path = \"/app/start.sh\"\n    assert host.file(activate_sh_path).exists\n\n\ndef test_container_running(host):\n    process = host.process.get(comm=\"start.sh\")\n    assert process.user == \"root\"\n\n\ndef test_host_system(host):\n    system_type = 'linux'\n    distribution = 'ubuntu'\n    release = '18.04'\n\n    assert system_type == host.system_info.type\n    assert distribution == host.system_info.distribution\n    assert release == host.system_info.release\n\n\ndef extract_env_var(file_content):\n    import re\n\n    regex = r\"ENV_VAR=\\\"(?P&lt;s&gt;[^\\\"]*)\\\"\"\n\n    match = re.match(regex, file_content)\n    return match.group('s')\n\n\ndef test_ports_exposed(host):\n    port1 = \"9010\"\n    st1 = f\"grep -q {port1} /app/Dockerfile &amp;&amp; echo 'true' || echo 'false'\"\n    cmd1 = host.run(st1)\n    assert cmd1.stdout\n\n\ndef test_listening_simserver_sockets(host):\n    assert host.socket(\"tcp://0.0.0.0:32512\").is_listening\n    assert host.socket(\"tcp://0.0.0.0:32513\").is_listening\n</code></pre> <p>To start the test, a pytest command is executed through tox.</p> <p>A task containing the tox command can look like this:</p> <pre><code>- task: Bash@3\n  name: RunTestCommands\n  displayName: \"Test - Run test commands\"\n  inputs:\n    workingDirectory: \"$(System.DefaultWorkingDirectory)\"\n    targetType: 'inline'\n    script: |\n      tox -e testinfra-${{ parameters.makeTarget }} -- ${{ parameters.imageName }}\n    failOnStderr: true\n</code></pre> <p>Which could trigger the following pytest code, which is contained in the tox.ini file:</p> <pre><code>pytest -vv tests/{env:CONTEXT} --container-image={posargs:{env:IMAGE_TAG}} --volume={env:VOLUME}\n</code></pre> <p>As a last task of this pipeline to build and test the container, we set a variable called <code>testsPassed</code> which is only <code>true</code>, if the previous tasks succeeded:</p> <pre><code>- task: Bash@3\n  name: UpdateTestResultVariable\n  condition: succeeded()\n  inputs:\n    targetType: 'inline'\n    script: |\n      echo '##vso[task.setvariable variable=testsPassed]true'\n</code></pre>"},{"location":"lab2/tech-specific-samples/building-containers-with-azure-devops/#push-the-container","title":"Push the Container","text":"<p>After building and testing, if our container runs as expected, we want to release it to our Azure Container Registry (ACR) to be used by our larger application. Before that, we want to automate the push behavior and define a meaningful tag.</p> <p>As a developer it is often helpful to have containers pushed to ACR, even if they are failing. This can be done by checking for the <code>testsPassed</code> variable we introduced at the end of our testing.</p> <p>If the test failed, we want to add a failed suffix at the end of the tag:</p> <pre><code>- task: Bash@3\n  name: SetFailedSuffixTag\n  displayName: \"Set failed suffix, if the tests failed.\"\n  condition: and(eq(variables['testsPassed'], false), ne(variables['Build.SourceBranchName'], 'main'))\n  # if this is not a release and failed -&gt; retag the image to add failedSuffix\n  inputs:\n    targetType: inline\n    script: |\n      docker tag ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }} ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }}$(failedSuffix)\n</code></pre> <p>The condition checks, if the value of <code>testsPassed</code> is <code>false</code> and also if we are not on the main branch, as we don't want to push failed containers from main. This helps us to keep our production environment clean.</p> <p>The value for imageRepository was defined in another template, along with the <code>failedSuffix</code> and <code>testsPassed</code>:</p> <pre><code>parameters:\n  - name: component\n\nvariables:\n  testsPassed: false\n  failedSuffix: \"-failed\"\n  # the imageRepo will changed based on dev or release\n  ${{ if eq( variables['Build.SourceBranchName'], 'main' ) }}:\n    imageRepository: 'stable/${{ parameters.component }}'\n  ${{ if ne( variables['Build.SourceBranchName'], 'main' ) }}:\n    imageRepository: 'dev/${{ parameters.component }}'\n</code></pre> <p>The imageTag is open to discussion, as it depends highly on how your team wants to use the container. We went for <code>Build.SourceVersion</code> which is the commit ID of the branch the container was developed in. This allows you to easily track the origin of the container and aids debugging.</p> <p>A link to Azure DevOps predefined variables can be found in the Azure Docs on Azure DevOps</p> <p>After a tag was added to the container, the image must be pushed. This can be done with the following task:</p> <pre><code>- task: Docker@1\n  name: pushFailedDockerImage\n  displayName: 'Pushes failed image via Docker'\n  condition: and(eq(variables['testsPassed'], false), ne(variables['Build.SourceBranchName'], 'main'))\n  # if this is not a release and failed -&gt; push the image with the failed tag\n  inputs:\n    containerregistrytype: 'Azure Container Registry'\n    azureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\n    azureContainerRegistry: ${{ parameters.containerRegistry }}\n    command: 'Push an image'\n    imageName: '${{ parameters.imageRepository }}:${{ parameters.imageTag }}$(failedSuffix)'\n</code></pre> <p>Similarly, these are the steps to publish the container to the ACR, if the tests succeeded:</p> <pre><code>- task: Bash@3\n  name: SetLatestSuffixTag\n  displayName: \"Set latest suffix, if the tests succeed.\"\n  condition:  eq(variables['testsPassed'], true)\n  inputs:\n    targetType: inline\n    script: |\n      docker tag ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }} ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:latest\n- task: Docker@1\n  name: pushSuccessfulDockerImageSha\n  displayName: 'Pushes successful image via Docker'\n  condition: eq(variables['testsPassed'], true)\n  inputs:\n    containerregistrytype: 'Azure Container Registry'\n    azureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\n    azureContainerRegistry: ${{ parameters.containerRegistry }}\n    command: 'Push an image'\n    imageName: '${{ parameters.imageRepository }}:${{ parameters.imageTag }}'\n- task: Docker@1\n  name: pushSuccessfulDockerImageLatest\n  displayName: 'Pushes successful image as latest'\n  condition: eq(variables['testsPassed'], true)\n  inputs:\n    containerregistrytype: 'Azure Container Registry'\n    azureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\n    azureContainerRegistry: ${{ parameters.containerRegistry }}\n    command: 'Push an image'\n    imageName: '${{ parameters.imageRepository }}:latest'\n</code></pre> <p>If you don't want to include the <code>latest</code> tag, you can also remove the steps involving latest (SetLatestSuffixTag &amp; pushSuccessfulDockerImageLatest).</p>"},{"location":"lab2/tech-specific-samples/building-containers-with-azure-devops/#resources","title":"Resources","text":"<ul> <li>DevTest pattern</li> <li>Azure Docs on Azure DevOps</li> <li>official tox documentation page</li> <li>Testinfra</li> <li>Testinfra project on GitHub</li> <li>pytest</li> </ul>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/","title":"Using Azurite to Run Blob Storage Tests in a Pipeline","text":"<p>This document determines the approach for writing automated tests with a short feedback loop (i.e. unit tests) against security considerations (private endpoints) for the Azure Blob Storage functionality.</p> <p>Once private endpoints are enabled for the Azure Storage accounts, the current tests will fail when executed locally or as part of a pipeline because this connection will be blocked.</p>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#utilize-an-azure-storage-emulator-azurite","title":"Utilize an Azure Storage Emulator - Azurite","text":"<p>To emulate a local Azure Blob Storage, we can use Azure Storage Emulator. The Storage Emulator currently runs only on Windows. If you need a Storage Emulator for Linux, one option is the community maintained, open-source Storage Emulator Azurite.</p> <p>The Azure Storage Emulator is no longer being actively developed. Azurite is the Storage Emulator platform going forward. Azurite supersedes the Azure Storage Emulator. Azurite will continue to be updated to support the latest versions of Azure Storage APIs. For more information, see Use the Azurite emulator for local Azure Storage development.</p> <p>Some differences in functionality exist between the Storage Emulator and Azure storage services. For more information about these differences, see the Differences between the Storage Emulator and Azure Storage.</p> <p>There are several ways to install and run Azurite on your local system as listed here. In this document we will cover <code>Install and run Azurite using NPM</code> and <code>Install and run the Azurite Docker image</code>.</p>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#1-install-and-run-azurite","title":"1. Install and Run Azurite","text":""},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#a-using-npm","title":"a. Using NPM","text":"<p>In order to run Azurite V3 you need Node.js &gt;= 8.0 installed on your system. Azurite works cross-platform on Windows, Linux, and OS X.</p> <p>After the Node.js installation, you can install Azurite simply with npm which is the Node.js package management tool included with every Node.js installation.</p> <pre><code># Install Azurite\nnpm install -g azurite\n\n# Create azurite directory\nmkdir c:/azurite\n\n# Launch Azurite for Windows\nazurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log\n</code></pre> <p>If you want to avoid any disk persistence and destroy the test data when the Azurite process terminates, you can pass the <code>--inMemoryPersistence</code> option, as of Azurite 3.28.0.</p> <p>The output will be:</p> <pre><code>Azurite Blob service is starting at http://127.0.0.1:10000\nAzurite Blob service is successfully listening at http://127.0.0.1:10000\nAzurite Queue service is starting at http://127.0.0.1:10001\nAzurite Queue service is successfully listening at http://127.0.0.1:10001\n</code></pre>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#b-using-a-docker-image","title":"b. Using a Docker Image","text":"<p>Another way to run Azurite is using docker, using default <code>HTTP</code> endpoint</p> <pre><code>docker run -p 10000:10000 mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0\n</code></pre> <p>Docker Compose is another option and can run the same docker image using the <code>docker-compose.yml</code> file below.</p> <pre><code>version: '3.4'\nservices:\n  azurite:\n    image: mcr.microsoft.com/azure-storage/azurite\n    hostname: azurite\n    volumes:\n      - ./cert/azurite:/data\n    command: \"azurite-blob --blobHost 0.0.0.0 -l /data --cert /data/127.0.0.1.pem --key /data/127.0.0.1-key.pem --oauth basic\"\n    ports:\n      - \"10000:10000\"\n      - \"10001:10001\"\n</code></pre>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#2-run-tests-on-your-local-machine","title":"2. Run Tests on Your Local Machine","text":"<p>Python 3.8.7 is used for this, but it should be fine on other 3.x versions as well.</p> <ol> <li> <p>Install and run Azurite for local tests:</p> <p>Option 1: using npm:</p> <pre><code># Install Azurite\nnpm install -g azurite\n# Create azurite directory\nmkdir c:/azurite\n# Launch Azurite for Windows\nazurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log\n</code></pre> <p>Option 2: using docker</p> <pre><code>docker run -p 10000:10000 mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0\n</code></pre> </li> <li> <p>In Azure Storage Explorer, select <code>Attach to a local emulator</code></p> <p></p> </li> <li> <p>Provide a Display name and port number, then your connection will be ready, and you can use Storage Explorer to manage your local blob storage.</p> <p></p> <p>To test and see how these endpoints are running you can attach your local blob storage to the Azure Storage Explorer.</p> </li> <li> <p>Create a virtual python environment    <code>python -m venv .venv</code></p> </li> <li> <p>Container name and initialize env variables: Use conftest.py for test integration.</p> <pre><code>from azure.storage.blob import BlobServiceClient\nimport os\n\ndef pytest_generate_tests(metafunc):\n   os.environ['STORAGE_CONNECTION_STRING'] = 'DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;'\n   os.environ['STORAGE_CONTAINER'] = 'test-container'\n\n   # Crete container for Azurite for the first run\n   blob_service_client = BlobServiceClient.from_connection_string(os.environ.get(\"STORAGE_CONNECTION_STRING\"))\n   try:\n      blob_service_client.create_container(os.environ.get(\"STORAGE_CONTAINER\"))\n   except Exception as e:\n      print(e)\n</code></pre> <p>*Note: value for <code>STORAGE_CONNECTION_STRING</code> is default value for Azurite, it's not a private key</p> </li> <li> <p>Install the dependencies</p> <p><code>pip install -r requirements_tests.txt</code></p> </li> <li> <p>Run tests:</p> <pre><code>python -m pytest ./tests\n</code></pre> </li> </ol> <p>After running tests, you can see the files in your local blob storage</p> <p></p>"},{"location":"lab2/tech-specific-samples/blobstorage-unit-tests/#3-run-tests-on-azure-pipelines","title":"3. Run Tests on Azure Pipelines","text":"<p>After running tests locally we need to make sure these tests pass on Azure Pipelines too. We have 2 options here, we can use docker image as hosted agent on Azure or install an npm package in the Pipeline steps.</p> <pre><code>trigger:\n- master\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.7'\n  inputs:\n    versionSpec: 3.7\n\n- bash: |\n    pip install -r requirements_tests.txt\n  displayName: 'Setup requirements for tests'\n\n- bash: |\n    sudo npm install -g azurite\n    sudo mkdir azurite\n    sudo azurite --silent --location azurite --debug azurite\\debug.log &amp;\n  displayName: 'Install and Run Azurite'\n\n- bash: |\n    python -m pytest --junit-xml=unit_tests_report.xml --cov=tests --cov-report=html --cov-report=xml ./tests\n  displayName: 'Run Tests'\n\n- task: PublishCodeCoverageResults@1\n  inputs:\n    codeCoverageTool: Cobertura\n    summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'\n    reportDirectory: '$(System.DefaultWorkingDirectory)/**/htmlcov'\n\n- task: PublishTestResults@2\n  inputs:\n    testResultsFormat: 'JUnit'\n    testResultsFiles: '**/*_tests_report.xml'\n    failTaskOnFailedTests: true\n</code></pre> <p>Once we set up our pipeline in Azure Pipelines, result will be like below</p> <p></p>"},{"location":"lab2/templates/case-study-template/","title":"Case study template","text":"<p>[Customer Project] Case Study</p>"},{"location":"lab2/templates/case-study-template/#background","title":"Background","text":"<p>Describe the customer and business requirements with the explicit problem statement.</p>"},{"location":"lab2/templates/case-study-template/#system-under-test-sut","title":"System Under Test (SUT)","text":"<p>Include the system's conceptual architecture and highlight the architecture components that were included in the E2E testing.</p>"},{"location":"lab2/templates/case-study-template/#problems-and-limitations","title":"Problems and Limitations","text":"<p>Describe about the problems of the overall SUT solution that prevented from testing specific (or any) part of the solution. Describe limitation of the testing tools and framework(s) used in this implementation</p>"},{"location":"lab2/templates/case-study-template/#e2e-testing-framework-and-tools","title":"E2E Testing Framework and Tools","text":"<p>Describe what testing framework and/or tools were used to implement E2E testing in the SUT.</p>"},{"location":"lab2/templates/case-study-template/#test-cases","title":"Test Cases","text":"<p>Describe the E2E test cases were created to E2E test the SUT</p>"},{"location":"lab2/templates/case-study-template/#test-metrics","title":"Test Metrics","text":"<p>Describe any architecture solution were used to monitor, observe and track the various service states that were used as the E2E testing metrics. Also, include the list of test cases were build to measure the progress of E2E testing.</p>"},{"location":"lab2/templates/case-study-template/#e2e-testing-architecture","title":"E2E Testing Architecture","text":"<p>Describe any testing architecture were built to run E2E testing.</p>"},{"location":"lab2/templates/case-study-template/#e2e-testing-implementation-code-samples","title":"E2E Testing Implementation (Code Samples)","text":"<p>Include sample test cases and their implementation in the programming language of choice. Include any common reusable code implementation blocks that could be leveraged in the future project's E2E testing implementation.</p>"},{"location":"lab2/templates/case-study-template/#e2e-testing-reporting-and-results","title":"E2E Testing Reporting and Results","text":"<p>Include sample of E2E testing reports and results obtained from the E2E testing runs in this project.</p>"},{"location":"lab2/templates/test-type-template/","title":"Test Type Template","text":"<p>[Test Technique Name Here]</p> <p>Put a 2-3 sentence overview about the test technique here.</p>"},{"location":"lab2/templates/test-type-template/#when-to-use","title":"When To Use","text":""},{"location":"lab2/templates/test-type-template/#problem-addressed","title":"Problem Addressed","text":"<p>Describing the problem that this test type addresses, this should focus on the motivation behind the test type/technique to help the reader correlate this technique to their problem.</p>"},{"location":"lab2/templates/test-type-template/#when-to-avoid","title":"When to Avoid","text":"<p>Describe when NOT to use, if applicable.</p>"},{"location":"lab2/templates/test-type-template/#roi-tipping-point","title":"ROI Tipping Point","text":"<p>How much is enough?  For example, some opine that unit test ROI drops significantly at 80% block coverage and when the codebase is well-exercised by real traffic in production.</p>"},{"location":"lab2/templates/test-type-template/#applicable-to","title":"Applicable to","text":"<ul> <li> Local dev 'desktop'</li> <li> Build pipelines</li> <li> Non-production deployments</li> <li> Production deployments</li> </ul>"},{"location":"lab2/templates/test-type-template/#note-if-there-is-great-clear-succinct-documentation-for-the-technique-on-the-web-supply-a-pointer-and-skip-the-rest-of-this-template-no-need-to-re-type-content","title":"NOTE: If there is great (clear, succinct) documentation for the technique on the web, supply a pointer and skip the rest of this template.  No need to re-type content","text":""},{"location":"lab2/templates/test-type-template/#how-to-use","title":"How to Use","text":""},{"location":"lab2/templates/test-type-template/#architecture","title":"Architecture","text":"<p>Describe the components of the technique and how they interact with each other and the subject of the test technique.  Add a simple diagram of how the technique's parts are organized, if helpful to illustrate.</p>"},{"location":"lab2/templates/test-type-template/#pre-requisites","title":"Pre-requisites","text":"<p>Anything required in advance?</p>"},{"location":"lab2/templates/test-type-template/#high-level-step-by-step","title":"High-level Step-by-Step","text":"<p>1. 1. 1.</p>"},{"location":"lab2/templates/test-type-template/#best-practices-and-advice","title":"Best Practices and Advice","text":"<p>Describe what good testing looks like for this technique, best practices, pitfalls.</p>"},{"location":"lab2/templates/test-type-template/#anti-patterns","title":"Anti patterns","text":"<p>e.g. unit tests should never require off-box or even out-of-process dependencies.  Are there similar things to avoid when applying this technique?</p>"},{"location":"lab2/templates/test-type-template/#frameworks-tools-templates","title":"Frameworks, Tools, Templates","text":"<p>Describe known good (i.e. actually used and known to provide good results) frameworks, tools, templates, their pros and cons, with links.</p>"},{"location":"lab2/templates/test-type-template/#resources","title":"Resources","text":"<p>Provide links to further readings about this technique to dive deeper.</p>"},{"location":"lab2/ui-testing/","title":"User Interface Testing","text":"<p>This section is primarily geared towards web-based UIs, but the guidance is similar for mobile and OS based applications.  </p>"},{"location":"lab2/ui-testing/#applicability","title":"Applicability","text":"<p>UI Testing is not always going to be applicable, for example applications without a UI or parts of an application that require no human interaction.  In those cases unit, functional and integration/e2e testing would be the primary means.  UI Testing is going to be mainly applicable when dealing with a public facing UI that is used in a diverse environment or in a mission critical UI that requires higher fidelity.  With something like an admin UI that is used by just a handful of people, UI Testing is still valuable but not as high priority.</p>"},{"location":"lab2/ui-testing/#goals","title":"Goals","text":"<p>UI testing provides the ability to ensure that users have a consistent visual user experience across a variety of means of access and that the user interaction is consistent with the function requirements.</p> <ul> <li>Ensure the UI appearance and interaction satisfy the functional and non-functional requirements</li> <li>Detect changes in the UI both across devices and delivery platforms and between code changes</li> <li>Provide confidence to designers and developers the user experience is consistent</li> <li>Support fast code evolution and refactoring while reducing the risk of regressions</li> </ul>"},{"location":"lab2/ui-testing/#evidence-and-measures","title":"Evidence and Measures","text":"<p>Integrating UI Tests in to your CI/CD is necessary but more challenging than unit tests.  The increased challenge is that UI tests either need to run in headless mode with something like Puppeteer or there needs to be more extensive orchestration with Azure DevOps or GitHub that would handle the full testing integration for you like BrowserStack</p> <p>Integrations like <code>BrowserStack</code> are nice since they provide Azure DevOps reports as part of the test run.</p> <p>That said, Azure DevOps supports a variety of test adapters, so you can use any UI Testing framework that supports outputting the test results to one of the output formats listed at Publish Test Results task.</p> <p>If you're using an Azure DevOps pipeline to run UI tests, consider using a self hosted agent in order to manage framework versions and avoid unexpected updates.</p>"},{"location":"lab2/ui-testing/#general-guidance","title":"General Guidance","text":"<p>The scope of UI testing should be strategic. UI tests can take a significant amount of time to both implement and run, and it's challenging to test every type of user interaction in a production application due to the large number of possible interactions.</p> <p>Designing the UI tests around the functional tests makes sense.  For example, given an input form, a UI test would ensure that the visual representation is consistent across devices, is accessible and easy to interact with, and is consistent across code changes.</p> <p>UI Tests will catch 'runtime' bugs that unit and functional tests won't.  For example if the submit button for an input form is rendered but not clickable due to a positioning bug in the UI, then this could be considered a runtime bug that would not have been caught by unit or functional tests.</p> <p>UI Tests can run on mock data or snapshots of production data, like in QA or staging.</p>"},{"location":"lab2/ui-testing/#writing-tests","title":"Writing Tests","text":"<p>Good UI tests follow a few general principles:</p> <ul> <li>Choose a UI testing framework that enables quick feedback and is easy to use</li> <li>Design the UI to be easily testable.  For example, add CSS selectors or set the id on elements in a web page to allow easier selecting.</li> <li>Test on all primary devices that the user uses, don't just test on a single device or OS.</li> <li>When a test mutates data ensure that data is created on demand and cleaned up after.  The consequence of not doing this would be inconsistent testing.</li> </ul>"},{"location":"lab2/ui-testing/#common-issues","title":"Common Issues","text":"<p>UI Testing can get very challenging at the lower level, especially with a testing framework like Selenium.  If you choose to go this route, then you'll likely encounter timeouts, missing elements, and you'll have significant friction with the testing framework itself.  Due to many issues with UI testing there have been a number of free and paid solutions that help alleviate certain issues with frameworks like Selenium.  This is why you'll find Cypress in the recommended frameworks as it solves many of the known issues with Selenium.</p> <p>This is an important point though.  Depending on the UI testing framework you choose will result in either a smoother test creation experience, or a very frustrating and time-consuming one.  If you were to choose just Selenium the development costs and time costs would likely be very high.  It's better to use either a framework built on top of Selenium or one that attempts to solve many of the problems with something like Selenium.</p> <p>Note there that there are further considerations as when running in headless mode the UI can render differently than what you may see on your development machine, particularly with web applications.  Furthermore, note that when rendering in different page dimensions elements may disappear on the page due to CSS rules, therefore not be selectable by certain frameworks with default options out of the box.  All of these issues can be resolved and worked around, but the rendering demonstrates another particular challenge of UI testing.</p>"},{"location":"lab2/ui-testing/#specific-guidance","title":"Specific Guidance","text":"<p>Recommended testing frameworks:</p> <ul> <li>Web<ul> <li>BrowserStack</li> <li>Cypress</li> <li>Jest</li> <li>Selenium</li> <li>Appium</li> </ul> </li> </ul> <ul> <li>OS/Mobile Applications<ul> <li>Coded UI tests (CUITs)</li> <li>Xamarin.UITest</li> <li>BrowserStack</li> <li>Appium</li> </ul> </li> </ul> <p>Note that the framework listed above that is paid is BrowserStack, it's listed as it's an industry standard, the rest are open source and free.</p>"},{"location":"lab2/ui-testing/teams-tests/","title":"Automated UI Tests for a Teams Application","text":""},{"location":"lab2/ui-testing/teams-tests/#overview","title":"Overview","text":"<p>This is an overview on how you can implement UI tests for a custom Teams application. The insights provided can also be applied to automated end-to-end testing.</p>"},{"location":"lab2/ui-testing/teams-tests/#general-observations","title":"General Observations","text":"<ul> <li>Testing in a web browser is easier than on a native app.</li> <li>Testing a Teams app on a mobile device in an automated way is more challenging due to the fact that you are testing an app within an app:<ul> <li>There is no Android Application Package (APK) / iOS App Store Package (IPA) publicly available for Microsoft Teams app itself.</li> <li>Mobile testing frameworks are designed with the assumption that you own the APK/IPA of the app under test.</li> <li>Workarounds need to be found to first automate the installation of Teams.</li> </ul> </li> <li>Should you choose working with emulators, testing in a local Windows box is more stable than in a CI/CD. The latter involves a CI/CD agent and an emulator in a VM.</li> <li>When deciding whether to implement such tests, consider the project requirements as well as the advantages and disadvantages. Manual UI tests are often an acceptable solution due to their low effort requirements.</li> </ul> <p>The following are learnings from various engagements:</p>"},{"location":"lab2/ui-testing/teams-tests/#web-based-ui-tests","title":"Web Based UI Tests","text":"<p>To implement web-based UI tests for your Teams application, follow the same approach as you would for testing any other web application with a UI. UI testing provides valuable guidance in this regard. Your starting point for the test would be to automatically launch a browser (using Selenium or similar frameworks) and navigate to https://teams.microsoft.com.</p> <p>If you want to test a Teams app that hasn\u2019t been published in the Teams store yet or if you\u2019d like to test the DEV/QA version of your app, you can use the Teams Toolkit and package your app based on the manifest.json.</p> <pre><code>npx teamsfx package --env dev --manifest-path ...\n</code></pre> <p>Once the app is installed, implement selectors to access your custom app and to perform various actions within the app.</p>"},{"location":"lab2/ui-testing/teams-tests/#pipeline","title":"Pipeline","text":"<p>If you are using Selenium and Edge as the browser, consider leveraging the selenium/standalone-edge Docker image which contains a standalone Selenium server with the Microsoft Edge browser installed. By default, it will run in headless mode, but by setting <code>START_XVFB</code> variable to <code>True</code>, you can control whether to start a virtual framebuffer server (Xvfb) that allows GUI applications to run without a display. Below is a code snippet which illustrates the usage of the image in a Gitlab pipeline:</p> <pre><code>...\nrun-tests-dev:\n  allow_failure: false\n  image: ...\n  environment:\n    name: dev\n  stage: tests\n  services:\n    - name: selenium/standalone-edge:latest\n      alias: selenium\n      variables:\n        START_XVFB: \"true\"\n        description: \"Start Xvfb server\"\n...\n</code></pre> <p>When running a test, you need to use the Selenium server URL for remote execution. With the definition from above, the URL is: <code>http://selenium:4444/wd/hub</code>.</p> <p>The code snippet below illustrates how you can initialize the Selenium driver to point to the remote Selenium server using JavaScript:</p> <pre><code>var { Builder } = require(\"selenium-webdriver\");\nconst edge = require(\"selenium-webdriver/edge\");\n\nvar buildEdgeDriver = function () {\n  let builder = new Builder().forBrowser(\"MicrosoftEdge\");\n  builder = builder.usingServer(\"http://selenium:4444/wd/hub\");\n  builder.setEdgeOptions(new edge.Options().addArguments(\"--inprivate\"));\n  return builder.build();\n};\n</code></pre>"},{"location":"lab2/ui-testing/teams-tests/#mobile-based-ui-tests","title":"Mobile Based UI Tests","text":"<p>Testing your custom Teams application on mobile devices is a bit more difficult than using the web-based approach as it requires usage of actual or simulated devices. Running such tests in a CI/CD pipeline can be more difficult and resource-intensive.</p> <p>One approach is to use real devices or cloud-based emulators from vendors such as BrowserStack which requires a license. Alternatively, you can use virtual devices hosted in Azure Virtual Machines.</p>"},{"location":"lab2/ui-testing/teams-tests/#option-1-using-android-virtual-devices-avd","title":"Option 1: Using Android Virtual Devices (AVD)","text":"<p>This approach enables the creation of Android UI tests using virtual devices. It comes with the advantage of not requiring paid licenses to certain vendors. However, due to the nature of emulators, compared to real devices, it may prove to be less stable. Always choose the solution that best fits your project requirements and resources.</p> <p>Overall setup:</p> <p>AVD - Android Virtual Devices - which are virtual representation of physical Android devices.</p> <p>Appium is an open-source project designed to facilitate UI automation of many app platforms, including mobile.</p> <ul> <li>Appium is based on the W3C WebDriver specification. <p>Note: If you look at these commands in the WebDriver specification, you will notice that they are not defined in terms of any particular programming language. They are not Java commands, or JavaScript commands, or Python commands. Instead, they form part of an HTTP API which can be accessed from within any programming language.</p> </li> </ul> <ul> <li> <p>Appium implements a client-server architecture:</p> <ul> <li>The server (consisting of Appium itself along with any drivers or plugins you are using for automation) is connected to the devices under test, and is actually responsible for making automation happen on those devices. UiAutomator driver is compatible with Android platform.</li> <li> <p>The client is responsible for sending commands to the server over the network, and receiving responses from the server as a result. You can choose the language of your choice to write the commands. For example, for Javascript WebDriverIO can be used as client.</p> <p>Here you can read more about Appium ecosystem</p> </li> </ul> <ul> <li>The advantage of this architecture is that it opens the possibility of running the server in a VM, and the client in a pipeline, enabling the tests to be ran automatically on scheduled basis as part of CI/CD pipelines.</li> </ul> </li> </ul>"},{"location":"lab2/ui-testing/teams-tests/#how-to-run-mobile-tests-locally-on-a-windows-machine-using-avd","title":"How to Run Mobile Tests Locally on a Windows Machine Using AVD?","text":"<p>This approach involves:</p> <ul> <li>An emulator (AVD - Android Virtual Devices), which will represent the physical device.</li> <li>Appium server, which will redirect the commands from the test to your virtual device.</li> </ul>"},{"location":"lab2/ui-testing/teams-tests/#creating-an-android-virtual-device","title":"Creating an Android Virtual Device","text":"<ol> <li> <p>Install Android Studio from official link.</p> <p>Note: At the time of writing the documentation, the latest version available was Android Studio Giraffe, 2022.3.1 Patch 2 for Window.</p> <p>Set ANDROID_HOME environment variable to point to the installation path of Android SDK. i.e. <code>C:Users\\&lt;user-name&gt;\\AppData\\Local\\Android\\Sdk</code></p> </li> <li> <p>Install Java Development Kit (JDK) from official link. For the most recent devices JDK 9 is required, otherwise JDK 8 is required. Make sure you get the JDK and not the JRE.</p> <p>Set JAVA_HOME environment variable to the installation path, i.e.<code>C:\\Program Files\\Java\\jdk-11</code></p> </li> <li> <p>Create an AVD (Android Virtual Device): - Open Android Studio. From the Android Studio welcome screen, select More Action -&gt; Virtual Device Manager, as instructed here - Click Create Device. - Choose a device definition with Play Store enabled. This is important, otherwise Teams cannot be installed on the device. - Choose a System image from the Recommended tab which includes access to Google Play services. You may need to install it before selecting it. - Start the emulator by clicking on the Run button from the Device Manage screen. - Manually install Microsoft Teams from Google Playstore on the device.</p> </li> </ol>"},{"location":"lab2/ui-testing/teams-tests/#setting-up-appium","title":"Setting up Appium","text":"<p>Install <code>appium</code>:</p> <ol> <li>Download NodeJs, if it is not already installed on your machine: Download | Node.js (nodejs.org)</li> <li>Install Appium globally: Install Appium - Appium Documentation</li> <li>Install the UiAutomator2 driver: Install the UiAutomator2 Driver - Appium Documentation. Go through the <code>Set up Android automation requirements</code> in the documentation, to make sure you have set up everything correctly. Read more about Appium Drivers here. - Start appium server by running <code>appium</code> command in a command prompt.</li> </ol>"},{"location":"lab2/ui-testing/teams-tests/#useful-commands","title":"Useful commands","text":"<p>List emulators that you have previously created, without opening Android Studio:</p> <pre><code>emulator -list-avds\n</code></pre>"},{"location":"lab2/ui-testing/teams-tests/#how-to-run-teams-mobile-tests-in-a-pipeline-using-an-azure-vm","title":"How to run Teams mobile tests in a pipeline using an Azure VM?","text":"<p>This approach leverages the fact that Appium implements a client-server architecture. In this approach, the Appium server as well as the AVD run on an Azure VM, while the client operates within a pipeline and sends commands to be executed on the device.</p>"},{"location":"lab2/ui-testing/teams-tests/#configure-the-vm","title":"Configure the VM","text":"<p>This approach involves hosting a virtual device within a virtual machine. To set up the emulator (Android Virtual Device) in an Azure VM, the VM must support nested virtualization. Azure VM configuration which, at the time of writing the documentation, worked successfully with AVD and appium:</p> <ul> <li>Operating system: Windows (Windows-10 Pro)</li> <li>VM generation: V1</li> <li>Size: Standard D4ds v5 16 GiB memory</li> </ul>"},{"location":"lab2/ui-testing/teams-tests/#enable-connection-from-outside-to-appium-server-on-the-vm","title":"Enable connection from outside to Appium server on the VM","text":"<p>Note: By default appium server runs on port 4723. The rest of the steps will assume that this is the port where your appium server runs.</p> <p>In order to be able to reach appium server which runs on the VM from outside:</p> <ol> <li>Create an Inbound Rule for port 4723 from within the VM.</li> <li>Create an Inbound Security Rule in the NSG (Network Security Group) of the VM to be able to connect from that IP address to port 4723: - Find out the IP of the machine on which the tests will run on. - Replace the Source IP Address with the IP of your machine.</li> </ol>"},{"location":"lab2/ui-testing/teams-tests/#installing-android-studio-and-create-avd-inside-the-vm","title":"Installing Android Studio and create AVD inside the VM","text":"<ol> <li>Follow the instructions under the end to end tests on a Windows machine section to install Android Studio and create an Android Virtual Device.</li> <li>When you launch the emulator, it may show a warning as below and will eventually crash: </li> </ol> <p>Solution to fix it: 1. Enable Windows Hypervisor Platform 1. Enable Hyper-V (if not enabled by default) 1. Restart the VM. 1. Restart the AVD.</p>"},{"location":"lab2/ui-testing/teams-tests/#how-to-inspect-the-teams-app-in-an-azure-virtual-device-avd","title":"How to inspect the Teams app in an Azure Virtual Device (AVD)?","text":"<p>Inspecting the app is highly valuable when writing new tests, as it enables you to identify the unique IDs of various elements displayed on the screen. This process is similar to using DevTools, which allows you to navigate through the Document Object Model (DOM) of a web page.</p> <p>Appium Inspector is a very useful tool that allows you to inspect an app runing on an emulator.</p> <p>Note: This section assumes that you have already performed the prerequisites from How to run mobile test locally on a Windows machine using AVD?</p>"},{"location":"lab2/ui-testing/teams-tests/#steps","title":"Steps","text":"<ol> <li> <p>Run the appium server with --alow-cors flag by running the following command in a terminal:</p> <pre><code>appium --allow-cors\n</code></pre> </li> <li> <p>Go to https://inspector.appiumpro.com and type in the following properties:</p> <pre><code>{\n\"appium:deviceName\": \"your-emulator-name\",\n\"appium:appPackage\": \"com.microsoft.teams\",\n\"appium:appActivity\": \"com.microsoft.skype.teams.Launcher\",\n\"appium:automationName\": \"UiAutomator2\",\n\"platformName\": \"Android\"\n}\n</code></pre> </li> </ol> <ul> <li>\"appium:deviceName\" - is the name of your emulator. In Useful commands sections from above, you can see how to get the name of your AVD.</li> <li>\"appium:appPackage\" - is the name of the package, should be kept to \"com.microsoft.teams\".</li> <li>\"appium:appActivity\"-  is the name of the activity in the app that you want to launch, should be kept to \"com.microsoft.skype.teams.Launcher\"</li> <li>\"appium:automationName\" - is the name of the driver you are using, in this case, \"UiAutomator2\"</li> </ul> <p>If the appium server runs on your local machine at the default portal, then Remote Host and Remote Port can be kept to the default values.</p> <p>The configuration should look similar to the printscren below: </p> <ol> <li>Press on Start Session. - In the browser, you should see a similar view as below: </li> </ol> <ul> <li>You can do any action on the emulator, and if you press on the \"Refresh\" button in the browser, the left hand side of the Appium Inspector will reflect your app. In the App Source you will be able to see the IDs of the elements, so you can write relevant selectors in your tests.</li> </ul> <p>Connecting to Appium server</p> <p>Below it is outlined how this can be achieved with JavaScript. A similar approach can be followed for other languages. Assuming you are using webdriverio as the client, you would need to initialize the remote connection as follows:</p> <pre><code> const opts = {\n      port: 4723,\n      hostname: \"your-hostname\",\n      capabilities: {\n        platformName: \"android\",\n        \"appium:deviceName\": \"the-name-of-the-virtual-device\",\n        \"appium:appPackage\": \"com.microsoft.teams\",\n        \"appium:appActivity\": \"com.microsoft.skype.teams.Launcher\",\n        \"appium:automationName\": \"the-name-of-the-driver\",\n      },\n    };\n\n    // Create a new WebDriverIO instance with the Appium server URL and capabilities\n    await wdio.remote(opts);\n</code></pre> <ul> <li>\"port\": the port on which the Appium server runs on. By default, it is 4723.</li> <li>\"hostname\": the IP of the machine where the Appium sever runs on. If it is running locally, that is 127.0.0.1. If it runs in an Azure VM, it would be the public IP address of the VM. Note: ensure you have followed the steps from 2. Enable connection from outside to Appium server on the VM.</li> <li>\"platformName\": Appium can be used to connect to different platforms (Windows, iOS, Android). In our case, it would be \"android\".</li> <li>\"appium:deviceName\": the name of the Android Virtual Device. See Useful commands on how to find the name of the device.</li> <li>\"appium:appPackage\": the name of the app's package that you would like to launch. Teams' package name is \"com.microsoft.teams\".</li> <li>\"appium:appActivity\": the activity within Teams that you would like to launch on the device. In our case, we would like just to launch the app. The activity name for launching Teams is called \"com.microsoft.skype.teams.Launcher\".</li> <li>\"appium:automationName\": the name of the driver you are using. Note: Appium can communicate to different platforms. This is achieved by installing a dedicated driver, designed for each platform. In our case, it would be UiAutomator2 or Espresso, since they are both designed for Android platform.</li> </ul>"},{"location":"lab2/ui-testing/teams-tests/#option-2-using-browserstack","title":"Option 2: Using BrowserStack","text":"<p>BrowserStack serves as a cloud-based platform that enables developers to test both the web and mobile application across various browsers, operating systems, and real mobile devices. This can be seen as an alternative solution to the approach described earlier. The specific insights provided below relate to implementing such tests for a custom Microsoft Teams application:</p> <ul> <li>BrowserStack does not support out of the box the installation of Teams from the App Store or Play Store. However, there is a workaround, described in their documentation. Therefore, if you choose to go this way, you would first need to implement a step that installs Teams on the cloud-based device, by implementing the workaround described above.</li> <li>You may encounter issues with Google login, as it requires a newly created Google account, in order to log in to the store. To overcome this, make sure to disable 2FA from Google, further described in Troubleshooting Google login issues.</li> </ul>"},{"location":"lab2/unit-testing/","title":"Unit Testing","text":"<p>Unit testing is a fundamental tool in every developer's toolbox. Unit tests not only help us test our code, they encourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or documentation on how code functions. Properly written unit tests can also improve developer efficiency.</p> <p>Unit testing also is one of the most commonly misunderstood forms of testing. Unit testing refers to a very specific type of testing; a unit test should be:</p> <ul> <li>Provably reliable - should be 100% reliable so failures indicate a bug in the code</li> <li>Fast - should run in milliseconds, a whole unit testing suite shouldn't take longer than a couple seconds</li> <li>Isolated - removing all external dependencies ensures reliability and speed</li> </ul>"},{"location":"lab2/unit-testing/#why-unit-testing","title":"Why Unit Testing","text":"<p>It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we write them?</p> <p>Unit tests</p> <ul> <li>reduce costs by catching bugs earlier and preventing regressions</li> <li>increase developer confidence in changes</li> <li>speed up the developer inner loop</li> <li>act as documentation as code</li> </ul> <p>For more details, see all the detailed descriptions of the points above.</p>"},{"location":"lab2/unit-testing/#unit-testing-design-blocks","title":"Unit Testing Design Blocks","text":"<p>Unit testing is the lowest level of testing and as such generally has few components and dependencies.</p> <p>The system under test (abbreviated SUT) is the \"unit\" we are testing. Generally these are methods or functions, but depending on the language these could be different. In general, you want the unit to be as small as possible though.</p> <p>Most languages also have a wide suite of unit testing frameworks and test runners. These test frameworks have a wide range of functionality, but the base functionality should be a way to organize your tests and run them quickly.</p> <p>Finally, there is your unit test code; unit test code is generally short and simple, preferring repetition to adding layers and complexity to the code.</p>"},{"location":"lab2/unit-testing/#applying-the-unit-testing","title":"Applying the Unit Testing","text":"<p>Getting started with writing a unit test is much easier than some other test types since it should require next to no setup and is just code. Each test framework is different in how you organize and write your tests, but the general techniques and best practices of writing a unit test are universal.</p>"},{"location":"lab2/unit-testing/#techniques","title":"Techniques","text":"<p>These are some commonly used techniques that will help when authoring unit tests. For some examples, see the pages on using abstraction and dependency injection to author a unit test, or how to do test-driven development.</p> <p>Note that some of these techniques are more specific to strongly typed, object-oriented languages. Functional languages and scripting languages have similar techniques that may look different, but these terms are commonly used in all unit testing examples.</p>"},{"location":"lab2/unit-testing/#abstraction","title":"Abstraction","text":"<p>Abstraction is when we take an exact implementation detail, and we generalize it into a concept instead. This technique can be used in creating testable design and is used often especially in object-oriented languages. For unit tests, abstraction is commonly used to break a hard dependency and replace it with an abstraction. That abstraction then allows for greater flexibility in the code and allows for the a mock or simulator to be used in its place.</p> <p>One of the side effects of abstracting dependencies is that you may have an abstraction that has no test coverage. This is case where unit testing is not well-suited, you can not expect to unit test everything, things like dependencies will always be an uncovered case. This is why even if you have a robust unit testing suite, integration or functional testing should still be used - without that, a change in the way the dependency functions would never be caught.</p> <p>When building wrappers around third-party dependencies, it is best to keep the implementations with as little logic as possible, using a very simple facade that calls the dependency.</p> <p>An example of using abstraction can be found here.</p>"},{"location":"lab2/unit-testing/#dependency-injection","title":"Dependency Injection","text":"<p>Dependency injection is a technique which allows us to extract dependencies from our code. In a normal use-case of a dependant class, the dependency is constructed and used within the system under test. This creates a hard dependency between the two classes, which can make it particularly hard to test in isolation. Dependencies could be things like classes wrapping a REST API, or even something as simple as file access. By injecting the dependencies into our system rather than constructing them, we have \"inverted control\" of the dependency. You may see \"Inversion of Control\" and \"Dependency Injection\" used as separate terms, but it is very hard to have one and not the other, with some arguing that Dependency Injection is a more specific way of saying inversion of control. In certain languages such as C#, not using dependency injection can lead to code that is not unit testable since there is no way to inject mocked objects. Keeping testability in mind from the beginning and evaluating using dependency injection can save you from a time-intensive refactor later.</p> <p>One of the downsides of dependency injection is that it can easily go overboard. While there are no longer hard dependencies, there is still coupling between the interfaces, and passing around every interface implementation into every class presents just as many downsides as not using Dependency Injection. Being intentional with what dependencies get injected to what classes, is key to developing a maintainable system.</p> <p>Many languages include special Dependency Injection frameworks that take care of the boilerplate code and construction of the objects. Examples of this are Spring in Java or built into ASP.NET Core</p> <p>An example of using dependency injection can be found here.</p>"},{"location":"lab2/unit-testing/#test-driven-development","title":"Test-Driven Development","text":"<p>Test-Driven Development (TDD) is less a technique in how your code is designed, but a technique for writing your code that will lead you to a testable design from the start. The basic premise of test-driven development is that you write your test code first and then write the system under test to match the test you just wrote. This way all the test design is done up front and by the time you finish writing your system code, you are already at 100% test pass rate and test coverage. It also guarantees testable design is built into the system since the test was written first!</p> <p>For more information on TDD and an example, see the page on Test-Driven Development</p>"},{"location":"lab2/unit-testing/#best-practices","title":"Best Practices","text":""},{"location":"lab2/unit-testing/#arrangeactassert","title":"Arrange/Act/Assert","text":"<p>One common form of organizing your unit test code is called Arrange/Act/Assert. This divides up your unit test into 3 different discrete sections:</p> <ol> <li>Arrange - Set up all the variables, mocks, interfaces, and state you will need to run the test</li> <li>Act - Run the system under test, passing in any of the above objects that were created</li> <li>Assert - Check that with the given state that the system acted appropriately.</li> </ol> <p>Using this pattern to write tests makes them very readable and also familiar to future developers who would need to read your unit tests.</p>"},{"location":"lab2/unit-testing/#example","title":"Example","text":"<p>Let's assume we have a class <code>MyObject</code> with a method <code>TrySomething</code> that interacts with an array of strings, but if the array has no elements, it will return false. We want to write a test that checks the case where array has no elements:</p> <pre><code>[Fact]\npublic void TrySomething_NoElements_ReturnsFalse()\n{\n    // Arrange\n    var elements = Array.Empty&lt;string&gt;();\n    var myObject = new MyObject();\n\n    // Act\n    var myReturn = myObject.TrySomething(elements);\n\n    // Assert\n    Assert.False(myReturn);\n}\n</code></pre>"},{"location":"lab2/unit-testing/#keep-tests-small-and-test-only-one-thing","title":"Keep Tests Small and Test Only One Thing","text":"<p>Unit tests should be short and test only one thing. This makes it easy to diagnose when there was a failure without needing something like which line number the test failed at. When using Arrange/Act/Assert, think of it like testing just one thing in the \"Act\" phase.</p> <p>There is some disagreement on whether testing one thing means \"assert one thing\" or \"test one state, with multiple asserts if needed\". Both have their advantages and disadvantages, but as with most technical disagreements there is no \"right\" answer. Consistency when writing your tests one way or the other is more important!</p>"},{"location":"lab2/unit-testing/#using-a-standard-naming-convention-for-all-unit-tests","title":"Using a Standard Naming Convention for All Unit Tests","text":"<p>Without having a set standard convention for unit test names, unit test names end up being either not descriptive enough, or duplicated across multiple different test classes. Establishing a standard is not only important for keeping your code consistent, but a good standard also improves the readability and debug-ability of a test. In this article, the convention used for all unit tests has been <code>UnitName_StateUnderTest_ExpectedResult</code>, but there are lots of other possible conventions as well, the important thing is to be consistent and descriptive. Having descriptive names such as the one above makes it trivial to find the test when there is a failure, and also already explains what the expectation of the test was and what state caused it to fail. This can be especially helpful when looking at failures in a CI/CD system where all you know is the name of the test that failed - instead now you know the name of the test and exactly why it failed (especially coupled with a test framework that logs helpful output on failures).</p>"},{"location":"lab2/unit-testing/#things-to-avoid","title":"Things to Avoid","text":"<p>Some common pitfalls when writing a unit test that are important to avoid:</p> <ul> <li>Sleeps - A sleep can be an indicator that perhaps something is making a request to a dependency that it should not be.   In general, if your code is flaky without the sleep, consider why it is failing and if you can remove the flakiness by   introducing a more reliable way to communicate potential state changes. Adding sleeps to your unit tests also breaks   one of our original tenets of unit testing: tests should be fast, as in order of milliseconds. If tests are taking on   the order of seconds, they become more cumbersome to run.</li> <li>Reading from disk - It can be really tempting to the expected value of a function return in a file and read that file   to compare the results. This creates a dependency with the system drive, and it breaks our tenet of keeping our unit   tests isolated and 100% reliable. Any outside dependency such as file system access could potentially cause   intermittent failures. Additionally, this could be a sign that perhaps the test or unit under test is too complex and   should be simplified.</li> <li>Calling third-party APIs - When you do not control a third-party library that you are calling into, it's impossible to   know for sure what that is doing, and it is best to abstract it out. Otherwise, you may be making REST calls or other   potential areas of failure without directly writing the code for it. This is also generally a sign that the design of   the system is not entirely testable. It is best to wrap third party API calls in interfaces or other structures so   that they do not get invoked in unit tests. For more information see the page on mocking.</li> </ul>"},{"location":"lab2/unit-testing/#unit-testing-frameworks-and-tools","title":"Unit Testing Frameworks and Tools","text":""},{"location":"lab2/unit-testing/#test-frameworks","title":"Test Frameworks","text":"<p>Unit test frameworks are constantly changing. For a full list of every unit testing framework see the page on Wikipedia. Frameworks have many features and should be picked based on which feature-set fits best for the particular project.</p>"},{"location":"lab2/unit-testing/#mock-frameworks","title":"Mock Frameworks","text":"<p>Many projects start with both a unit test framework, and also add a mock framework. While mocking frameworks have their uses and sometimes can be a requirement, it should not be something that is added without considering the broader implications and risks associated with heavy usage of mocks.</p> <p>To see if mocking is right for your project, or if a mock-free approach is more appropriate, see the page on mocking.</p>"},{"location":"lab2/unit-testing/#tools","title":"Tools","text":"<p>These tools allow for constant running of your unit tests with in-line code coverage, making the dev inner loop extremely fast and allows for easy TDD:</p> <ul> <li>Visual Studio Live Unit Testing</li> <li>Wallaby.js</li> <li>Infinitest for Java</li> <li>PyCrunch for Python</li> </ul>"},{"location":"lab2/unit-testing/#things-to-consider","title":"Things to Consider","text":""},{"location":"lab2/unit-testing/#transferring-responsibility-to-integration-tests","title":"Transferring Responsibility to Integration Tests","text":"<p>In some situations it is worth considering to include the integration tests in the inner development loop to provide a sufficient code coverage to ensure the system is working properly. The prerequisite for this approach to be successful is to have integration tests being able to execute at a speed comparable to that of unit tests both locally and in a CI environment. Modern application frameworks like .NET or Spring Boot combined with the right mocking or stubbing approach for external dependencies offer excellent capabilities to enable such scenarios for testing.</p> <p>Usually, integration tests only prove that independently developed modules connect together as designed. The test coverage of integration tests can be extended to verify the correct behavior of the system as well. The responsibility of providing a sufficient branch and line code coverage can be transferred from unit tests to integration tests. Instead of several unit tests needed to test a specific case of functionality of the system, one integration scenario is created that covers the entire flow. For example in case of an API, the received HTTP responses and their content are verified for each request in test. This covers both the integration between components of the API and the correctness of its business logic.</p> <p>With this approach efficient integration tests can be treated as an extension of unit testing, taking over the responsibility of validating happy/failure path scenarios. It has the advantage of testing the system as a black box without any knowledge of its internals. Code refactoring has no impact on tests. Common testing techniques as TDD can be applied at a higher level which results in a development process that is driven by acceptance tests. Depending on the project specifics unit tests still play an important role. They can be used to help dictate a testable design at a lower level or to test complex business logic and corner cases if necessary.</p>"},{"location":"lab2/unit-testing/#conclusion","title":"Conclusion","text":"<p>Unit testing is extremely important, but it is also not the silver bullet; having proper unit tests is just a part of a well-tested system. However, writing proper unit tests will help with the design of your system as well as help catch regressions, bugs, and increase developer velocity.</p>"},{"location":"lab2/unit-testing/#resources","title":"Resources","text":"<ul> <li>Unit Testing Best Practices</li> </ul>"},{"location":"lab2/unit-testing/authoring-example/","title":"Writing a Unit Test","text":"<p>To illustrate some unit testing techniques for an object-oriented language, let's start with an example of some code we wish to add unit tests for. In this example, we have a configuration class that contains all the startup options for an app we are writing. Normally it reads from a <code>.config</code> file, but we are having three problems with the current implementation:</p> <ol> <li>There is a bug in the Configuration class, and we have no unit tests since it relies on reading a config file</li> <li>We can't unit test any of the code that relies on the Configuration class reading a config file</li> <li>In the future, we want to allow for configuration to be saved in the cloud and accessed via REST api.</li> </ol> <p>The bug we are trying to fix is that if there are multiple empty lines in the configuration file, an IndexOutOfRangeException is being thrown. Our class currently looks like this:</p> <pre><code>using System.IO;\nusing System.Linq;\n\npublic class Configuration\n{\n    // Public getter properties from configuration object\n    public string MyProperty { get; private set; }\n\n    public void Initialize()\n    {\n        var configContents = File.ReadAllLines(\".config\");\n\n        // Config is in the format: key=value\n        var config = configContents.Select(l =&gt; l.Split('='))\n                                   .ToDictionary(kv =&gt; kv[0], kv =&gt; kv[1]);\n\n        // Assign all properties here\n        this.MyProperty = config[\"myproperty\"];\n    }\n}\n</code></pre>"},{"location":"lab2/unit-testing/authoring-example/#abstraction","title":"Abstraction","text":"<p>In our example, we have a single dependency: the file system. Rather than just abstracting the file system entirely, let us think about why we need the file system and abstract the concept rather than the implementation. In this case, we are using the <code>File</code> class to read from the config file, and the config contents. The abstraction concept here is some form or configuration reader that returns each line of the configuration in a string array. We could call it <code>ConfigurationReader</code>, and it has a single method, <code>Read</code>, which returns the contents.</p> <p>When creating abstractions, it can be good practice creating an interface for that abstraction, in languages that support it. In the example with C#, we can create an <code>IConfigurationReader</code> interface, and instead of just having a <code>ConfigurationReader</code> class we can be more specific and name if <code>FileConfigurationReader</code> to indicate that it reads from the file system:</p> <pre><code>// IConfigurationReader.cs\npublic interface IConfigurationReader\n{\n    string[] Read();\n}\n\n// FileConfigurationReader.cs\npublic class FileConfigurationReader : IConfigurationReader\n{\n    public string[] Read()\n    {\n        return File.ReadAllLines(\".config\");\n    }\n}\n</code></pre> <p>Now that the file dependency has been abstracted away, we need to update our Configuration class's Initialize method to use the new abstraction instead of calling <code>File.ReadAllLines</code> directly:</p> <pre><code>public void Initialize()\n{\n    var configContents = new FileConfigurationReader().Read();\n\n    // Config is in the format: key=value\n    var config = configContents.Select(l =&gt; l.Split('='))\n                               .ToDictionary(kv =&gt; kv[0], kv =&gt; kv[1]);\n\n    // Assign all properties here\n    this.MyProperty = config[\"myproperty\"];\n}\n</code></pre> <p>As you can see, we still have a dependency on the file system, but that dependency has been abstracted out. We will need to use other techniques to break the dependency completely.</p>"},{"location":"lab2/unit-testing/authoring-example/#dependency-injection","title":"Dependency Injection","text":"<p>In the previous section, we abstracted the file access into a <code>FileConfigurationReader</code> but we still had a dependency on the file system in our function. We can use dependency injection to inject the right reader into our <code>Configuration</code> class:</p> <pre><code>using System.IO;\nusing System.Linq;\n\npublic class Configuration\n{\n    private readonly IConfigurationReader configReader;\n\n    // Public getter properties from configuration object\n    public string MyProperty { get; private set; }\n\n    public Configuration(IConfigurationReader reader)\n    {\n        this.configReader = reader;\n    }\n\n    public void Initialize()\n    {\n        var configContents = configReader.Read();\n\n        // Config is in the format: key=value\n        var config = configContents.Select(l =&gt; l.Split('='))\n                                   .ToDictionary(kv =&gt; kv[0], kv =&gt; kv[1]);\n\n        // Assign all properties here\n        this.MyProperty = config[\"myproperty\"];\n    }\n}\n</code></pre> <p>Above, a technique was used called Constructor Injection. This uses the object's constructor to set what our dependencies will be, which means whichever object creates the <code>Configuration</code> object will control which reader needs to get passed in. This is an example of \"inversion of control\", previously the <code>Configuration</code> object controlled the dependency, but instead we pushed up the control to whatever component creates this object.</p> <p>Note that we injected the interface <code>IConfigurationReader</code> and not the concrete class. This is what allows us to break the dependency; whereas originally we had a hard-coded dependency on the <code>File</code> class, now we only depend on an object that implements <code>IConfigurationReader</code>.</p>"},{"location":"lab2/unit-testing/authoring-example/#writing-our-first-unit-tests","title":"Writing our first unit tests","text":"<p>We started down this venture because we have a bug in the <code>Configuration</code> class that was not caught because we do not have unit tests. Let us write some unit tests that gives us full coverage of the <code>Configuration</code> class, including a test that tests the scenario described by the bug (if there are multiple empty lines in the configuration file, an IndexOutOfRangeException is being thrown).</p> <p>However, we still have one problem, we only have a single implementation of <code>IConfigurationReader</code>, and it uses the file system, meaning any unit tests we write will still have a dependency on the file system! Luckily since we used dependency injection, all we need to do is create an implementation of <code>IConfigurationReader</code> that does not depend on the file system. We could create a mock here, but instead let's create a concrete implementation of the interface which simply returns the passed in string[] - we can call it <code>PassThroughConfigurationReader</code> (for more details on why this approach may be better than mocking, see the page on mocking)</p> <pre><code>public class PassThroughConfigurationReader : IConfigurationReader\n{\n    private readonly string[] contents;\n\n    public PassThroughConfigurationReader(string[] contents)\n    {\n        this.contents = contents;\n    }\n\n    public string[] Read()\n    {\n        return this.contents;\n    }\n}\n</code></pre> <p>This simple class will be used in our unit tests, so we can create different states without requiring lots of file access. Now that we have this in place, we can go ahead and write our unit tests, starting with the tests that describe the current behavior:</p> <pre><code>public class ConfigurationTests\n{\n    [Fact]\n    public void Initialize_EmptyConfig_Throws()\n    {\n        var reader = new PassThroughConfigurationReader(Array.Empty&lt;string&gt;());\n        var config = new Configuration(reader);\n\n        Assert.Throws&lt;KeyNotFoundException&gt;(() =&gt; config.Initialize());\n    }\n\n    [Fact]\n    public void Initialize_CorrectFormat_SetsProperty()\n    {\n        var reader = new PassThroughConfigurationReader(new[] {\n            \"myproperty=myvalue\"\n        });\n        var config = new Configuration(reader);\n\n        config.Initialize();\n\n        Assert.Equal(\"myvalue\", config.MyProperty);\n    }\n}\n</code></pre>"},{"location":"lab2/unit-testing/authoring-example/#fixing-the-bug","title":"Fixing the Bug","text":"<p>All our current tests pass, and give us 100% coverage, however as evidenced by the bug, we must not be covering all possible inputs and outputs. In the case of the bug, multiple empty lines would cause an issue. Additionally, <code>KeyNotFoundException</code> is not a very friendly exception and is an implementation detail, not something that makes sense when designing the Configuration API. Let's add some more tests and align the tests with how we think the <code>Configuration</code> class should behave:</p> <pre><code>public class ConfigurationTests\n{\n    [Fact]\n    public void Initialize_EmptyConfig_Throws()\n    {\n        var reader = new PassThroughConfigurationReader(Array.Empty&lt;string&gt;());\n        var config = new Configuration(reader);\n\n        Assert.Throws&lt;InvalidOperationException&gt;(() =&gt; config.Initialize());\n    }\n\n    [Fact]\n    public void Initialize_MalformedLine_Throws()\n    {\n        var reader = new PassThroughConfigurationReader(new[] {\n            \"myproperty\",\n        });\n        var config = new Configuration(reader);\n\n        Assert.Throws&lt;InvalidOperationException&gt;(() =&gt; config.Initialize());\n    }\n\n    [Fact]\n    public void Initialize_MultipleEqualSigns_PropertyContainsNoEquals()\n    {\n        var reader = new PassThroughConfigurationReader(new[] {\n            \"myproperty=myval1=myval2\",\n        });\n        var config = new Configuration(reader);\n\n        config.Initialize();\n\n        Assert.Equal(\"myval1=myval2\", config.MyProperty);\n    }\n\n    [Fact]\n    public void Initialize_WithBlankLines_Ignores()\n    {\n        var reader = new PassThroughConfigurationReader(new[] {\n            \"myproperty=myvalue\",\n            string.Empty,\n        });\n        var config = new Configuration(reader);\n\n        config.Initialize();\n\n        Assert.Equal(\"myvalue\", config.MyProperty);\n    }\n\n    [Fact]\n    public void Initialize_CorrectFormat_SetsProperty()\n    {\n        var reader = new PassThroughConfigurationReader(new[] {\n            \"myproperty=myvalue\"\n        });\n        var config = new Configuration(reader);\n\n        config.Initialize();\n\n        Assert.Equal(\"myvalue\", config.MyProperty);\n    }\n}\n</code></pre> <p>Now we have 4 failing tests and 1 passing test, but we have firmly established through the use of these tests how we expect callers to user the Configuration class and what is and isn't allowed as inputs. Now we just need to fix the <code>Configuration</code> class so that our tests pass:</p> <pre><code>public void Initialize()\n{\n    var configContents = configReader.Read();\n\n    if (configContents.Length == 0)\n    {\n        throw new InvalidOperationException(\"Empty config\");\n    }\n\n    // Config is in the format: key=value\n    var config = configContents.Where(l =&gt; !string.IsNullOrWhiteSpace(l))\n                               .Select(l =&gt;\n                               {\n                                   var splitLine = l.Split('=', 2);\n                                   if (splitLine.Length &lt; 2)\n                                   {\n                                       throw new InvalidOperationException(\"Malformed line\");\n                                   }\n                                   return splitLine;\n                               })\n                               .ToDictionary(kv =&gt; kv[0], kv =&gt; kv[1]);\n\n    // Assign all properties here\n    this.MyProperty = config[\"myproperty\"];\n}\n</code></pre> <p>Now all our tests pass! We have fixed our bug, added unit tests to the <code>Configuration</code> class, and have much higher confidence in future changes.</p>"},{"location":"lab2/unit-testing/authoring-example/#untestable-code","title":"Untestable Code","text":"<p>As described in the abstraction section, not all code can be properly unit tested. In our case we have a single class that has 0% test coverage: <code>FileConfigurationReader</code>. This is expected; in this case we kept <code>FileConfigurationReader</code> as light as possible with no additional logic other than calling into the third-party dependency. <code>FileConfigurationReader</code> is an example of the facade design pattern.</p>"},{"location":"lab2/unit-testing/authoring-example/#testable-design-and-future-improvements","title":"Testable Design and Future Improvements","text":"<p>One of our original problems described in this example is that in the future we expect to load the configuration from a web API. By doing all the work of abstracting the way we load the configuration text and breaking the dependency on the file system, we have already done all the hard work to enable this future scenario! All that needs to be done next is to create a <code>WebApiConfigurationReader</code> implementation and use that the construct the <code>Configuration</code> object, and it should just work.</p> <p>That is one of the benefits of testable design, in the process of writing our tests in a safe way, a side effect of that is that we already have our dependencies that might change abstracted, and will require minimal changes to implement.</p> <p>Another added benefit is we have multiple possibilities opened by this testable design. For example, we can have a cascading configuration set up now using all 3 <code>IConfigurationReader</code> implementations, including the one we wrote only for our tests! We can first check if internet access is available and if so use <code>WebApiConfigurationReader</code>. If no internet is available, we can fall back to the local config file on the current system using <code>FileConfigurationReader</code>. If for some reason the config file does not exist, we can use the <code>PassThroughConfigurationReader</code> as a hard-coded default configuration somewhere in the code. We have full flexibility to do whatever we may need to do in the future!</p>"},{"location":"lab2/unit-testing/custom-connector/","title":"Custom Connector Testing","text":"<p>When developing Custom Connectors to put data into the Power Platform there are some strategies you can follow:</p>"},{"location":"lab2/unit-testing/custom-connector/#unit-testing","title":"Unit Testing","text":"<p>There are several verifications one can do while developing custom connectors in order to be sure the code is working properly.</p> <p>There are two main ones:</p> <ul> <li>Validating the OpenAPI schema which the connector is defined.</li> <li>Validating if the schema also have all the information necessary for the certified connector process.</li> </ul> <p>(the later one is optional, but necessary in case you want to publish it as a certified connector).</p> <p>There are several tool to help validate the OpenAPI schema, a list of them are available in this link. A suggested tool would be swagger-cli.</p> <p>On the other hand, to validate if the custom connector you are building is correct to become a certified connector, use the paconn-cli, since it has a validate command that shows missing information from the custom connector definition.</p>"},{"location":"lab2/unit-testing/mocking/","title":"Mocking in Unit Tests","text":"<p>One of the key components of writing unit tests is to remove the dependencies your system has and replacing it with an implementation you control. The most common method people use as the replacement for the dependency is a mock, and mocking frameworks exist to help make this process easier.</p> <p>Many frameworks and articles use different meanings for the differences between test doubles. A test double is a generic term for any \"pretend\" object used in place of a real one. This term, as well as others used in this page are the definitions provided by Martin Fowler. The most commonly used form of test double is Mocks, but there are many cases where Mocks perhaps are not the best choice and Fakes should be considered instead.</p>"},{"location":"lab2/unit-testing/mocking/#stubs","title":"Stubs","text":"<p>Stub allows you to have predetermined behavior that substitutes real behavior. The dependency (abstract class or interface) is implemented as a stub with a logic as expected by the client. Stubs can be useful when the clients of the stubs all expect the same set of responses, e.g. you use a third party service. The key concept here is that stubs should never fail a unit or integration test where a mock can. Stubs do not require any sort of framework to run, but are usually supported by mocking frameworks to quickly build the stubs. Stubs are commonly used in combination with a dependency injection frameworks or libraries, where the real object is replaced by a stub implementation.</p> <p>Stubs can be useful especially during early development of a system, but since nearly every test requires its own stubs (to test the different states), this quickly becomes repetitive and involves a lot of boilerplate code. Rarely will you find a codebase that uses only stubs for mocking, they are usually paired with other test doubles.</p> <p>Stubs do not require any sort of framework to run, but are usually supported by mocking frameworks to quickly build the stubs.</p> <pre><code># Python test example, that creates an application\n# with a dependency injection framework an overrides\n# a service with a stub\n\nclass StubTestCase(TestBase):\n    def setUp(self) -&gt; None:\n        super(StubTestCase, self).setUp()\n        self.app.container.service_a.override(StubService())\n\n    def test_service():\n        service = self.app.container.service_a()\n        self.assertTrue(isinstance(service, StubService))\n</code></pre>"},{"location":"lab2/unit-testing/mocking/#upsides","title":"Upsides","text":"<ul> <li>Do not require any framework, easy to set up.</li> </ul>"},{"location":"lab2/unit-testing/mocking/#downsides","title":"Downsides","text":"<ul> <li>Can involve rewriting the same code many times, lots of boilerplate.</li> </ul>"},{"location":"lab2/unit-testing/mocking/#mocks","title":"Mocks","text":"<p>Fowler describes mocks as pre-programmed objects with expectations which form a specification of the calls they are expected to receive. In other words, mocks are a replacement object for the dependency that has certain expectations that are placed on it; those expectations might be things like validating a sub-method has been called a certain number of times or that arguments are passed down in a certain way.</p> <p>Mocking frameworks are abundant for every language, with some languages having mocks built into the unit test packages. They make writing unit tests easy and still encourage good unit testing practices.</p> <p>The main difference between a mock and most of the other test doubles is that mocks do behavioral verification, whereas other test doubles do state verification. With behavioral verification, you end up testing that the implementation of the system under test is as you expect, whereas with state verification the implementation is not tested, rather the inputs and the outputs to the system are validated.</p> <p>The major downside to behavioral verification is that it is tied to the implementation. One of the biggest advantages of writing unit tests is that when you make code changes you have confidence that if your unit tests continue to pass, that you are making a relatively safe change. If tests need to be updated every time because the behavior of the method has changed, then you lose that confidence because bugs could also be introduced into the test code. This also increases the development time and can be a source of frustration.</p> <p>For example, let's assume you have a method that you are testing that makes 5 web service calls. With mocks, one of your tests could be to check that those 5 web service calls were made. Sometime later the API is updated and only a single web service call needs to be made. Once the system code is changed, the unit test will fail because it expects 5 calls and not 1. The test needs to be updated, which results in lowered confidence in the change, as well as potentially introduces more areas for bugs to sneak in.</p> <p>Some would argue that in the example above, the unit test is not a good test anyway because it depends on the implementation, and that may be true; but one of the biggest problems with using mocks (and specifically mocking frameworks that allow these verifications), is that it encourages these types of tests to be written. By not using a mock framework that allows this, you never run the risk of writing tests that are validating the implementation.</p>"},{"location":"lab2/unit-testing/mocking/#upsides-to-mocking","title":"Upsides to Mocking","text":"<ul> <li>Easy to write.</li> <li>Encourages testable design.</li> </ul>"},{"location":"lab2/unit-testing/mocking/#downsides-to-mocking","title":"Downsides to Mocking","text":"<ul> <li>Behavioral testing can present problems with maintainability in unit test code.</li> <li>Usually requires a framework to be installed (or if no framework, lots of boilerplate code)</li> </ul>"},{"location":"lab2/unit-testing/mocking/#fakes","title":"Fakes","text":"<p>Fake objects actually have working implementations, but usually take some shortcut which may make them not suitable for production. One of the common examples of using a Fake is an in-memory database - typically you want your database to be able to save data somewhere between application runs, but when writing unit tests if you have a fake implementation of your database APIs that are store all data in memory, you can use these for unit tests and not break abstraction as well as still keep your tests fast.</p> <p>Writing a fake does take more time than other test doubles, because they are full implementations, and can have their own suite of unit tests. In this sense though, they increase confidence in your code even more because your test double has been thoroughly tested for bugs before you even use it as a downstream dependency.</p> <p>Similarly to mocks, fakes also promote testable design, but unlike mocks they do not require any frameworks to write. Writing a fake is as easy as writing any other implementation class. Fakes can be included in the test code only, but many times they end up being \"promoted\" to the product code, and in some cases can even start off in the product code since it is held to the same standard with full unit tests. Especially if writing a library or an API that other developers can use, providing a fake in the product code means those developers no longer need to write their own mock implementations, further increasing re-usability of code.</p>"},{"location":"lab2/unit-testing/mocking/#upsides-to-fakes","title":"Upsides to Fakes","text":"<ul> <li>No framework needed, is just like any other implementation.</li> <li>Encourages testable design.</li> <li>Code can be \"promoted\" to product code, so it is not wasted effort.</li> </ul>"},{"location":"lab2/unit-testing/mocking/#downsides-to-fakes","title":"Downsides to Fakes","text":"<ul> <li>Takes more time to implement.</li> </ul>"},{"location":"lab2/unit-testing/mocking/#best-practices","title":"Best Practices","text":"<p>To keep your mocking efficient, consider these best practices to make your code testable, save time and make your test assertions more meaningful.</p>"},{"location":"lab2/unit-testing/mocking/#dependency-injection","title":"Dependency Injection","text":"<p>If you don\u2019t keep testability in mind from the beginning, once you start writing your tests, you might realize you have to do a time-intensive refactor to make the code unit testable. A common problem that can lead to non-testable code in certain languages such as C# is not using dependency injection. Consider using dependency injection so that a mock can easily be injected into your Subject Under Test (SUT) during a unit test.</p> <p>More information on using dependency injection can be found here.</p>"},{"location":"lab2/unit-testing/mocking/#assertions","title":"Assertions","text":"<p>When it comes to assertions in unit tests you want to make sure that you assert the right things, not necessarily lots of things. Some assertions can be inefficient and not give you the confidence you need in the test result. When you are mocking a client or configuration and your method passes the mock result directly as a return value without significant changes, consider not asserting on the return value. Because if you do, you are mainly asserting whether you set up the mock correctly. For a very simple example, look at this class:</p> <pre><code>public class SearchController : ControllerBase {\n\n   public ISearchClient SearchClient { get; }\n\n   public SearchController(ISearchClient searchClient)\n   {\n      SearchClient = searchClient;\n   }\n\n   public String GetName(string id)\n   {\n      return this.SearchClient.GetName(id);\n   }\n}\n</code></pre> <p>When testing the <code>GetName</code> method, you can set up a mock search client to return a certain value. Then, it\u2019s easy to assert that the return value is, in fact, this value from the mock.</p> <pre><code>mockSearchClient.Setup(x =&gt; x.GetName(id))\n   .ReturnsAsync(\"myResult\");\nvar result = searchController.GetName(id);\nAssert.Equal(\"myResult\",result.Value);\n</code></pre> <p>But now, your method could look like this, and the test would still pass:</p> <pre><code>public String GetName(string id)\n{\n   return \"myResult\";\n}\n</code></pre> <p>Similarly, if you set up your mock wrong, the test would fail even though the logic inside the method is sound. For efficient assertions that will give you confidence in your SUT, make assertions on your logic, not mock return values. The simple example above doesn\u2019t have a lot of logic, but you want to make sure that it calls the search client to retrieve the result. For this, you can use the verify method to make sure the search client was called using the right parameters even though you don\u2019t care about the result.</p> <pre><code>mockSearchClient.Verify(mock =&gt; mock.GetName(id), Times.Once());\n</code></pre> <p>This example is kept simple to visualize the principle of making meaningful assertions. In a real world application, your SUT will probably have more logic inside. Pieces of glue code that have as little logic as this example don't always have to be unit tested and might instead be covered by integration tests. If there is more logic and a unit test with mocking is required, you should apply this principle by verifying mock calls and making assertions on the part of the mock result that was modified by your SUT.</p>"},{"location":"lab2/unit-testing/mocking/#callbacks","title":"Callbacks","text":"<p>It can be time-consuming to set up mocks if you want to make sure they are being called with the right parameters, especially if the parameters are complex. To make your testing more efficient, consider using callbacks to make assertions on the parameters after a method was called. Often you don\u2019t care about all the parameters but only a few, or even only parts of them if the parameters are also objects. It\u2019s easy to make a small mistake in the creation of the parameter, like missing an attribute that the actual method sets, and then your mock won\u2019t be called, even though you might not care about this attribute at all. To avoid this, you can define only the most relevant parameters to differentiate between method calls and use an <code>any</code>-statement for the others. In this example, the method has a complex search options parameter which would take a lot of time to set up manually. Since you only care about 2 attributes in the search options, you use an <code>any</code>-statement and store the options in a callback for later assertions.</p> <pre><code>var actualOptions = new SearchOptions();\n\nmockSearchClient\n   .Setup(x =&gt;\n      x.Search(\n         \"[This parameter is most relevant]\",\n         It.IsAny&lt;SearchOptions&gt;()\n      )\n   )\n   .Returns(mockResults)\n   .Callback&lt;string, SearchOptions&gt;((query, searchOptions) =&gt;\n     {\n       actualOptions = searchOptions;\n     }\n   );\n</code></pre> <p>Since you want to test your method logic, you should care only about the parts of the parameter which are influenced by your SUT, in this example, let's say the search mode and the search query type. So, with the variable you stored in the callback, you can make assertions on only these two attributes.</p> <pre><code>Assert.Equal(SearchMode.All, actualOptions.SearchMode);\nAssert.Equal(SearchQueryType.Full, actualOptions.QueryType);\n</code></pre> <p>This makes the test more explicit since it shows which parts of the logic you care about. It\u2019s also more efficient since you don\u2019t have to spend a lot of time setting up the parameters for the mock.</p>"},{"location":"lab2/unit-testing/mocking/#conclusion","title":"Conclusion","text":"<p>Using test doubles in unit tests is an essential part of having a healthy test suite. When looking at mocking frameworks and using test doubles, it is important to consider the future implications of integrating with a mocking framework from the start. Sometimes certain features of mocking frameworks seem essential, but usually that is a sign that the code itself is not abstracted enough if it requires a framework.</p> <p>If possible, starting without a mocking framework and attempting to create fake implementations will lead to a more healthy code base, but when that is not possible the onus is on the technical leaders of the team to find cases where mocks may be overused, rely too much on implementation details, or end up not testing the right things.</p>"},{"location":"lab2/unit-testing/tdd-example/","title":"Test-Driven Development Example","text":"<p>With this method, rather than writing all your tests up front, you write one test at a time and then switch to write the system code that would make that test pass. It's important to write the bare minimum of code necessary even if it is not actually \"correct\". Once the test passes you can refactor the code to make it maybe make more sense, but again the logic should be simple. As you write more tests, the logic gets more and more complex, but you can continue to make the minimal changes to the system code with confidence because all code that was written is covered.</p> <p>As an example, let's assume we are trying to write a new function that validates a string is a valid password format. The password format should be a string larger than 8 characters containing at least one number. We start with the simplest possible test; one of the easiest ways to do this is to first write tests that validate inputs into the function:</p> <pre><code>// Tests.cs\npublic class Tests\n{\n    [Fact]\n    public void ValidatePassword_NullInput_Throws()\n    {\n        var s = new MyClass();\n        Assert.Throws&lt;ArgumentNullException&gt;(() =&gt; s.ValidatePassword(null));\n    }\n}\n\n// MyClass.cs\npublic class MyClass\n{\n    public bool ValidatePassword(string input)\n    {\n        return false;\n    }\n}\n</code></pre> <p>If we run this code, the test will fail as no exception was thrown since our code in <code>ValidateString</code> is just a stub. This is ok! This is the \"Red\" part of Red-Green-Refactor. Now we want to move onto the \"Green\" part - making the minimal change required to make this test pass:</p> <pre><code>// MyClass.cs\npublic class MyClass\n{\n    public bool ValidatePassword(string input)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n}\n</code></pre> <p>Our tests pass, but this function doesn't really work, it will always throw the exception. That's ok! As we continue to write tests we will slowly add the logic for this function, and it will build on itself, all while guaranteeing our tests continue to pass.</p> <p>We will skip the \"Refactor\" stage at this point because there isn't anything to refactor. Next let's add a test that checks that the function returns false if the password is less than size 8:</p> <pre><code>[Fact]\npublic void ValidatePassword_SmallSize_ReturnsFalse()\n{\n    var s = new MyClass();\n    Assert.False(s.ValidatePassword(\"abc\"));\n}\n</code></pre> <p>This test will pass as it still only throws an <code>ArgumentNullException</code>, but again, that is an expected failure. Fixing our function should see it pass:</p> <pre><code>public bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n    return false;\n}\n</code></pre> <p>Finally, some code that looks real! Note how it wasn't the test that checked for null that had us add the <code>if</code> statement for the null-check, but rather the subsequent test which unlocked a whole new branch. By adding that if statement, we made the bare minimum change necessary in order to get both tests to pass, but we still have work to do.</p> <p>In general, working in the order of adding a negative test first before adding a positive test will ensure that both cases get covered by the code in a way that can get tests. Red-Green-Refactor makes that process super easy by requiring the bare minimum change - since we only want to make the bare minimum changes, we just simply return false here, knowing full well that we will be adding logic later that will expand on this.</p> <p>Speaking of which, let's add the positive test now:</p> <pre><code>[Fact]\npublic void ValidatePassword_RightSize_ReturnsTrue()\n{\n    var s = new MyClass();\n    Assert.True(s.ValidatePassword(\"abcdefgh1\"));\n}\n</code></pre> <p>Again, this test will fail at the start. One thing to note here if that its important that we try and make our tests resilient to future changes. When we write the code under test, we act very naively, only trying to make the current tests we have pass; when you write tests though, you want to ensure that everything you are doing is a valid case in the future. In this case, we could have written the input string as <code>abcdefgh</code> and when we eventually write the function it would pass, but later when we add tests that validate the function has the rest of the proper inputs it would fail incorrectly.</p> <p>Anyways, the next code change is:</p> <pre><code>public bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n    if (input.Length &gt; 8)\n    {\n        return true;\n    }\n    return false;\n}\n</code></pre> <p>Here we now have a passing test! However, the logic doesn't actually make much sense. We did the bare minimum change which was adding a new condition that passed for longer strings, but thinking forward we know this won't work as soon as we add additional validations. So let's use our first \"Refactor\" step in the Red-Green-Refactor flow!</p> <pre><code>public bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n    if (input.Length &lt; 8)\n    {\n        return false;\n    }\n    return true;\n}\n</code></pre> <p>That looks better. Note how from a functional perspective, inverting the if-statement does not change what the function returns. This is an important part of the refactor flow, maintaining the logic by doing provably safe refactors, usually through the use of tooling and automated refactors from your IDE.</p> <p>Finally, we have one last requirement for our <code>ValidatePassword</code> method and that is that it needs to check that there is a number in the password. Let's again start with the negative test and validate that with a string with the valid length that the function returns <code>false</code> if we do not pass in a number:</p> <pre><code>[Fact]\npublic void ValidatePassword_ValidLength_ReturnsFalse()\n{\n    var s = new MyClass();\n    Assert.False(s.ValidatePassword(\"abcdefghij\"));\n}\n</code></pre> <p>Of course the test fails as it is only checking length requirements. Let's fix the method to check for numbers:</p> <pre><code>public bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n    if (input.Length &lt; 8)\n    {\n        return false;\n    }\n\n    if (!input.Any(char.IsDigit))\n    {\n        return false;\n    }\n    return true;\n}\n</code></pre> <p>Here we use a handy LINQ method to check if any of the <code>char</code>s in the <code>string</code> are a digit, and if not, return false. Tests now pass, and we can refactor. For readability, why not combine the <code>if</code> statements:</p> <pre><code>public bool ValidatePassword(string input)\n{\n    if (input == null)\n    {\n        throw new ArgumentNullException(nameof(input));\n    }\n\n    if ((input.Length &lt; 8) ||\n        (!input.Any(char.IsDigit)))\n    {\n        return false;\n    }\n\n    return true;\n}\n</code></pre> <p>As we refactor this code, we feel 100% confident in the changes we made as we have 100% test coverage which tests both positive and negative scenarios. In this case we actually already have a method that tests the positive case, so our function is done!</p> <p>Now that our code is completely tested we can make all sorts of changes and still have confidence that it works. For example, if we wanted to change the implementation of the method to use regex, all of our tests would still pass and still be valid.</p> <p>That is it! We finished writing our function, we have 100% test coverage, and if we had done something a little more complex, we are guaranteed that whatever we designed is already testable since the tests were written first!</p>"},{"location":"lab2/unit-testing/why-unit-tests/","title":"Why Unit Tests","text":"<p>It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we bother writing them?</p>"},{"location":"lab2/unit-testing/why-unit-tests/#reduce-costs","title":"Reduce Costs","text":"<p>There is no question that the later a bug is found, the more expensive it is to fix; especially so if the bug makes it into production. A 2008 research study by IBM estimates that a bug caught in production could cost 6 times as much as if it was caught during implementation.</p>"},{"location":"lab2/unit-testing/why-unit-tests/#increase-developer-confidence","title":"Increase Developer Confidence","text":"<p>Many changes that developers make are not big features or something that requires an entire testing suite. A strong unit test suite helps increase the confidence of the developer that their change is not going to cause any downstream bugs. Having unit tests also helps with making safe, mechanical refactors that are provably safe; using things like refactoring tools to do mechanical refactoring and running unit tests that cover the refactored code should be enough to increase confidence in the commit.</p>"},{"location":"lab2/unit-testing/why-unit-tests/#speed-up-development","title":"Speed Up Development","text":"<p>Unit tests take time to write, but they also speed up development? While this may seem like an oxymoron, it is one of the strengths of a unit testing suite - over time it continues to grow and evolve until the tests become an essential part of the developer workflow.</p> <p>If the only testing available to a developer is a long-running system test, integration tests that require a deployment, or manual testing, it will increase the amount of time taken to write a feature. These types of tests should be a part of the \"Outer loop\"; tests that may take some time to run and validate more than just the code you are writing. Usually these types of outer loop tests get run at the PR stage or even later during merges into branches.</p> <p>The Developer Inner Loop is the process that developers go through as they are authoring code. This varies from developer to developer and language to language but typically is something like code -&gt; build -&gt; run -&gt; repeat. When unit tests are inserted into the inner loop, developers can get early feedback and results from the code they are writing. Since unit tests execute really quickly, running tests shouldn't be seen as a barrier to entry for this loop. Tooling such as Visual Studio Live Unit Testing also help to shorten the inner loop even more.</p>"},{"location":"lab2/unit-testing/why-unit-tests/#documentation-as-code","title":"Documentation as Code","text":"<p>Writing unit tests is a great way to show how the units of code you are writing are supposed to be used. In some ways, unit tests are better than any documentation or samples because they are (or at least should be) executed with every build so there is confidence that they are not out of date. Unit tests also should be so simple that they are easy to follow.</p>"},{"location":"lab3/","title":"Machine Learning Fundamentals at ISE","text":"<p>This guideline documents the Machine Learning (ML) practices in ISE. ISE works with customers on developing ML models and putting them in production, with an emphasis on engineering and research best practices throughout the project's life cycle.</p>"},{"location":"lab3/#goals","title":"Goals","text":"<ul> <li>Provide a set of ML practices to follow in an ML project.</li> <li>Provide clarity on ML process and how it fits within a software engineering project.</li> <li>Provide best practices for the different stages of an ML project.</li> </ul>"},{"location":"lab3/#how-to-use-these-fundamentals","title":"How to use these Fundamentals","text":"<ul> <li>If you are starting a new ML project, consider reading through the general guidance documents.</li> <li>For specific aspects of an ML project, refer to the guidelines for different project phases.</li> </ul>"},{"location":"lab3/#ml-project-phases","title":"ML Project Phases","text":"<p>The diagram below shows different phases in an ideal ML project. Due to practical constraints and requirements, it might not always be possible to have a project structured in such a manner, however best practices should be followed for each individual phase.</p> <p></p> <ul> <li>Envisioning: Initial problem understanding, customer goals and objectives.</li> <li>Feasibility Study: Assess whether the problem in question is feasible to solve satisfactorily using ML with the available data.</li> <li>Model Milestone: There is a basic model that is achieving the minimum required performance, both in terms of ML performance and system performance. Using the knowledge gathered to this milestone, define the scope, objectives, high-level architecture, definition of done and plan for the entire project.</li> <li>Model(s) experimentation: Tools and best practices for conducting successful model experimentation.</li> <li>Model(s) Operationalization: Model readiness for production checklist.</li> </ul>"},{"location":"lab3/#general-guidance","title":"General Guidance","text":"<ul> <li>ML Process Guidance</li> <li>ML Fundamentals checklist</li> <li>Data Exploration</li> <li>Agile ML development</li> <li>Testing Data Science and ML Ops code</li> <li>Profiling Machine Learning and ML Ops code</li> <li>Responsible AI</li> <li>Program Management for ML projects</li> </ul>"},{"location":"lab3/#resources","title":"Resources","text":"<ul> <li>Model Operationalization</li> </ul>"},{"location":"lab3/agile-development-considerations-for-ml-projects/","title":"Agile Development Considerations for ML Projects","text":""},{"location":"lab3/agile-development-considerations-for-ml-projects/#overview","title":"Overview","text":"<p>When running ML projects, we follow the Agile methodology for software development with some adaptations, as we acknowledge that research and experimentation are sometimes difficult to plan and estimate.</p>"},{"location":"lab3/agile-development-considerations-for-ml-projects/#goals","title":"Goals","text":"<ol> <li>Run and manage ML projects effectively</li> <li>Create effective collaboration between the ML team and the other teams working on the project</li> </ol> <p>To learn more about how ISE runs the Agile process for software development teams, refer to this doc.</p> <p>Within this framework, the team follows these Agile ceremonies:</p> <ul> <li>Backlog management</li> <li>Retrospectives</li> <li>Scrum of Scrums (where applicable)</li> <li>Sprint planning</li> <li>Stand-ups</li> <li>Working agreement</li> </ul>"},{"location":"lab3/agile-development-considerations-for-ml-projects/#agile-process-during-exploration-and-experimentation","title":"Agile Process During Exploration and Experimentation","text":"<ol> <li> <p>While acknowledging the fact that ML user stories and research spikes are less predictable than software development ones, we strive to have a deliverable for every user story in every sprint.</p> </li> <li> <p>User stories and spikes are usually estimated using T-shirt sizes or similar, and not in actual days/hours.</p> </li> <li> <p>ML design sessions should be included in each sprint.</p> </li> </ol>"},{"location":"lab3/agile-development-considerations-for-ml-projects/#examples-of-ml-deliverables-for-each-sprint","title":"Examples of ML Deliverables for each Sprint","text":"<ul> <li>Working code (e.g. models, pipelines, exploratory code)</li> <li>Documentation of new hypotheses, and the acceptance or rejection of previous hypotheses as part of a Hypothesis Driven Analysis (HDA). For more information see Hypothesis Driven Development on Barry Oreilly's website</li> <li>Exploratory Data Analysis (EDA) results and learnings documented</li> </ul>"},{"location":"lab3/agile-development-considerations-for-ml-projects/#collaboration-between-data-scientists-and-software-developers","title":"Collaboration Between Data Scientists and Software Developers","text":"<ul> <li>Data scientists and software developers work together on the project. The team uses one backlog and attend the same Agile ceremonies. In cases where the project has many participants, we will divide into working groups, but still have the entire team join the Agile ceremonies.</li> </ul> <ul> <li>If possible, feasibility study and initial model experimentation takes place before the operationalization work kicks off.</li> <li>Everyone shares the accountability for the MLOps solution.</li> <li>The ML model interface (API) is determined as early as possible, to allow the developers to consider its integration into the production pipeline.</li> <li>MLOps artifacts are developed with a continuous collaboration and review of the data scientists, to ensure the appropriate approaches for experimentation and productization are used.</li> <li>Retrospectives and sprint planning are performed on the entire team level, and not the specific work groups level.</li> </ul>"},{"location":"lab3/data-exploration/","title":"Data Exploration","text":"<p>After envisioning, and typically as part of the ML feasibility study, the next step is to confirm resource access and then dive deep into the available data through data exploration workshops.</p>"},{"location":"lab3/data-exploration/#purpose-of-the-data-exploration-workshop","title":"Purpose of the Data Exploration Workshop","text":"<p>The purpose of the data exploration workshop is as follows:</p> <ol> <li>Ensure the team can access the data and compute resources that are necessary for the ML feasibility study</li> <li>Ensure that the data provided is of quality and is relevant to the ML solution</li> <li>Make sure that the project team has a good understanding of the data</li> <li>Make sure that the SMEs (Subject Matter Experts) needed are present for Data Exploration Workshop</li> <li>List people needed for the data exploration workshop</li> </ol>"},{"location":"lab3/data-exploration/#accessing-resources","title":"Accessing Resources","text":"<p>Prior to diving into data exploration workshops, it is important to confirm that you have access to the necessary resources (including data).</p> <p>Below is an example list of questions to consider before starting a data exploration workshop.</p> <ol> <li>What are the requirements for an account to be set up in order for the team to access data and compute resources?</li> <li>Are there security requirements around accessing resources (Subscriptions, Azure Resources, project management, etc.) such as VPN, 2FA, jump boxes, etc.?</li> <li>Data access:     * Is it on-prem or on Azure already?     * If it is on-prem, can we move the needed data to Azure under the appropriate subscription? Who has permission to move the data?     * Is the data access approved from a legal/compliance perspective?</li> <li>Computation:     * Is a VPN needed for the project team to access these computation nodes (Virtual Machines, Databricks clusters, etc) from their work PCs/Macs?     * Any restrictions on accessing the source data system from these computation nodes?     * If we want to create some compute resources, who has permissions to do so?</li> <li>Source code repository:     * Do you have any preference on source code repository location?</li> <li>Backlog management and work planning:     * Do you have any preference on backlog management and work planning, such as Azure DevOps, Jira or anything else?     * If an existing system, are special accounts / system setups required to access?</li> <li>Programming Language:     * Is Python/PySpark a preferred language?     * Is there any internal approval processes for the Python/PySpark libraries we want to use for this engagement?</li> </ol>"},{"location":"lab3/data-exploration/#data-exploration-workshop","title":"Data Exploration Workshop","text":"<p>Key objectives of the exploration workshops include the following:</p> <ol> <li>Understand and document the features, location, and availability of the data.</li> <li>What order of magnitude is the current data (e.g., GB, TB)? Is this all relevant?</li> <li>How does the organization decide when to collect additional data or purchase external data? Are there any examples of this?</li> <li>Understand the quality of the data. Is there already a data validation strategy in place?</li> <li>What data has been used so far to analyze recent data-driven projects? What has been found to be most useful? What was not useful? How was this judged?</li> <li>What additional internal data may provide insights useful for data-driven decision-making for proposed projects? What external data could be useful?</li> <li>What are the possible constraints or challenges in accessing or incorporating this data?</li> <li>How was the data collected? Are there any obvious biases due to how the data was collected?</li> <li>What changes to data collection, coding, integration, etc has occurred in the last 2 years that may impact the interpretation or availability of the collected data</li> </ol>"},{"location":"lab3/envisioning-and-problem-formulation/","title":"Envisioning and Problem Formulation","text":"<p>Before beginning a data science investigation, we need to define a problem statement which the data science team can explore; this problem statement can have a significant influence on whether the project is likely to be successful.</p>"},{"location":"lab3/envisioning-and-problem-formulation/#envisioning-goals","title":"Envisioning Goals","text":"<p>The main goals of the envisioning process are:</p> <ul> <li>Establish a clear understanding of the problem domain and the underlying business objective</li> <li>Define how a potential solution would be used and how its performance should be measured</li> <li>Determine what data is available to solve the problem</li> <li>Understand the capabilities and working practices of the data science team</li> <li>Ensure all parties have the same understanding of the scope and next steps (e.g., onboarding, data exploration workshop)</li> </ul> <p>The envisioning process usually entails a series of 'envisioning' sessions where the data science team work alongside subject-matter experts to formulate the problem in such a way that there is a shared understanding a shared understanding of the problem domain, a clear goal, and a predefined approach to evaluating a potential solution.</p>"},{"location":"lab3/envisioning-and-problem-formulation/#understanding-the-problem-domain","title":"Understanding the Problem Domain","text":"<p>Generally, before defining a project scope for a data science investigation, we must first understand the problem domain:</p> <ul> <li>What is the problem?</li> <li>Why does the problem need to be solved?</li> <li>Does this problem require a machine learning solution?</li> <li>How would a potential solution be used?</li> </ul> <p>However, establishing this understanding can prove difficult, especially for those unfamiliar with the problem domain. To ease this process, we can approach problems in a structured way by taking the following steps:</p> <ul> <li>Identify a measurable problem and define this in business terms. The objective should be clear, and we should have a good understanding of the factors that we can control - that can be used as inputs - and how they affect the objective. Be as specific as possible.</li> <li>Decide how the performance of a solution should be measured and identify whether this is possible within the restrictions of this problem. Make sure it aligns with the business objective and that you have identified the data required to evaluate the solution. Note that the data required to evaluate a solution may differ from the data needed to create a solution.</li> <li>Thinking about the solution as a black box, detail the function that a solution to this problem should perform to fulfil the objective and verify that the relevant data is available to solve the problem.<ul> <li>One way of approaching this is by thinking about how a subject-matter expert could solve the problem manually, and the data that would be required; if a human subject-matter expert is unable to solve the problem given the available data, this is indicative that additional information is required and/or more data needs to be collected.</li> </ul> </li> <li>Based on the available data, define specific hypothesis statements - which can be proved or disproved - to guide the exploration of the data science team. Where possible, each hypothesis statement should have a clearly defined success criteria (e.g., with an accuracy of over 60%), however, this is not always possible - especially for projects where no solution to the problem currently exists. In these cases, the measure of success could be based on a subject-matter expert verifying that the results meet their expectations.</li> <li>Document all the above information, to ensure alignment between stakeholders and establish a clear understanding of the problem to be solved. Try to ensure that as much relevant domain knowledge is captured as possible, and that the features present in available data - and the way that the data was collected - are clearly explained, such that they can be understood by a non-subject matter expert.</li> </ul> <p>Once an understanding of the problem domain has been established, it may be necessary to break down the overall problem into smaller, meaningful chunks of work to maintain team focus and ensure a realistic project scope within the given time frame.</p>"},{"location":"lab3/envisioning-and-problem-formulation/#listening-to-the-end-user","title":"Listening to the End User","text":"<p>These problems are complex and require understanding from a variety of perspectives. It is not uncommon for the stakeholders to not be the end user of the solution framework. In these cases, listening to the actual end users is critical to the success of the project.</p> <p>The following questions can help guide discussion in understanding the stakeholders' perspectives:</p> <ul> <li>Who is the end user?</li> <li>What is the current practice related to the business problem?</li> <li>What's the performance of the current solution?</li> <li>What are their pain points?</li> <li>What is their toughest problem?</li> <li>What is the state of the data used to build the solution?</li> <li>How does the end user or SME envision the solution?</li> </ul>"},{"location":"lab3/envisioning-and-problem-formulation/#envisioning-guidance","title":"Envisioning Guidance","text":"<p>During envisioning sessions, the following may prove useful for guiding the discussion. Many of these points are taken directly, or adapted from, [1] and [2].</p>"},{"location":"lab3/envisioning-and-problem-formulation/#problem-framing","title":"Problem Framing","text":"<ol> <li>Define the objective in business terms.</li> <li>How will the solution be used?</li> <li>What are the current solutions/workarounds (if any)? What work has been done in this area so far? Does this solution need to fit into an existing system?</li> <li>How should performance be measured?</li> <li>Is the performance measure aligned with the business objective?</li> <li>What would be the minimum performance needed to reach the business objective?</li> <li>Are there any known constraints around non-functional requirements that would have to be taken into account? (e.g., computation times)</li> <li>Frame this problem (supervised/unsupervised, online/offline, etc.)</li> <li>Is human expertise available?</li> <li>How would you solve the problem manually?</li> <li>Are there any restrictions on the type of approaches which can be used? (e.g., does the solution need to be completely explainable?)</li> <li>List the assumptions you or others have made so far. Verify these assumptions if possible.</li> <li>Define some initial hypothesis statements to be explored.</li> <li>Highlight and discuss any responsible AI concerns if appropriate.</li> </ol>"},{"location":"lab3/envisioning-and-problem-formulation/#workflow","title":"Workflow","text":"<ol> <li>What data science skills exist in the organization?</li> <li>How many data scientists/engineers would be available to work on this project? In what capacity would these resources be available (full-time, part-time, etc.)?</li> <li>What does the team's current workflow practices look like? Do they work on the cloud/on-prem? In notebooks/IDE? Is version control used?</li> <li>How are data, experiments and models currently tracked?</li> <li>Does the team employ an Agile methodology? How is work tracked?</li> <li>Are there any ML solutions currently running in production? Who is responsible for maintaining these solutions?</li> <li>Who would be responsible for maintaining a solution produced during this project?</li> <li>Are there any restrictions on tooling that must/cannot be used?</li> </ol>"},{"location":"lab3/envisioning-and-problem-formulation/#example-a-recommendation-engine-problem","title":"Example: A Recommendation Engine Problem","text":"<p>To illustrate how the above process can be applied to a tangible problem domain, as an example, consider that we are looking at implementing a recommendation engine for a clothing retailer. This example was, in part, inspired by [3].</p> <p>Often, the objective may be simply presented, in a form such as \"to improve sales\". However, whilst this is ultimately the main goal, we would benefit from being more specific here. Suppose that we were to deploy a solution in November and then observed a December sales surge; how would we be able to distinguish how much of this was as a result of the new recommendation engine, as opposed to the fact that December is a peak buying season?</p> <p>A better objective, in this case, would be \"to drive additional sales by presenting the customer with items that they would not otherwise have purchased without the recommendation\". Here, the inputs that we can control are the choice of items that are presented to each customer, and the order in which they are displayed; considering factors such as how frequently these should change, seasonality, etc.</p> <p>The data required to evaluate a potential solution in this case would be which recommendations resulted in new sales, and an estimation of a customer's likeliness to purchase a specific item without a recommendation. Note that, whilst this data could also be used to build a recommendation engine, it is unlikely that this data will be available before a recommendation system has been implemented, so it is likely that we will have to use an alternate data source to build the model.</p> <p>We can get an initial idea of how to approach a solution to this problem by considering how it would be solved by a subject-matter expert. Thinking of how a personal stylist may provide a recommendation, they are likely to recommend items based on one or more of the following:</p> <ul> <li>generally popular items</li> <li>items similar to those liked/purchased by the customer</li> <li>items that were liked/purchased by similar customers</li> <li>items which are complementary to those owned by the customer</li> </ul> <p>Whilst this list is by no means exhaustive, it provides a good indication of the data that is likely to be useful to us:</p> <ul> <li>item sales data</li> <li>customer purchase histories</li> <li>customer demographics</li> <li>item descriptions and tags</li> <li>previous outfits, or sets, which have been curated by the stylist</li> </ul> <p>We would then be able to use this data to explore:</p> <ul> <li>a method of measuring similarity between items</li> <li>a method of measuring similarity between customers</li> <li>a method of measuring how complementary items are relative to one another</li> </ul> <p>which can be used to create and rank recommendations. Depending on the project scope, and available data, one or more of these areas could be selected to create hypotheses to be explored by the data science team. Some examples of such hypothesis statements could be:</p> <ul> <li>From the descriptions of each item, we can determine a measure of similarity between different items to a degree of accuracy which is specified by a stylist.</li> <li>Based on the behavior of customers with similar purchasing histories, we are able to predict certain items that a customer is likely to purchase; with a certainty which is greater than random choice.</li> <li>Using sets of items which have previously been sold together, we can formulate rules around the features which determine whether items are complementary or not which can be verified by a stylist.</li> </ul>"},{"location":"lab3/envisioning-and-problem-formulation/#next-steps","title":"Next Steps","text":"<p>To ensure clarity and alignment, it is useful to summarize the envisioning stage findings focusing on proposed detailed scenarios, assumptions and agreed decisions as well next steps.</p> <p>We suggest confirming that you have access to all necessary resources (including data) as a next step before proceeding with data exploration workshops.</p> <p>Below are the links to the exit document template and to some questions which may be helpful in confirming resource access.</p> <ul> <li>Summary of Scope Exit Document Template</li> <li>List of Resource Access Questions</li> <li>List of Data Exploration Workshop Questions</li> </ul>"},{"location":"lab3/envisioning-and-problem-formulation/#resources","title":"Resources","text":"<p>Many of the ideas presented here - and much more - were inspired by, and can be found in the following resources; all of which are highly recommended.</p> <ol> <li>Aur\u00e9lien G\u00e9ron's Machine learning project checklist</li> <li>Fast.ai's Data project checklist</li> <li>Designing great data products. Jeremy Howard, Margit Zwemer and Mike Loukides</li> </ol>"},{"location":"lab3/envisioning-summary-template/","title":"Generic Envisioning Summary","text":""},{"location":"lab3/envisioning-summary-template/#purpose-of-this-template","title":"Purpose of this Template","text":"<p>This is an example of an envisioning summary completed after envisioning sessions have concluded. It summarizes the materials reviewed, application scenarios discussed and decided, and the next steps in the process.</p>"},{"location":"lab3/envisioning-summary-template/#summary-of-envisioning","title":"Summary of Envisioning","text":""},{"location":"lab3/envisioning-summary-template/#introduction","title":"Introduction","text":"<p>This document is to summarize what we have discussed in these envisioning sessions, and what we have decided to work on in this machine learning (ML) engagement. With this document, we hope that everyone can be on the same page regarding the scope of this ML engagement, and will ensure a successful start for the project.</p>"},{"location":"lab3/envisioning-summary-template/#materials-shared-with-the-team","title":"Materials Shared with the Team","text":"<p>List materials shared with you here. The list below contains some examples. You will want to be more specific.</p> <ol> <li>Business vision statement</li> <li>Sample Data</li> <li>Current problem statement</li> </ol> <p>Also discuss:</p> <ol> <li>How the current solution is built and implemented</li> <li>Details about the current state of the systems and processes.</li> </ol>"},{"location":"lab3/envisioning-summary-template/#applications-scenarios-that-can-help-people-achieve-task","title":"Applications Scenarios that Can Help [People] Achieve [Task]","text":"<p>The following application scenarios were discussed:</p> <p>Scenario 1:</p> <p>Scenario 2:</p> <p>Add more scenarios as needed</p> <p>For each scenario, provide an appropriately descriptive name and then follow up with more details.</p> <p>For each scenario, discuss:</p> <ol> <li>What problem statement was discussed</li> <li>How we propose to solve the problem (there may be several proposals)</li> <li>Who would use the solution</li> <li>What would it look like to use our solution? An example of how it would bring value to the end user.</li> </ol>"},{"location":"lab3/envisioning-summary-template/#selected-scenario-for-this-ml-engagement","title":"Selected Scenario for this ML Engagement","text":"<p>Which scenario was selected?</p> <p>Why was this scenario prioritised over the others?</p> <p>Will other scenarios be considered in the future? When will we revisit them / what conditions need to be met to pursue them?</p>"},{"location":"lab3/envisioning-summary-template/#more-details-of-the-scope-for-selected-scenario","title":"More Details of the Scope for Selected Scenario","text":"<ol> <li>What is in scope?</li> <li>What data is available?</li> <li>Which performance metric to use?</li> <li>Bar of performance metrics</li> <li>What are deliverables?</li> </ol>"},{"location":"lab3/envisioning-summary-template/#whats-next","title":"What\u2019s Next?","text":""},{"location":"lab3/envisioning-summary-template/#legal-documents-to-be-signed","title":"Legal Documents to be Signed","text":"<p>State documents and timeline</p>"},{"location":"lab3/envisioning-summary-template/#responsible-ai-review","title":"Responsible AI Review","text":"<p>Plan when to conduct a responsible AI process. What are the prerequisites to start this process?</p>"},{"location":"lab3/envisioning-summary-template/#data-exploration-workshop","title":"Data Exploration Workshop","text":"<p>A data exploration workshop is planned for DATE RANGE. This data exploration workshops will be X-Y days, not including the time to gain access resources. The purpose of the data exploration workshop is as follows:</p> <ol> <li>Ensure the team can access the data and compute resources that are necessary for the ML feasibility study</li> <li>Ensure that the data provided is of quality and is relevant to the ML solution</li> <li>Make sure that the project team has a good understanding of the data</li> <li>Make sure that the SMEs (Subject Matter Experts) needed are present for Data Exploration Workshop</li> <li>List people needed for the data exploration workshop</li> </ol>"},{"location":"lab3/envisioning-summary-template/#ml-feasibility-study-til-date","title":"ML Feasibility Study til [date]","text":""},{"location":"lab3/envisioning-summary-template/#objectives","title":"Objectives","text":"<p>State what we expect to be the objective in the feasibility study</p>"},{"location":"lab3/envisioning-summary-template/#timeline","title":"Timeline","text":"<p>Give a possible timeline for the feasibility study</p>"},{"location":"lab3/envisioning-summary-template/#personnel-needed","title":"Personnel Needed","text":"<p>What sorts of people/roles are needed for the feasibility study?</p>"},{"location":"lab3/envisioning-summary-template/#whats-after-ml-feasibility-study","title":"What\u2019s After ML Feasibility Study","text":"<p>Detail here</p>"},{"location":"lab3/envisioning-summary-template/#summary-of-timeline","title":"Summary of Timeline","text":"<p>Below is a high-level summary of the upcoming timeline:</p> <p>Discuss dates for the data exploration workshop, and feasibility study along with any to-do items such as starting responsible AI process, identifying engineering resources. We suggest using a concise bulleted list or a table to easily convey the information.</p>"},{"location":"lab3/feasibility-studies/","title":"Feasibility Studies","text":"<p>The main goal of feasibility studies is to assess whether it is feasible to solve the problem satisfactorily using ML with the available data. We want to avoid investing too much in the solution before we have:</p> <ul> <li>Sufficient evidence that a solution would be the best technical solution given the business case</li> <li>Sufficient evidence that a solution is compatible with the problem context</li> <li>Sufficient evidence that a solution is possible</li> <li>Some vetted direction on what a solution should look like</li> </ul> <p>This effort ensures quality solutions backed by the appropriate, thorough amount of consideration and evidence.</p>"},{"location":"lab3/feasibility-studies/#when-are-feasibility-studies-useful","title":"When are Feasibility Studies Useful?","text":"<p>Every engagement can benefit from a feasibility study early in the project.</p> <p>Architectural discussions can still occur in parallel as the team works towards gaining a solid understanding and definition of what will be built.</p> <p>Feasibility studies can last between 4-16 weeks, depending on specific problem details, volume of data, state of the data etc. Starting with a 4-week milestone might be useful, during which it can be determined how much more time, if any, is required for completion.</p>"},{"location":"lab3/feasibility-studies/#who-collaborates-on-feasibility-studies","title":"Who Collaborates on Feasibility Studies?","text":"<p>Collaboration from individuals with diverse skill sets is desired at this stage, including data scientists, data engineers, software engineers, PMs, human experience researchers, and domain experts. It embraces the use of engineering fundamentals, with some flexibility. For example, not all experimentation requires full test coverage and code review. Experimentation is typically not part of a CI/CD pipeline. Artifacts may live in the <code>main</code> branch as a folder excluded from the CI/CD pipeline, or as a separate experimental branch, depending on customer/team preferences.</p>"},{"location":"lab3/feasibility-studies/#what-do-feasibility-studies-entail","title":"What do Feasibility Studies Entail?","text":""},{"location":"lab3/feasibility-studies/#problem-definition-and-desired-outcome","title":"Problem Definition and Desired Outcome","text":"<ul> <li>Ensure that the problem is complex enough that coding rules or manual scaling is unrealistic</li> <li>Clear definition of the problem from business and technical perspectives</li> </ul>"},{"location":"lab3/feasibility-studies/#deep-contextual-understanding","title":"Deep Contextual Understanding","text":"<p>Confirm that the following questions can be answered based on what was learned during the Discovery Phase of the project. For items that can not be satisfactorily answered, undertake additional investigation to answer.</p> <ul> <li>Understanding the people who are using and/or affected by the solution</li> <li>Understanding the contextual forces at play around the problem, including goals, culture, and historical context</li> <li>To accomplish this a researcher will:</li> <li>Collaborate with customers and colleagues to explore the landscape of people who relate to and may be affected by the problem space being explored (Users, stakeholders, subject matter experts, etc)</li> <li>Formulate the research question(s) to be addressed</li> <li>Select and design research to best serve the research question(s)</li> <li>Identify and select representative research participants across the problem space with whom to conduct the research</li> <li>Construct a research plan and necessary preparation documents for the selected research method(s)</li> <li>Conduct research activity with the participants via the selected method(s)</li> <li>Synthesize, analyze, and interpret research findings</li> <li>Where relevant, build frameworks, artifacts and processes that help explore the findings and implications of the research across the team</li> <li>Share what was uncovered and understood, and the implications thereof across the engagement team and relevant stakeholders.</li> <li>If the above research was conducted during the Discovery phase, it should be reviewed, and any substantial knowledge gaps should be identified and filled by following the above process.</li> </ul>"},{"location":"lab3/feasibility-studies/#data-access","title":"Data Access","text":"<ul> <li>Verify that the full team has access to the data</li> <li>Set up a dedicated and/or restricted environment if required</li> <li>Perform any required de-identification or redaction of sensitive information</li> <li>Understand data access requirements (retention, role-based access, etc.)</li> </ul>"},{"location":"lab3/feasibility-studies/#data-discovery","title":"Data Discovery","text":"<ul> <li>Hold a data exploration workshop and deep dive with domain experts</li> <li>Understand data availability and confirm the team's access</li> <li>Understand the data dictionary, if available</li> <li>Understand the quality of the data. Is there already a data validation strategy in place?</li> <li>Ensure required data is present in reasonable volumes</li> <li>For supervised problems (most common), assess the availability of labels or data that can be used to effectively approximate labels</li> <li>If applicable, ensure all data can be joined as required and understand how<ul> <li>Ideally obtain or create an entity relationship diagram (ERD)</li> </ul> </li> <li>Potentially uncover new useful data sources</li> </ul>"},{"location":"lab3/feasibility-studies/#architecture-discovery","title":"Architecture Discovery","text":"<ul> <li>Clear picture of existing architecture</li> <li>Infrastructure spikes</li> </ul>"},{"location":"lab3/feasibility-studies/#concept-ideation-and-iteration","title":"Concept Ideation and Iteration","text":"<ul> <li>Develop value proposition(s) for users and stakeholders based on the contextual understanding developed through the discovery process (e.g. key elements of value, benefits)</li> <li>As relevant, make use of</li> <li>Co-creation with team</li> <li>Co-creation with users and stakeholders</li> <li>As relevant, create vignettes, narratives or other materials to communicate the concept</li> <li>Identify the next set of hypotheses or unknowns to be tested (see concept testing)</li> <li>Revisit and iterate on the concept throughout discovery as understanding of the problem space evolves</li> </ul>"},{"location":"lab3/feasibility-studies/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<ul> <li>Data deep dive</li> <li>Understand feature and label value distributions</li> <li>Understand correlations among features and between features and labels</li> <li>Understand data specific problem constraints like missing values, categorical cardinality, potential for data leakage etc.</li> <li>Identify any gaps in data that couldn't be identified in the data discovery phase</li> <li>Pave the way of further understanding of what techniques are applicable</li> <li>Establish a mutual understanding of what data is in or out of scope for feasibility, ensuring that the data in scope is significant for the business</li> </ul>"},{"location":"lab3/feasibility-studies/#data-pre-processing","title":"Data Pre-Processing","text":"<ul> <li>Happens during EDA and hypothesis testing</li> <li>Feature engineering</li> <li>Sampling</li> <li>Scaling and/or discretization</li> <li>Noise handling</li> </ul>"},{"location":"lab3/feasibility-studies/#hypothesis-testing","title":"Hypothesis Testing","text":"<ul> <li>Design several potential solutions using theoretically applicable algorithms and techniques, starting with the simplest reasonable baseline</li> <li>Train model(s)</li> <li>Evaluate performance and determine if satisfactory</li> <li>Tweak experimental solution designs based on outcomes</li> <li>Iterate</li> <li>Thoroughly document each step and outcome, plus any resulting hypotheses for easy following of the decision-making process</li> </ul>"},{"location":"lab3/feasibility-studies/#concept-testing","title":"Concept Testing","text":"<ul> <li>Where relevant, to test the value proposition, concepts or aspects of the experience</li> <li>Plan user, stakeholder and expert research</li> <li>Develop and design necessary research materials</li> <li>Synthesize and evaluate feedback to incorporate into concept development</li> <li>Continue to iterate and test different elements of the concept as necessary, including testing to best serve RAI goals and guidelines</li> <li>Ensure that the proposed solution and framing are compatible with and acceptable to affected people</li> <li>Ensure that the proposed solution and framing is compatible with existing business goals and context</li> </ul>"},{"location":"lab3/feasibility-studies/#risk-assessment","title":"Risk Assessment","text":"<ul> <li>Identification and assessment of risks and constraints</li> </ul>"},{"location":"lab3/feasibility-studies/#responsible-ai","title":"Responsible AI","text":"<ul> <li>Consideration of responsible AI principles</li> <li>Understanding of users and stakeholders\u2019 contexts, needs and concerns to inform development of RAI</li> <li>Testing AI concept and experience elements with users and stakeholders</li> <li>Discussion and feedback from diverse perspectives around any responsible AI concerns</li> </ul>"},{"location":"lab3/feasibility-studies/#output-of-a-feasibility-study","title":"Output of a Feasibility Study","text":"<p>The main outcome is a feasibility study report, with a recommendation on next steps:</p> <p>If there is not enough evidence to support the hypothesis that this problem can be solved using ML, as aligned with the pre-determined performance measures and business impact:</p> <ul> <li>We detail the gaps and challenges that prevented us from reaching a positive outcome</li> <li>We may scope down the project, if applicable</li> <li>We may look at re-scoping the problem taking into account the findings of the feasibility study</li> <li>We assess the possibility to collect more data or improve data quality</li> </ul> <p>If there is enough evidence to support the hypothesis that this problem can be solved using ML</p> <ul> <li>Provide recommendations and technical assets for moving to the operationalization phase</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/","title":"ML Fundamentals Checklist","text":"<p>This checklist helps ensure that our ML projects meet our ML Fundamentals. The items below are not sequential, but rather organized by different parts of an ML project.</p>"},{"location":"lab3/ml-fundamentals-checklist/#data-quality-and-governance","title":"Data Quality and Governance","text":"<ul> <li> There is access to data.</li> <li> Labels exist for dataset of interest.</li> <li> Data quality evaluation.</li> <li> Able to track data lineage.</li> <li> Understanding of where the data is coming from and any policies related to data access.</li> <li> Gather Security and Compliance requirements.</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/#feasibility-study","title":"Feasibility Study","text":"<ul> <li> A feasibility study was performed to assess if the data supports the proposed tasks.</li> <li> Rigorous Exploratory data analysis was performed (including analysis of data distribution).</li> <li> Hypotheses were tested producing sufficient evidence to either support or reject that an ML approach is feasible to solve the problem.</li> <li> ROI estimation and risk analysis was performed for the project.</li> <li> ML outputs/assets can be integrated within the production system.</li> <li> Recommendations on how to proceed have been documented.</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/#evaluation-and-metrics","title":"Evaluation and Metrics","text":"<ul> <li> Clear definition of how performance will be measured.</li> <li> The evaluation metrics are somewhat connected to the success criteria.</li> <li> The metrics can be calculated with the datasets available.</li> <li> Evaluation flow can be applied to all versions of the model.</li> <li> Evaluation code is unit-tested and reviewed by all team members.</li> <li> Evaluation flow facilitates further results and error analysis.</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/#model-baseline","title":"Model Baseline","text":"<ul> <li> Well-defined baseline model exists and its performance is calculated. (More details on well defined baselines)</li> <li> The performance of other ML models can be compared with the model baseline.</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/#experimentation-setup","title":"Experimentation setup","text":"<ul> <li> Well-defined train/test dataset with labels.</li> <li> Reproducible and logged experiments in an environment accessible by all data scientists to quickly iterate.</li> <li> Defined experiments/hypothesis to test.</li> <li> Results of experiments are documented.</li> <li> Model hyper parameters are tuned systematically.</li> <li> Same performance evaluation metrics and consistent datasets are used when comparing candidate models.</li> </ul>"},{"location":"lab3/ml-fundamentals-checklist/#production","title":"Production","text":"<ul> <li> Model readiness checklist reviewed.</li> <li> Model reviews were performed (covering model debugging, reviews of training and evaluation approaches, model performance).</li> <li> Data pipeline for inferencing, including an end-to-end tests.</li> <li> SLAs requirements for models are gathered and documented.</li> <li> Monitoring of data feeds and model output.</li> <li> Ensure consistent schema is used across the system with expected input/output defined for each component of the pipelines (data processing as well as models).</li> <li> Responsible AI reviewed.</li> </ul>"},{"location":"lab3/ml-model-checklist/","title":"ML Model Production Checklist","text":"<p>The purpose of this checklist is to make sure that:</p> <ul> <li>The team assessed if the model is ready for production before moving to the scoring process</li> <li>The team has prepared a production plan for the model</li> </ul> <p>The checklist provides guidelines for creating this production plan. It should be used by teams/organizations that already built/trained an ML model and are now considering putting it into production.</p>"},{"location":"lab3/ml-model-checklist/#checklist","title":"Checklist","text":"<p>Before putting an individual ML model into production, the following aspects should be considered:</p> <ul> <li> Is there a well defined baseline? Is the model performing better than the baseline?</li> <li> Are machine learning performance metrics defined for both training and scoring?</li> <li> Is the model benchmarked?</li> <li> Can ground truth be obtained or inferred in production?</li> <li> Has the data distribution of training, testing and validation sets been analyzed?</li> <li> Have goals and hard limits for performance, speed of prediction and costs been established so they can be considered if trade-offs need to be made?</li> <li> How will the model be integrated into other systems, and what impact will it have?</li> <li> How will incoming data quality be monitored?</li> <li> How will drift in data characteristics be monitored?</li> <li> How will performance be monitored?</li> <li> Have any ethical concerns been taken into account?</li> </ul> <p>Please note that there might be scenarios where it is not possible to check all the items on this checklist. However, it is advised to go through all items and make informed decisions based on your specific use case.</p>"},{"location":"lab3/ml-model-checklist/#will-your-model-performance-be-different-in-production-than-during-the-training-phase","title":"Will Your Model Performance be Different in Production than During the Training Phase","text":"<p>Once deployed into production, the model might be performing much worse than expected. This poor performance could be a result of:</p> <ul> <li>The data to be scored in production is significantly different from the train and test datasets</li> <li>The feature engineering steps are different or inconsistent in production compared to the training process</li> <li>The performance measure is not consistent (for example your test set covers several months of data where the performance metric for production has been calculated for one month of data)</li> </ul>"},{"location":"lab3/ml-model-checklist/#is-there-a-well-defined-baseline-is-the-model-performing-better-than-the-baseline","title":"Is there a Well-Defined Baseline? Is the Model Performing Better than the Baseline?","text":"<p>A good way to think of a model baseline is the simplest model one can come up with: either a simple threshold, a random guess or a very basic linear model. This baseline is the reference point your model needs to outperform. A well-defined baseline is different for each problem type and there is no one size fits all approach.</p> <p>As an example, let's consider some common types of machine learning problems:</p> <ul> <li>Classification: Predicting between a positive and a negative class. Either the class with the most observations or a simple logistic regression model can be the baseline.</li> <li>Regression: Predicting the house prices in a city. The average house price for the last year or last month, a simple linear regression model, or the previous median house price in a neighborhood could be the baseline.</li> <li>Image classification: Building an image classifier to distinguish between cats and no cats in an image. If your classes are unbalanced: 70% cats and 30% no cats and if you always predict cats, your naive classifier has 70% accuracy and this can be your baseline. If your classes are balanced: 52% cats and 48% no cats, then a simple convolutional architecture can be the baseline (1 conv layer + 1 max pooling + 1 dense). Additionally, human accuracy at labelling can also be the baseline in an image classification scenario.</li> </ul> <p>Some questions to ask when comparing to a baseline:</p> <ul> <li>How does your model compare to a random guess?</li> <li>How does your model performance compare to applying a simple threshold?</li> <li>How does your model compare with always predicting the most common value?</li> </ul> <p>Note: In some cases, human parity might be too ambitious as a baseline, but this should be decided on a case by case basis. Human accuracy is one of the available options, but not the only one.</p> <p>Resources:</p> <ul> <li>\"How To Get Baseline Results And Why They Matter\" article</li> <li>\"Always start with a stupid model, no exceptions.\" article</li> </ul>"},{"location":"lab3/ml-model-checklist/#are-machine-learning-performance-metrics-defined-for-both-training-and-scoring","title":"Are Machine Learning Performance Metrics Defined for Both Training and Scoring?","text":"<p>The methodology of translating the training metrics to scoring metrics should be well-defined and understood. Depending on the data type and model, the model metrics calculation might differ in production and in training. For example, the training procedure calculated metrics for a long period of time (a year, a decade) with different seasonal characteristics while the scoring procedure will calculate the metrics per a restricted time interval (for example a week, a month, a quarter). Well-defined ML performance metrics are essential in production so that a decrease or increase in model performance can be accurately detected.</p> <p>Things to consider:</p> <ul> <li>In forecasting, if you change the period of assessing the performance, from one month to a year for example, then you might get a different result. For example, if your model is predicting sales of a product per day and the RMSE (Root Mean Squared Error) is very low for the first month the model is in production. As the model is live for longer, the RMSE is increasing, becoming 10x the RMSE for the first year compared to the first month.</li> <li>In a classification scenario, the overall accuracy is good, but the model is performing poorly for some subgroups. For example, a classifier has an accuracy of 80% overall, but only 55% for the 20-30 age group. If this is a significant age group for the production data, then your accuracy might suffer greatly when in production.</li> <li>In scene classification scenario, the model is trying to identify a specific scene in a video, and the model has been trained and tested (80-20 split) on 50000 segments where half are segments containing the scene and half of the segments do not contain the scene. The accuracy on the training set is 85% and 84% on the test set. However, when an entire video is scored, scores are obtained on all segments, and we expect few segments to contain the scene. The accuracy for an entire video is not comparable with the training/test set procedure in this case, hence different metrics should be considered.</li> <li>If sampling techniques (over-sampling, under-sampling) are used to train model when classes are imbalanced, ensure the metrics used during training are comparable with the ones used in scoring.</li> <li>If the number of samples used for training and testing is small, the performance metrics might change significantly as new data is scored.</li> </ul>"},{"location":"lab3/ml-model-checklist/#is-the-model-benchmarked","title":"Is the Model Benchmarked?","text":"<p>The trained model to be put into production is well benchmarked if machine learning performance metrics (such as accuracy, recall, RMSE or whatever is appropriate) are measured on the train and test set. Furthermore, the train and test set split should be well documented and reproducible.</p>"},{"location":"lab3/ml-model-checklist/#can-ground-truth-be-obtained-or-inferred-in-production","title":"Can Ground Truth be Obtained or Inferred in Production?","text":"<p>Without a reliable ground truth, the machine learning metrics cannot be calculated. It is important to identify if the ground truth can be obtained as the model is scoring new data by either manual or automatic means. If the ground truth cannot be obtained systematically, other proxies and methodology should be investigated in order to obtain some measure of model performance.</p> <p>One option is to use humans to manually label samples. One important aspect of human labelling is to take into account the human accuracy. If there are two different individuals labelling an image, the labels will likely be different for some samples. It is important to understand how the labels were obtained to assess the reliability of the ground truth (that is why we talk about human accuracy).</p> <p>For clarity, let's consider the following examples (by no means an exhaustive list):</p> <ul> <li>Forecasting: Forecasting scenarios are an example of machine learning problems where the ground truth could be obtained in most cases even though a delay might occur. For example, for a model predicting the sales of ice cream in a local shop, the ground truth will be obtained as the sales are happening, but it might appear in the system at a later time than as the model prediction.</li> <li>Recommender systems: For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.</li> <li>Object detection in images: For an object detection model, as new images are scored, there are no new labels being generated automatically. One option to obtain the ground truth for the new images is to use people to manually label the images. Human labelling is costly, time-consuming and not 100% accurate, so in most cases, only a subset of images can be labelled. These samples can be chosen at random or by using active learning techniques of selecting the most informative unlabeled samples.</li> </ul>"},{"location":"lab3/ml-model-checklist/#has-the-data-distribution-of-training-testing-and-validation-sets-been-analyzed","title":"Has the Data Distribution of Training, Testing and Validation Sets Been Analyzed?","text":"<p>The data distribution of your training, test and validation (if applicable) dataset (including labels) should be analyzed to ensure they all come from the same distribution. If this is not the case, some options to consider are: re-shuffling,  re-sampling, modifying the data, more samples need to be gathered or features removed from the dataset.</p> <p>Significant differences in the data distributions of the different datasets can greatly impact the performance of the model. Some potential questions to ask:</p> <ul> <li>How much does the training and test data represent the end result?</li> <li>Is the distribution of each individual feature consistent across all your datasets? (i.e. same representation of age groups, gender, race etc.)</li> <li>Is there any data lineage information? Where did the data come from? How was the data collected? Can collection and labelling be automated?</li> </ul> <p>Resources:</p> <ul> <li>\"Splitting into train, dev and test\" tutorial</li> </ul>"},{"location":"lab3/ml-model-checklist/#have-goals-and-hard-limits-for-performance-speed-of-prediction-and-costs-been-established-so-they-can-be-considered-if-trade-offs-need-to-be-made","title":"Have Goals and Hard Limits for Performance, Speed of Prediction and Costs been Established, so they can be Considered if Trade-Offs Need to be Made?","text":"<p>Some machine learning models achieve high ML performance, but they are costly and time-consuming to run. In those cases, a less performant and cheaper model could be preferred. Hence, it is important to calculate the model performance metrics (accuracy, precision, recall, RMSE etc), but also to gather data on how expensive it will be to run the model and how long it will take to run. Once this data is gathered, an informed decision should be made on what model to productionize.</p> <p>System metrics to consider:</p> <ul> <li>CPU/GPU/memory usage</li> <li>Cost per prediction</li> <li>Time taken to make a prediction</li> </ul>"},{"location":"lab3/ml-model-checklist/#how-will-the-model-be-integrated-into-other-systems-and-what-impact-will-it-have","title":"How Will the Model be Integrated into Other Systems, and what Impact will it Have?","text":"<p>Machine Learning models do not exist in isolation, but rather they are part of a much larger system. These systems could be old, proprietary systems or new systems being developed as a results of the creation a new machine learning model. In both of those cases, it is important to understand where the actual model is going to fit in, what output is expected from the model and how that output is going to be used by the larger system. Additionally, it is essential to decide if the model will be used for batch and/or real-time inference as production paths might differ.</p> <p>Possible questions to assess model impact:</p> <ul> <li>Is there a human in the loop?</li> <li>How is feedback collected through the system? (for example how do we know if a prediction is wrong)</li> <li>Is there a fallback mechanism when things go wrong?</li> <li>Is the system transparent that there is a model making a prediction and what data is used to make this prediction?</li> <li>What is the cost of a wrong prediction?</li> </ul>"},{"location":"lab3/ml-model-checklist/#how-will-incoming-data-quality-be-monitored","title":"How Will Incoming Data Quality be Monitored?","text":"<p>As data systems become increasingly complex in the mainstream, it is especially vital to employ data quality monitoring, alerting and rectification protocols. Following data validation best practices can prevent insidious issues from creeping into machine learning models that, at best, reduce the usefulness of the model, and at worst, introduce harm. Data validation, reduces the risk of data downtime (increasing headroom) and technical debt and supports long-term success of machine learning models and other applications that rely on the data.</p> <p>Data validation best practices include:</p> <ul> <li>Employing automated data quality testing processes at each stage of the data pipeline</li> <li>Re-routing data that fails quality tests to a separate data store for diagnosis and resolution</li> <li>Employing end-to-end data observability on data freshness, distribution, volume, schema and lineage</li> </ul> <p>Note that data validation is distinct from data drift detection. Data validation detects errors in the data (ex. a datum is outside of the expected range), while data drift detection uncovers legitimate changes in the data that are truly representative of the phenomenon being modeled (ex. user preferences change). Data validation issues should trigger re-routing and rectification, while data drift should trigger adaptation or retraining of a model.</p> <p>Resources:</p> <ul> <li>\"Data Quality Fundamentals\" by Moses et al.</li> </ul>"},{"location":"lab3/ml-model-checklist/#how-will-drift-in-data-characteristics-be-monitored","title":"How Will Drift in Data Characteristics be Monitored?","text":"<p>Data drift detection uncovers legitimate changes in incoming data that are truly representative of the phenomenon being modeled,and are not erroneous (ex. user preferences change). It is imperative to understand if the new data in production will be significantly different from the data in the training phase. It is also important to check that the data distribution information can be obtained for any of the new data coming in. Drift monitoring can inform when changes are occurring and what their characteristics are (ex. abrupt vs gradual) and guide effective adaptation or retraining strategies to maintain performance.</p> <p>Possible questions to ask:</p> <ul> <li>What are some examples of drift, or deviation from the norm, that have been experience in the past or that might be expected?</li> <li>Is there a drift detection strategy in place? Does it align with expected types of changes?</li> <li>Are there warnings when anomalies in input data are occurring?</li> <li>Is there an adaptation strategy in place? Does it align with expected types of changes?</li> </ul> <p>Resources:</p> <ul> <li>\"Learning Under Concept Drift: A Review\" by Lu at al.</li> <li>Understanding dataset shift</li> </ul>"},{"location":"lab3/ml-model-checklist/#how-will-performance-be-monitored","title":"How Will Performance be Monitored?","text":"<p>It is important to define how the model will be monitored when it is in production and how that data is going to be used to make decisions. For example, when will a model need retraining as the performance has degraded and how to identify what are the underlying causes of this degradation could be part of this monitoring methodology.</p> <p>Ideally, model monitoring should be done automatically. However, if this is not possible, then there should be a manual periodical check of the model performance.</p> <p>Model monitoring should lead to:</p> <ul> <li>Ability to identify changes in model performance</li> <li>Warnings when anomalies in model output are occurring</li> <li>Retraining decisions and adaptation strategy</li> </ul>"},{"location":"lab3/ml-model-checklist/#have-any-ethical-concerns-been-taken-into-account","title":"Have any Ethical Concerns Been Taken into Account?","text":"<p>Every ML project goes through the Responsible AI process to ensure that it upholds Microsoft's 6 Responsible AI principles.</p>"},{"location":"lab3/model-experimentation/","title":"Model Experimentation","text":""},{"location":"lab3/model-experimentation/#overview","title":"Overview","text":"<p>Machine learning model experimentation involves uncertainty around the expected model results and future operationalization. To handle this uncertainty as much as possible, we propose a semi-structured process, balancing between engineering/research best practices and rapid model/data exploration.</p>"},{"location":"lab3/model-experimentation/#model-experimentation-goals","title":"Model Experimentation Goals","text":"<ul> <li>Performance: Find the best performing solution</li> <li>Operationalization: Keep an eye towards production, making sure that operationalization is feasible</li> <li>Code quality Maintain code and artifacts quality</li> <li>Reproducibility: Keep research active by allowing experiment tracking and reproducibility</li> <li>Collaboration: Foster the collaboration and joint work of multiple people on the team</li> </ul>"},{"location":"lab3/model-experimentation/#model-experimentation-challenges","title":"Model Experimentation Challenges","text":"<ul> <li>Trial and error process: Difficult to plan and estimate durations and capacity.</li> <li>Quick and dirty: We want to fail fast and get a sense of what\u2019s working efficiently.</li> <li>Collaboration: How do we form a team-wide trial and error process and effective brainstorming.</li> <li>Code quality: How do we maintain the quality of non-production code during research.</li> <li>Operationalization: Switching between approaches might have a significant impact on operationalization (e.g. GPU/CPU, batch/online, parallel/sequential, runtime environments).</li> </ul> <p>Creating an experimentation framework which facilitates rapid experimentation, collaboration, experiment and model reproducibility, evaluation  and defined APIs, and lets each team member focus on the model development and improvement, while trusting the framework to do the rest.</p> <p>The following tools and guidelines are aimed at achieving experimentation goals as well as addressing the aforementioned challenges.</p>"},{"location":"lab3/model-experimentation/#tools-and-guidelines-for-successful-model-experimentation","title":"Tools and Guidelines for Successful Model Experimentation","text":"<ul> <li>Virtual environments</li> <li>Source control and folder/package structure</li> <li>Experiment tracking</li> <li>Datasets and models abstractions</li> <li>Model evaluation</li> </ul>"},{"location":"lab3/model-experimentation/#virtual-environments","title":"Virtual Environments","text":"<p>In languages like Python and R, it is always advised to employ virtual environments. Virtual environments facilitate reproducibility, collaboration and productization. Virtual environments allow us to be consistent across our local dev envs as well as with compute resources. These environments' configuration files can be used to build the code from source in an consistent way. For more details on why we need virtual environments visit this blog post.</p>"},{"location":"lab3/model-experimentation/#which-virtual-environment-framework-should-i-choose","title":"Which Virtual Environment Framework should I Choose","text":"<p>All virtual environments frameworks create isolation, some also propose dependency management and additional features. Decision on which framework to use depends on the complexity of the development environment (dependencies and other required resources) and on the ease of use of the framework.</p>"},{"location":"lab3/model-experimentation/#types-of-virtual-environments","title":"Types of Virtual Environments","text":"<p>In ISE, we often choose from either <code>venv</code>, <code>Conda</code> or <code>Poetry</code>, depending on the project requirements and complexity.</p> <ul> <li>venv is included in Python, is the easiest to use, but lacks more advanced features like dependency management.</li> <li>Conda is a popular package, dependency and environment management framework. It supports multiple stacks (Python, R) and multiple versions of the same environment (e.g. multiple Python versions). <code>Conda</code> maintains its own package repository, therefore some packages might not be downloaded and managed directly through <code>Conda</code>.</li> <li>Poetry is a Python dependency management system which manages dependencies in a standard way using <code>pyproject.toml</code> files and <code>lock</code> files. Similar to <code>Conda</code>, <code>Poetry</code>'s dependency resolution process is sometimes slow (see FAQ), but in cases where dependency issues are common or tricky, it provides a robust way to create reproducible and stable environments.</li> </ul>"},{"location":"lab3/model-experimentation/#expected-outcomes-for-virtual-environments-setup","title":"Expected Outcomes for Virtual Environments Setup","text":"<ol> <li>Documentation describing how to create the selected virtual environment and how to install dependencies.</li> <li>Environment configuration files if applicable (e.g. <code>requirements.txt</code> for <code>venv</code>, environment.yml for <code>Conda</code> or pyrpoject.toml for <code>Poetry</code>).</li> </ol>"},{"location":"lab3/model-experimentation/#virtual-environments-benefits","title":"Virtual Environments Benefits","text":"<ul> <li>Productization</li> <li>Collaboration</li> <li>Reproducibility</li> </ul>"},{"location":"lab3/model-experimentation/#source-control-and-folder-or-package-structure","title":"Source Control and Folder or Package Structure","text":"<p>Applied ML projects often contain source code, notebooks, devops scripts, documentation, scientific resources, datasets and more. We recommend coming up with an agreed folder structure to keep resources tidy. Consider deciding upon a generic folder structure for projects (e.g. which contains the folders <code>data</code>, <code>src</code>, <code>docs</code> and <code>notebooks</code>), or adopt popular structures like the CookieCutter Data Science folder structure.</p> <p>Source control should be applied to allow collaboration, versioning, code reviews, traceability and backup. In data science projects, source control should be used for code, and the storing and versioning of other  artifacts (e.g. data, scientific literature) should be decided upon depending on the scenario.</p>"},{"location":"lab3/model-experimentation/#folder-structure-and-source-control-expected-outcomes","title":"Folder Structure and Source Control Expected Outcomes","text":"<ul> <li>Defined folder structure for all users to use, pushed to the repo.</li> <li>.gitignore file determining which folders should be synced with <code>git</code> and which should be kept locally. For example, this one.</li> <li>Determine how notebooks are stored and versioned (e.g. strip output from Jupyter notebooks)</li> </ul>"},{"location":"lab3/model-experimentation/#source-control-and-folder-structure-benefits","title":"Source Control and Folder Structure Benefits","text":"<ul> <li>Collaboration</li> <li>Reproducibility</li> <li>Code quality</li> </ul>"},{"location":"lab3/model-experimentation/#experiment-tracking","title":"Experiment Tracking","text":"<p>Experiment tracking tools allow data scientists and researchers to keep track of previous experiments for better understanding of the experimentation process and for the reproducibility of experiments or models.</p>"},{"location":"lab3/model-experimentation/#types-of-experiment-tracking-frameworks","title":"Types of Experiment Tracking Frameworks","text":"<p>Experiment tracking frameworks differ by the set of features they provide for collecting experiment metadata, and comparing and analyzing experiments. In ISE, we mainly use MLFlow on Databricks or Azure ML Experimentation. Note that some experiment tracking frameworks require a deployment, while others are SaaS.</p>"},{"location":"lab3/model-experimentation/#experiment-tracking-outcomes","title":"Experiment Tracking Outcomes","text":"<ol> <li>Decide on an experiment tracking framework</li> <li>Ensure it is accessible to all users</li> <li>Document set-up on local environments</li> <li>Define datasets and evaluation in a way which will allow the comparison of all experiments. Consistency across datasets and evaluation is paramount for experiment comparison.</li> <li>Ensure full reproducibility by assuring that all required details are tracked (i.e. dataset names and versions, parameters, code, environment)</li> </ol>"},{"location":"lab3/model-experimentation/#experiment-tracking-benefits","title":"Experiment Tracking Benefits","text":"<ul> <li>Model performance</li> <li>Reproducibility</li> <li>Collaboration</li> <li>Code quality</li> </ul>"},{"location":"lab3/model-experimentation/#datasets-and-models-abstractions","title":"Datasets and Models Abstractions","text":"<p>By creating abstractions to building blocks (e.g., datasets, models, evaluators), we allow the easy introduction of new logic into the experimentation pipeline while keeping the agreed upon experimentation flow intact.</p> <p>These abstractions can be created using different mechanisms. For example, we can use Object-Oriented Programming (OOP) solutions like abstract classes:</p> <ul> <li>An example from scikit-learn describing the creation of new estimators compatible with the API.</li> <li>An example from PyTorch on extending the abstract <code>Dataset</code> class.</li> </ul>"},{"location":"lab3/model-experimentation/#abstraction-outcomes","title":"Abstraction Outcomes","text":"<ol> <li>Different building blocks have defined APIs allowing them to be replaced or extended.</li> <li>Replacing building blocks does not break the original experimentation flow.</li> <li>Mock building blocks are used for unit tests</li> <li>APIs/mocks are shared with the engineering teams for integration with other modules.</li> </ol>"},{"location":"lab3/model-experimentation/#abstraction-benefits","title":"Abstraction Benefits","text":"<ul> <li>Collaboration</li> <li>Code quality</li> <li>Reproducibility</li> <li>Operationalization</li> <li>Model performance</li> </ul>"},{"location":"lab3/model-experimentation/#model-evaluation","title":"Model Evaluation","text":"<p>When deciding on the evaluation of the ML model/process, consider the following checklist:</p> <ul> <li> Evaluation logic is approved by all stakeholders.</li> <li> Relationship between evaluation logic and business KPIs is analyzed and decided.</li> <li> Evaluation flow is applicable for all present and future models (i.e. does not assume some prediction structure or method-specific process).</li> <li> Evaluation code is unit-tested and reviewed by all team members.</li> <li> Evaluation flow facilitates further results and error analysis.</li> </ul>"},{"location":"lab3/model-experimentation/#evaluation-development-process-outcomes","title":"Evaluation Development Process Outcomes","text":"<ol> <li>Evaluation strategy is agreed upon all stakeholders</li> <li>Research and discussion on various evaluation methods and metrics is documented.</li> <li>The code holding the logic and data structures for evaluation is reviewed and tested.</li> <li>Documentation on how to apply evaluation is reviewed.</li> <li>Performance metrics are automatically tracked into the experiment tracker.</li> </ol>"},{"location":"lab3/model-experimentation/#evaluation-development-process-benefits","title":"Evaluation Development Process Benefits","text":"<ul> <li>Model performance</li> <li>Code quality</li> <li>Collaboration</li> <li>Reproducibility</li> </ul>"},{"location":"lab3/profiling-ml-and-mlops-code/","title":"Profiling Machine Learning and MLOps Code","text":"<p>Data Science projects, especially the ones that involve Deep Learning techniques, usually are resource intensive. One model training iteration might be multiple hours long. Although large data volumes processing genuinely takes time, minor bugs and suboptimal implementation of some functional pieces might cause extra resources consumption.</p> <p>Profiling can be used to identify performance bottlenecks and see which functions are the costliest in the application code. Based on the outputs of the profiler, one can focus on largest and easiest-to-resolve inefficiencies and therefore achieve better code performance. Although profiling follows the same principles of any other software project, the purpose of this document is to provide profiling samples for the most common scenarios in MLOps/Data Science projects.</p> <p>Below are some common scenarios in MLOps/Data Science projects, along with suggestions on how to profile them.</p> <ul> <li>Generic Python profiling</li> <li>PyTorch model training profiling</li> <li>Azure Machine Learning pipeline profiling</li> </ul>"},{"location":"lab3/profiling-ml-and-mlops-code/#generic-python-profiling","title":"Generic Python Profiling","text":"<p>Usually an MLOps/Data Science solution contains plain Python code serving different purposes (e.g. data processing) along with specialized model training code. Although many Machine Learning frameworks provide their own profiler, sometimes it is also useful to profile the whole solution.</p> <p>There are two types of profilers: deterministic (all events are tracked, e.g. cProfile) and statistical (sampling with regular intervals, e.g., py-spy). The sample below shows an example of a deterministic profiler.</p> <p>There are many options of generic deterministic Python code profiling. One of the default options for profiling used to be a built-in cProfile profiler. Using cProfile one can easily profile either a Python script or just a chunk of code. This profiling tool produces a file that can be either visualized using open source tools or analyzed using <code>stats.Stats</code> class. The latter option requires setting up filtering and sorting parameters for better analysis experience.</p> <p>Below you can find an example of using cProfile to profile a chunk of code.</p> <pre><code>import cProfile\n\n# Start profiling\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# -- YOUR CODE GOES HERE ---\n\n# Stop profiling\nprofiler.disable()\n\n# Write profiler results to an html file\nprofiler.dump_stats(\"profiler_results.prof\")\n</code></pre> <p>You can also run cProfile outside of the Python script using the following command:</p> <pre><code>python -m cProfile [-o output_file] [-s sort_order] (-m module | myscript.py)\n</code></pre> <p>Note: one epoch of model training is usually enough for profiling. There's no need to run more epochs and produce additional cost.</p> <p>Refer to The Python Profilers for further details.</p>"},{"location":"lab3/profiling-ml-and-mlops-code/#pytorch-model-training-profiling","title":"PyTorch Model Training Profiling","text":"<p>PyTorch 1.8 includes an updated PyTorch profiler that is supplied together with the PyTorch distribution and doesn't require any additional installation. Using PyTorch profiler one can record CPU side operations as well as CUDA kernel launches on GPU side. The profiler can visualize analysis results using TensorBoard plugin as well as provide suggestions on bottlenecks and potential code improvements.</p> <pre><code> with torch.profiler.profile(\n    # Limit number of training steps included in profiling\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n    # Automatically saves profiling results to disk\n    on_trace_ready=torch.profiler.tensorboard_trace_handler,\n    with_stack=True\n) as profiler:\n    for step, data in enumerate(trainloader, 0):\n        # -- TRAINING STEP CODE GOES HERE ---\n        profiler.step()\n</code></pre> <p>The <code>tensorboard_trace_handler</code> can be used to generate result files for TensorBoard. Those can be visualized by installing TensorBoard. plugin and running TensorBoard on your log directory.</p> <pre><code>pip install torch_tb_profiler\ntensorboard --logdir=&lt;LOG_DIR_PATH&gt;\n# Navigate to `http://localhost:6006/#pytorch_profiler`\n</code></pre> <p>Note: make sure to provide the right parameters to the <code>torch.profiler.schedule</code>. Usually you would need several steps of training to be profiled rather than the whole epoch.</p> <p>More information on PyTorch profiler:</p> <ul> <li>PyTorch Profiler Recipe</li> <li>Introducing PyTorch Profiler - the new and improved performance tool</li> </ul>"},{"location":"lab3/profiling-ml-and-mlops-code/#azure-machine-learning-pipeline-profiling","title":"Azure Machine Learning Pipeline Profiling","text":"<p>In our projects we often use Azure Machine Learning pipelines to train Machine Learning models. Most of the profilers can also be used in conjunction with Azure Machine Learning. For a profiler to be used with Azure Machine Learning, it should meet the following criteria:</p> <ul> <li>Turning the profiler on/off can be achieved by passing a parameter to the script ran by Azure Machine Learning</li> <li>The profiler produces a file as an output</li> </ul> <p>In general, a recipe for using profilers with Azure Machine Learning is the following:</p> <ol> <li>(Optional) If you're using profiling with an Azure Machine Learning pipeline, you might want to add <code>--profile</code> Boolean flag as a pipeline parameter</li> <li>Use one of the profilers described above or any other profiler that can produce a file as an output</li> <li> <p>Inside of your Python script, create step output folder, e.g.:</p> <pre><code>output_dir = \"./outputs/profiler_results\"\nos.makedirs(output_dir, exist_ok=True)\n</code></pre> </li> <li> <p>Run your training pipeline</p> </li> <li>Once the pipeline is completed, navigate to Azure ML portal and open details of the step that contains training code. The results can be found in the <code>Outputs+logs</code> tab, under <code>outputs/profiler_results</code> folder.</li> <li>You might want to download the results and visualize it locally.</li> </ol> <p>Note: it's not recommended to run profilers simultaneously. Profiles also consume resources, therefore a simultaneous run might significantly affect the results.</p>"},{"location":"lab3/proposed-ml-process/","title":"Proposed ML Process","text":""},{"location":"lab3/proposed-ml-process/#introduction","title":"Introduction","text":"<p>The objective of this document is to provide guidance to produce machine learning (ML) applications that are based on code, data and models that can be reproduced and reliably released to production environments. When developing ML applications, we consider the following approaches:</p> <ul> <li> <p>Best practices in ML engineering:</p> <ul> <li>The ML application development should use engineering fundamentals to ensure high quality software deliverables.</li> <li>The ML application should be reliability released into production, leveraging automation as much as possible.</li> <li>The ML application can be deployed into production at any time. This makes the decision about when to release it a business decision rather than a technical one.</li> </ul> </li> </ul> <ul> <li> <p>Best practices in ML research:</p> <ul> <li>All artifacts, specifically data, code and ML models, should be versioned and managed using standard tools and workflows, in order to facilitate continuous research and development.</li> <li>While the model outputs can be non-deterministic and hard to reproduce, the process of releasing ML software into production should be reproducible.</li> <li>Responsible AI aspects are carefully analyzed and addressed.</li> </ul> </li> </ul> <ul> <li> <p>Cross-functional team:</p> <ul> <li>A cross-functional team consisting of different skill sets in data science, data engineering, development, operations, and industry domain specialists is required.</li> </ul> </li> </ul>"},{"location":"lab3/proposed-ml-process/#ml-process","title":"ML process","text":"<p>The proposed ML development process consists of:</p> <ol> <li>Data and problem understanding</li> <li>Responsible AI assessment</li> <li>Feasibility study</li> <li>Baseline model experimentation</li> <li>Model evaluation and experimentation</li> <li>Model operationalization     * Unit and Integration testing     * Deployment     * Monitoring and Observability</li> </ol>"},{"location":"lab3/proposed-ml-process/#version-control","title":"Version Control","text":"<ul> <li>During all stages of the process, it is suggested that artifacts should be version-controlled. Typically, the process is iterative and versioned artifacts can assist in traceability and reviewing.</li> </ul>"},{"location":"lab3/proposed-ml-process/#understanding-the-problem","title":"Understanding the Problem","text":"<ul> <li>Define the business problem for the ML project:<ul> <li>Agree on the success criteria with the customer.</li> <li>Identify potential data sources and determine the availability of these sources.</li> <li>Define performance evaluation metrics on ground truth data</li> </ul> </li> <li>Conduct a Responsible AI assessment to ensure development and deployment of the ML solution in a responsible manner.</li> <li>Conduct a feasibility study to assess whether the business problem is feasible to solve satisfactorily using ML with the available data. The objective of the feasibility study is to mitigate potential over-investment by ensuring sufficient evidence that ML is possible and would be the best solution. The study also provides initial indications of what the ML solution should look like. This ensures quality solutions supported by thorough consideration and evidence. Refer to feasibility study.</li> <li>Exploratory data analysis is performed and discussed with the team</li> </ul> <ul> <li> <p>Typical output:</p> <ul> <li>Data exploration source code (Jupyter notebooks/scripts) and slides/docs</li> <li>Initial ML model code (Jupyter notebook or scripts)</li> <li>Initial solution architecture with initial data engineering requirements</li> <li>Data dictionary (if not yet available)</li> <li>List of assumptions</li> </ul> </li> </ul>"},{"location":"lab3/proposed-ml-process/#baseline-model-experimentation","title":"Baseline Model Experimentation","text":"<ul> <li>Data preparation: creating data source connectors, determining storage services to be used and potential versioning of raw datasets.</li> <li>Feature engineering: create new features from raw source data to increase the predictive power of the learning algorithm. The features should capture additional information that is not apparent in the original feature set.</li> <li>Split data into training, validation and test sets: creating training, validation, and test datasets with ground truth to develop ML models. This would entail joining or merging various feature engineered datasets. The training dataset is used to train the model to find the patterns between its features and labels (ground truth). The validation dataset is used to assess the model architecture, and the test data is used to confirm the prediction quality of the model.</li> <li>Initial code to create access data sources, transform raw data into features and model training as well as scoring.</li> <li>During this phase, experiment code (Jupyter notebooks or scripts) and accompanying utility code should be version-controlled using tools such as ADO (Azure DevOps).</li> </ul> <ul> <li>Typical output: Rough Jupyter notebooks or scripts in Python or R, initial results from baseline model.</li> </ul> <p>For more information on experimentation, refer to the experimentation section.</p>"},{"location":"lab3/proposed-ml-process/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>Compare the effectiveness of different algorithms on the given problem.</li> </ul> <ul> <li>Typical output:<ul> <li>Evaluation flow is fully set up.</li> <li>Reproducible experiments for the different approaches experimented with.</li> </ul> </li> </ul>"},{"location":"lab3/proposed-ml-process/#model-operationalization","title":"Model Operationalization","text":"<ul> <li>Taking \"experimental\" code and preparing it, so it can be deployed. This includes data pre-processing, featurization code, training model code (if required to be trained using CI/CD) and model inference code.</li> </ul> <ul> <li>Typical output:<ul> <li>Production-grade code (Preferably in the form of a package) for:<ul> <li>Data preprocessing / post processing</li> <li>Serving a model</li> <li>Training a model</li> </ul> </li> <li>CI/CD scripts.</li> <li>Reproducibility steps for the model in production.</li> <li>See more in the ML model checklist.</li> </ul> </li> </ul>"},{"location":"lab3/proposed-ml-process/#unit-and-integration-testing","title":"Unit and Integration Testing","text":"<ul> <li>Ensuring that production code behaves in the way we expect it to, and that its results match those we saw during the Model Evaluation and Experimentation phases.</li> <li>Refer to ML testing post for further details.</li> <li>Typical output: Test suite with unit and end-to-end tests is created and completes successfully.</li> </ul>"},{"location":"lab3/proposed-ml-process/#deployment","title":"Deployment","text":"<ul> <li>Responsible AI considerations such as bias and fairness analysis. Additionally, explainability/interpretability of the model should also be considered.</li> <li>It is recommended for a human-in-the-loop to verify the model and manually approve deployment to production.</li> <li>Getting the model into production where it can start adding value by serving predictions. Typical artifacts are APIs for accessing the model and integrating the model to the solution architecture.</li> <li>Additionally, certain scenarios may require training the model periodically in production.</li> <li>Reproducibility steps of the production model are available.</li> <li>Typical output: model readiness checklist is completed.</li> </ul>"},{"location":"lab3/proposed-ml-process/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>This is the final phase, where we ensure our model is doing what we expect it to in production.</li> <li>Read more about ML observability.</li> <li>Read more about Azure ML's offerings around ML models production monitoring.</li> <li>It is recommended to consider incorporating data drift monitoring process in the production solution. This will assist in detecting potential changes in new datasets presented for inference that may significantly impact model performance. For more info on detecting data drift with Azure ML see the Microsoft docs article on how to monitor datasets.</li> <li>Typical output: Logging and monitoring scripts and tools set up, permissions for users to access monitoring tools.</li> </ul>"},{"location":"lab3/responsible-ai/","title":"Responsible AI in ISE","text":""},{"location":"lab3/responsible-ai/#microsofts-responsible-ai-principles","title":"Microsoft's Responsible AI principles","text":"<p>Every ML project in ISE goes through a Responsible AI (RAI) assessment to ensure that it upholds Microsoft's 6 Responsible AI principles:</p> <ul> <li>Fairness</li> <li>Reliability &amp; Safety</li> <li>Privacy &amp; Security</li> <li>Inclusiveness</li> <li>Transparency</li> <li>Accountability</li> </ul> <p>Every project goes through the RAI process, whether we are building a new ML model from scratch, or putting an existing model in production.</p>"},{"location":"lab3/responsible-ai/#ises-responsible-ai-process","title":"ISE's Responsible AI process","text":"<p>The process begins as soon as we start a prospective project. We start to complete a Responsible AI review document, and an impact assessment, which provides a structured way to explore topics such as:</p> <ul> <li>Can the problem be addressed with a non-technical (e.g. social) solution?</li> <li>Can the problem be solved without AI? Would simpler technology suffice?</li> <li>Will the team have access to domain experts (e.g. doctors, refugees) in the field where the AI is applicable?</li> <li>Who are the stakeholders in this project? Who does the AI impact? Are there any vulnerable groups affected?</li> <li>What are the possible benefits and harms to each stakeholder?</li> <li>How can the technology be misused, and what can go wrong?</li> <li>Has the team analyzed the input data properly to make sure that the training data is suitable for machine learning?</li> <li>Is the training data an accurate representation of data that will be used as input in production?</li> <li>Is there a good representation of all users?</li> <li>Is there a fall-back mechanism (a human in the loop, or a way to revert decisions based on the model)?</li> <li>Does data used by the model for training or scoring contain PII? What measures have been taken to remove sensitive data?</li> <li>Does the model impact consequential decisions, like blocking people from getting jobs, loans, health care etc. or in the cases where it may, have appropriate ethical considerations been discussed?</li> <li>Have measures for re-training been considered?</li> <li>How can we address any concerns that arise, and how can we mitigate risk?</li> </ul> <p>At this point we research available tools and resources, such as InterpretML or Fairlearn, that we may use on the project. We may change the project scope or re-define the ML problem definition if necessary.</p> <p>The Responsible AI review documents remain living documents that we re-visit and update throughout project development, through the feasibility study, as the model is developed and prepared for production, and new information unfolds. The documents can be used and expanded once the model is deployed, and monitored in production.</p>"},{"location":"lab3/testing-data-science-and-mlops-code/","title":"Testing Data Science and MLOps Code","text":"<p>The purpose of this document is to provide samples of tests for the most common operations in MLOps/Data Science projects. Testing the code used for MLOps or data science projects follows the same principles of any other software project.</p> <p>Some scenarios might seem different or more difficult to test. The best way to approach this is to always have a test design session, where the focus is on the input/outputs, exceptions and testing the behavior of data transformations. Designing the tests first makes it easier to test as it forces a more modular style, where each function has one purpose, and extracting common functionality functions and modules.</p> <p>Below are some common operations in MLOps or Data Science projects, along with suggestions on how to test them.</p> <ul> <li>Saving and loading data</li> <li>Transforming data</li> <li>Model load or predict</li> <li>Data validation</li> <li>Model testing</li> </ul>"},{"location":"lab3/testing-data-science-and-mlops-code/#saving-and-loading-data","title":"Saving and Loading Data","text":"<p>Reading and writing to csv, reading images or loading audio files are common scenarios encountered in MLOps projects.</p>"},{"location":"lab3/testing-data-science-and-mlops-code/#example-verify-that-a-load-function-calls-read_csv-if-the-file-exists","title":"Example: Verify that a Load Function Calls read_csv if the File Exists","text":"<p><code>utils.py</code></p> <pre><code>def load_data(filename: str) -&gt; pd.DataFrame:\n    if os.path.isfile(filename):\n        df = pd.read_csv(filename, index_col='ID')\n        return df\n    return None\n</code></pre> <p>There's no need to test the <code>read_csv</code> function, or the <code>isfile</code> functions, we can leave testing them to the pandas and os developers.</p> <p>The only thing we need to test here is the logic in this function, i.e. that <code>load_data</code> loads the file if the file exists with the right index column, and doesn't load the file if it doesn't exist, and that it returns the expected results.</p> <p>One way to do this would be to provide a sample file and call the function, and verify that the output is None or a DataFrame. This requires separate files to be present, or not present, for the tests to run. This can cause the same test to run on one machine and then fail on a build server which is not a desired behavior.</p> <p>A much better way is to mock calls to <code>isfile</code>, and <code>read_csv</code>. Instead of calling the real function, we will return a predefined return value, or call a stub that doesn't have any side effects. This way no files are needed in the repository to execute the test, and the test will always work the same, independent of what machine it runs on.</p> <p>Note: Below we mock the specific os and pd functions referenced in the utils file, any others are left unaffected and would run as normal.</p> <p><code>test_utils.py</code></p> <pre><code>import utils\nfrom mock import patch\n\n\n@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_calls_read_csv_if_exists(mock_isfile, mock_read_csv):\n    # arrange\n    # always return true for isfile\n    utils.os.path.isfile.return_value = True\n    filename = 'file.csv'\n\n    # act\n    _ = utils.load_data(filename)\n\n    # assert\n    # check that read_csv is called with the correct parameters\n    utils.pd.read_csv.assert_called_once_with(filename, index_col='ID')\n</code></pre> <p>Similarly, we can verify that it's called 0 or multiple times. In the example below where we verify that it's not called if the file doesn't exist</p> <pre><code>@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_does_not_call_read_csv_if_not_exists(mock_isfile, mock_read_csv):\n    # arrange\n    # file doesn't exist\n    utils.os.path.isfile.return_value = False\n    filename = 'file.csv'\n\n    # act\n    _ = utils.load_data(filename)\n\n    # assert\n    # check that read_csv is not called\n    assert utils.pd.read_csv.call_count == 0\n</code></pre>"},{"location":"lab3/testing-data-science-and-mlops-code/#example-using-the-same-sample-data-for-multiple-tests","title":"Example: Using the Same Sample Data for Multiple Tests","text":"<p>If more than one test will use the same sample data, fixtures are a good way to reuse this sample data. The sample data can be the contents of a json file, or a csv, or a DataFrame, or even an image.</p> <p>Note: The sample data is still hard coded if possible, and does not need to be large. Only add as much sample data as required for the tests to make the tests readable.</p> <p>Use the fixture to return the sample data, and add this as a parameter to the tests where you want to use the sample data.</p> <pre><code>import pytest\n\n\n@pytest.fixture\ndef house_features_json():\n  return {'area': 25, 'price': 2500, 'rooms': np.nan}\n\ndef test_clean_features_cleans_nan_values(house_features_json):\n  cleaned_features = clean_features(house_features_json)\n  assert cleaned_features['rooms'] == 0\n\ndef test_extract_features_extracts_price_per_area(house_features_json):\n  extracted_features = extract_features(house_features_json)\n  assert extracted_features['price_per_area'] == 100\n</code></pre>"},{"location":"lab3/testing-data-science-and-mlops-code/#transforming-data","title":"Transforming Data","text":"<p>For cleaning and transforming data, test fixed input and output, but try to limit each test to one verification.</p> <p>For example, create one test to verify the output shape of the data.</p> <pre><code>def test_resize_image_generates_the_correct_size():\n  # Arrange\n  original_image = np.ones((10, 5, 2, 3))\n\n  # act\n  resized_image = utils.resize_image(original_image, 100, 100)\n\n  # assert\n  resized_image.shape[:2] = (100, 100)\n</code></pre> <p>and one to verify that any padding is made appropriately</p> <pre><code>def test_resize_image_pads_correctly():\n  # Arrange\n  original_image = np.ones((10, 5, 2, 3))\n\n  # Act\n  resized_image = utils.resize_image(original_image, 100, 100)\n\n  # Assert\n  assert resized_image[0][0][0][0] == 0\n  assert resized_image[0][0][2][0] == 1\n</code></pre> <p>To test different inputs and expected outputs automatically, use parametrize</p> <pre><code>@pytest.mark.parametrize('orig_height, orig_width, expected_height, expected_width',\n                         [\n                             # smaller than target\n                             (10, 10, 20, 20),\n                             # larger than target\n                             (20, 20, 10, 10),\n                             # wider than target\n                             (10, 20, 10, 10)\n                         ])\ndef test_resize_image_generates_the_correct_size(orig_height, orig_width, expected_height, expected_width):\n  # Arrange\n  original_image = np.ones((orig_height, orig_width, 2, 3))\n\n  # act\n  resized_image = utils.resize_image(original_image, expected_height, expected_width)\n\n  # assert\n  resized_image.shape[:2] = (expected_height, expected_width)\n</code></pre>"},{"location":"lab3/testing-data-science-and-mlops-code/#model-load-or-predict","title":"Model Load or Predict","text":"<p>When unit testing we should mock model load and model predictions similarly to mocking file access.</p> <p>There may be cases when you want to load your model to do smoke tests, or integration tests.</p> <p>Since these will often take a bit longer to run it's important to be able to separate them from unit tests so that the developers on the team can still run unit tests as part of their test driven development.</p> <p>One way to do this is using marks</p> <pre><code>@pytest.mark.longrunning\ndef test_integration_between_two_systems():\n    # this might take a while\n</code></pre> <p>Run all tests that are not marked <code>longrunning</code></p> <pre><code>pytest -v -m \"not longrunning\"\n</code></pre>"},{"location":"lab3/testing-data-science-and-mlops-code/#basic-unit-tests-for-ml-models","title":"Basic Unit Tests for ML Models","text":"<p>ML unit tests are not intended to check the accuracy or performance of a model. Unit tests for an ML model is for code quality checks - for example:</p> <ul> <li>Does the model accept the correct inputs and produce the correctly shaped outputs?</li> <li>Do the weights of the model update when running <code>fit</code>?</li> </ul> <p>To do this, the ML model tests do not strictly follow best practices of standard Unit tests - not all outside calls are mocked. These tests are much closer to a narrow integration test. However, the benefits of having simple tests for the ML model help to stop a poorly configured model from spending hours in training, while still producing poor results.</p> <p>Examples of how to implement these tests (for Deep Learning models) include:</p> <ul> <li>Build a model and compare the shape of input layers to that of an example source of data. Then, compare the output layer shape to the expected output.</li> <li>Initialize the model and record the weights of each layer. Then, run a single epoch of training on a dummy data set, and compare the weights of the \"trained model\" - only check if the values have changed.</li> <li>Train the model on a dummy dataset for a single epoch, and then validate with dummy data - only validate that the prediction is formatted correctly, this model will not be accurate.</li> </ul>"},{"location":"lab3/testing-data-science-and-mlops-code/#data-validation","title":"Data Validation","text":"<p>An important part of the unit testing is to include test cases for data validation. For example, no data supplied, images that are not in the expected format, data containing null values or outliers to make sure that the data processing pipeline is robust.</p>"},{"location":"lab3/testing-data-science-and-mlops-code/#model-testing","title":"Model Testing","text":"<p>Apart from unit testing code, we can also test, debug and validate our models in different ways during the training process</p> <p>Some options to consider at this stage:</p> <ul> <li>Adversarial and Boundary tests to increase robustness</li> <li>Verifying accuracy for under-represented classes</li> </ul>"},{"location":"lab3/tpm-considerations-for-ml-projects/","title":"TPM considerations for Machine Learning projects","text":"<p>In this document, we explore some of the Program Management considerations for Machine Learning (ML) projects and suggest recommendations for Technical Program Managers (TPM) to effectively work with Data and Applied Machine Learning engineering teams.</p>"},{"location":"lab3/tpm-considerations-for-ml-projects/#determine-the-need-for-machine-learning-in-the-project","title":"Determine the Need for Machine Learning in the Project","text":"<p>In Artificial Intelligence (AI) projects, the ML component is generally a part of an overall business problem and NOT the problem itself. Determine the overall business problem first and then evaluate if ML can help address a part of the problem space. Few considerations for identifying the right fit for the project:</p> <ul> <li>Engage experts in human experience and employ techniques such as Design Thinking and Problem Formulation to understand the customer needs and human behavior first. Identify the right stakeholders from both business and technical leadership and invite them to these workshops. The outcome should be end-user scenarios and personas to determine the real needs of the users.</li> </ul> <ul> <li>Focus on System Design principles to identify the architectural components, entities, interfaces, constraints. Ask the right questions early and explore design alternatives with the engineering team.</li> </ul> <ul> <li>Think hard about the costs of ML and whether we are solving a repetitive problem at scale. Many a times, customer problems can be solved with data analytics, dashboards, or rule-based algorithms as the first phase of the project.</li> </ul>"},{"location":"lab3/tpm-considerations-for-ml-projects/#set-expectations-for-high-ambiguity-in-ml-components","title":"Set Expectations for High Ambiguity in ML components","text":"<p>ML projects can be plagued with a phenomenon we can call as the \"Death by Unknowns\". Unlike software engineering projects, ML focused projects can result in quick success early (aka sudden decrease in error rate), but this may flatten eventually. Few things to consider:</p> <ul> <li>Set clear expectations: Identify the performance metrics and discuss on a \"good enough\" prediction rate that will bring value to the business. An 80% \"good enough\" rate may save business costs and increase productivity but if going from 80 to 95% would require unimaginable cost and effort. Is it worth it? Can it be a progressive road map?</li> </ul> <ul> <li>Create a smaller team and undertake a feasibility analysis through techniques like EDA (Exploratory Data Analysis). A feasibility study is much cheaper to evaluate data quality, customer constraints and model feasibility. It allows a TPM to better understand customer use cases and current environment and can act as a fail-fast mechanism. Note that feasibility should be shorter (in weeks) else it misses the point of saving costs.</li> </ul> <ul> <li>As in any project, there will be new needs (additional data sources, technical constraints, hiring data labelers, business users time etc.). Incorporate Agile techniques to fail fast and minimize cost and schedule surprises.</li> </ul>"},{"location":"lab3/tpm-considerations-for-ml-projects/#notebooks-ml-production","title":"Notebooks != ML Production","text":"<p>Notebooks are a great way to kick start Data Analytics and Applied Machine Learning efforts, however for a production releases, additional constraints should be considered:</p> <ul> <li>Understand the end-end flow of data management, how data will be made available (ingestion flows), what's the frequency, storage, retention of data. Plan user stories and design spikes around these flows to ensure a robust ML pipeline is developed.</li> </ul> <ul> <li>Engineering team should follow the same rigor in building ML projects as in any software engineering project. We at ISE (Industry Solutions Engineering) have built a good set of resources from our learnings in our ISE Engineering Playbook.</li> <li>Think about the how the model will be deployed, for example, are there technical constraints due to an edge device, or network constraints that will prevent updating the model. Understanding of the environment is critical, refer to the Model Production Checklist as a reference to determine model deployment choices.</li> </ul> <ul> <li>ML Focussed projects are not a \"one-shot\" release solution, they need to be nurtured, evolved, and improved over time. Plan for a continuous improvement lifecycle, the initial phases can be model feasibility and validation to get the good enough prediction rate, the later phases can be then be scaling and improving the models through feedback loops and fresh data sets.</li> </ul>"},{"location":"lab3/tpm-considerations-for-ml-projects/#garbage-data-in-garbage-model-out","title":"Garbage Data In -&gt; Garbage Model Out","text":"<p>Data quality is a major factor in affecting model performance and production roll-out, consider the following:</p> <ul> <li>Conduct a data exploration workshop and generate a report on data quality that includes missing values, duplicates, unlabeled data, expired or not valid data, incomplete data (e.g., only having male representation in a people dataset).</li> </ul> <ul> <li>Identify data source reliability to ensure data is coming from a production source. (e.g., are the images from a production or industrial camera or taken from an iPhone/Android phone.)</li> </ul> <ul> <li>Identify data acquisition constraints: Determine how the data is being obtained and the constraints around it. Some example may include legal, contractual, Privacy, regulation, ethics constraints. These can significantly slow down production roll out if not captured in the early phases of the project.</li> </ul> <ul> <li>Determine data volumes: Identify if we have enough data for sampling the required business use case and how will the data be improved over time. The thumb rule here is that data should be enough for generalization to avoid overfitting.</li> </ul>"},{"location":"lab3/tpm-considerations-for-ml-projects/#plan-for-unique-roles-in-ai-projects","title":"Plan for Unique Roles in AI projects","text":"<p>An ML Project has multiple stages, and each stage may require additional roles. For example, Design Research &amp; Designers for Human Experience, Data Engineer for Data Collection, Feature Engineering, a Data Labeler for labeling structured data, engineers for MLOps and model deployment and the list can go on. As a TPM, factor in having these resources available at the right time to avoid any schedule risks.</p>"},{"location":"lab3/tpm-considerations-for-ml-projects/#feature-engineering-and-hyperparameter-tuning","title":"Feature Engineering and Hyperparameter Tuning","text":"<p>Feature Engineering enables the transformation of data so that it becomes usable for an algorithm. Creating the right features is an art and may require experimentation as well as domain expertise. Allocate time for domain experts to help with improving and identifying the best features. For example, for a natural language processing engine for text extraction of financial documents, we may involve financial researchers and run a relevance judgment exercise and provide a feedback loop to evaluate model performance.</p>"},{"location":"lab3/tpm-considerations-for-ml-projects/#responsible-ai-considerations","title":"Responsible AI Considerations","text":"<p>Bias in machine learning could be the number one issue of a model not performing to its intended needs. Plan to incorporate Responsible AI principles from Day 1 to ensure fairness, security, privacy and transparency of the models.  For example, for a person recognition algorithm, if the data source is only feeding a specific skin type, then production scenarios may not provide good results.</p>"},{"location":"lab3/tpm-considerations-for-ml-projects/#pm-fundamentals","title":"PM Fundamentals","text":"<p>Core to a TPM role are the fundamentals that include bringing clarity to the team, design thinking, driving the team to the right technical decisions, managing risk, managing stakeholders, backlog management, project management. These are a TPM superpowers. A TPM can complement the machine learning team by ensuring the problem and customer needs are understood, a wholistic system design is evaluated, the stakeholder expectations and driving customer objectives. Here are some references that may help:</p> <ul> <li>The T in a TPM</li> <li>The TPM Don't M*ck up framework</li> <li>The mind of a TPM</li> <li>ML Learning Journey for a TPM</li> </ul>"}]}